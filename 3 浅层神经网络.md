## 3 浅层神经网络(Shallow neural networks)

### 3.1 神经网络的表示


#### 3.1.1 神经网络的结构表示

下图所示为只包含一个隐藏层的神经网络：
<img src="https://upload-images.jianshu.io/upload_images/24435917-a1ba546501676c53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" style="zoom:50%;" />

- 输入层：输入特征$x_1$、$x_2$、$x_3$， 它们被竖直地堆叠起来形成列，作为神经网络的输入。
- 隐藏层：上图中间的四个神经元。在训练神经网络时，我们只知道网络的输入值和输出值，无法得知隐藏层中各节点的具体数值。
- 输出层：如上图中最后一层的一个神经元，负责产生预测值。

#### 3.1.2 神经网络的符号表示
![](https://upload-images.jianshu.io/upload_images/24435917-75c9d9f534c1e572.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

总结：

- **x**表示输入特征，**a**表示每个神经元的输出，**W**表示特征的权重。**在计算神经网络的层数时，不计算输入层**，故隐藏层第一层为1，以此类推；而在符号约定中，将输入层称为第零层。上标表示所处神经网络的层数，下标表示该层的第几个神经元。

细节：在上图所示的神经网络中

- 输入层的激活值称为$a^{[0]}$。
- 隐藏层激活值记作$a^{[1]}$。这一层第一个单元（节点）表示为$a_{1}^{[1]}$，第二个神经元的值为$a_{2}^{[1]}$，以此类推。本例中，神经网络第一层即隐藏层有四个节点，在Python代码中是一个4x1的矩阵：
                                                                                                                 $a^{[1]}=\left[\begin{array}{l}
a_{1}^{[1]} \\
a_{2}^{[1]} \\
a_{3}^{[1]} \\
a_{4}^{[1]}
\end{array}\right]$
-  输出层将产生某个数值$a^{[2]}$，即为该网络输出的预测值$\hat{y}$。

参数符号表示：

- 隐藏层拥有两个参数**W**和**b**，$\left(W^{[1]}, b^{[1]}\right)$表示该参数是神经网络第一层所用的参数。例如：在这个例子中**W**是一个4x3的矩阵，**b**是一个4x1的向量。4源于隐藏层含有四个神经元，3源于每个神经元都有三个输入特征。
- 类似地，输出层也有一些与之关联的参数$W^{[2]}$、$b^{[2]}$。它们的规模分别是1x4以及1x1。1x4是因为隐藏层有四个神经元而输出层只有一个。

### 3.2 计算单样本的神经网络输出
本节将介绍神经网络是如何运行的，即神经网络是怎么输入$x$，又是怎么得到$\hat{y}$。
#### 3.2.1 单个神经元的计算
![](https://upload-images.jianshu.io/upload_images/24435917-71a34a47fd490b26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

每个神经元的计算分为两步：首先计算出$z$，再计算出$a$。以隐藏层第一个神经元为例：
第一步，计算$z_{1}^{[1]}：z_{1}^{[1]}=w_{1}^{[1] T} x+b_{1}^{[1]}$。
第二步，以sigmoid函数为激活函数计算$z_{1}^{[1]}$得到$a_{1}^{[1]}$：$a_{1}^{[1]}=\sigma\left(z_{1}^{[1]}\right)$。
隐藏层后面神经元的计算过程一样，只是注意符号表示的不同，分别得到$a_{2}^{[1]}, a_{3}^{[1]}, a_{4}^{[1]}$，详细结果见下:
                                                  $$\begin{array}{l}
z_{1}^{[1]}=w_{1}^{[1] T} x+b_{1}^{[1]}, a_{1}^{[1]}=\sigma\left(z_{1}^{[1]}\right) \\
z_{2}^{[1]}=w_{2}^{[1] T} x+b_{2}^{[1]}, a_{2}^{[1]}=\sigma\left(z_{2}^{[1]}\right) \\
z_{3}^{[1]}=w_{3}^{[1] T} x+b_{3}^{[1]}, a_{3}^{[1]}=\sigma\left(z_{3}^{[1]}\right) \\
z_{4}^{[1]}=w_{4}^{[1] T} x+b_{4}^{[1]}, a_{4}^{[1]}=\sigma\left(z_{4}^{[1]}\right)
\end{array}$$

#### 3.2.2 向量化计算
在python中使用矩阵来执行计算程序，即向量化计算。使用向量化的方法，可以直接通过矩阵运算，不需要使用for循环，更加快速高效。

向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的$W$纵向堆积起来变成一个(4,3)的矩阵，用符号$W^{[1]}$表示。（4个逻辑回归单元，每一个逻辑回归单元都有3个相对应的参数）

因此， 得到公式
                                                                    $$z^{[n]}=W^{[n]} x+b^{[n]}$$

​                                                                    $$a^{[n]}=\sigma\left(z^{[n]}\right)$$

具体展开计算式可以理解为：

​                                                               $$a^{[1]}=\left[\begin{array}{l}
a_{1}^{[1]} \\
a_{2}^{[1]} \\
a_{3}^{[1]} \\
a_{4}^{[1]}
\end{array}\right]=\sigma\left(z^{[1]}\right)$$

​                                   $$z^{[1]}=\left[\begin{array}{c}
z_{1}^{[1]} \\
z_{2}^{[1]} \\
z_{3}^{[1]} \\
z_{4}^{[1]}
\end{array}\right]=\left[\begin{array}{c}
\left.\ldots W_{1}^{[1] T} \ldots\right ]\\
\cdots W_{2}^{[1] T} \cdots \\
\cdots W_{3}^{[1] T} \cdots \\
\ldots W_{4}^{[1] T} \cdots
\end{array}\right] *\left[\begin{array}{c}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]+\left[\begin{array}{c}
b_{1}^{[1]} \\
b_{2}^{[1]} \\
b_{3}^{[1]} \\
b_{4}^{[1]}
\end{array}\right]$$

类似的，下一层网络的表示可以写成类似的形式，得到$a^{[2]}, \hat{y}=a^{[2]}$，如下图所示：
![](https://upload-images.jianshu.io/upload_images/24435917-c45b3b511e99de5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 3.3 多样本向量化的神经网络输出
3.2节中仅针对于单一的训练样本解释向量化计算。本节介绍多样本的向量化计算，即重复m次单样本计算过程。

#### 3.3.1 向量化的运算符号定义

- 先利用第一个训练样本$x^{(1)}$计算出预测值$\hat{y}^{(1)}$；
- 用第二个样本$x^{(2)}$来计算出预测值$\hat{y}^{(2)}$。
- 以此类推，直到最后第m个样本，用$x^{(m)}$计算出$\hat{y}^{(m)}$。

表示方法：以激活函数表示法为例，$\alpha^{[2]}=\hat{y}$，它写成$a^{[2](1)}$, $a^{[2](2)}$, ..., $a^{[2](m)}$。

【注】：$a^{[2](i)}$，(i)是指第i个训练样本，[2]是指层数为第二层。

上述公式可以整理为：
$$
z^{[1](i)}=W^{[1](i)} x^{(i)}+b^{[1](i)} \\
a^{[1](i)}=\sigma\left(z^{[1](i)}\right) \\
z^{[2](i)}=W^{[2](i)} a^{[1](i)}+b^{[2](i)} \\
a^{[2](i)}=\sigma\left(z^{[2](i)}\right)
$$
可以看出(i)（i=1,2...m）是所有依赖于训练样本的变量。如果想计算m个训练样本上的所有输出，就应该向量化整个计算以简化。

#### 3.3.2 多样本向量化计算

定义矩阵$X$含有m个训练样本，则**每一列代表一个样本**，形成一个n×m的矩阵，即：

$$x=\left[\begin{array}{cccc}
\vdots & \vdots & \vdots & \vdots \\
x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\
\vdots & \vdots & \vdots & \vdots
\end{array}\right]$$

同理，定义矩阵$Z^{[1]}$的各列分别为$z^{[1](1)}$, $z^{[1](2)}$, ..., $z^{[1](m)}$的列向量，即：

$$Z^{[1]}=\left[\begin{array}{cccc}
\vdots & \vdots & \vdots & \vdots \\
z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)} \\
\vdots & \vdots & \vdots & \vdots
\end{array}\right]$$

通过这种定义方法，可以方便地利用矩阵对样本数据进行索引。

- **从水平上看，矩阵代表了各个训练样本。从竖直上看，矩阵的不同索引对应于不同的隐藏单元**。
- **单看矩阵中某一行，该行代表本层某一个神经单元，水平（列）索引分别对应第1个训练样本，第2个训练样本……直到最终第m个训练样本的激活值。**
- **单看矩阵中某一列，该列代表一个训练样本，垂直（行）索引对应于本层神经网络中的不同神经元。**

向量化实现的解释（Justification for vectorized implementation）

**为什么前面整理的公式就是将多个样本向量化的正确实现？**

运用线性代数的知识，以三个训练样本为例，手动计算前向传播的过程，得到如下规律：
$$
z^{[1](1)}=W^{[1]} x^{(1)}+b^{[1]} \\
z^{[1](2)}=W^{[1]} x^{(2)}+b^{[1]} \\
z^{[1](3)}=W^{[1]} x^{(3)}+b^{[1]}
$$
利用Python 的广播机制，可以很容易的将同等维度的$b^{[1]}$加进来，所以为了描述方便，我们可以先忽略公式中的$b^{[1]}$。

现在$W^{[1]}$是一个矩阵，$x^{(1)}$,$x^{(2)}$,$x^{(3)}$都是列向量，矩阵乘以列向量得到列向量，用图形直观的表示如下: 
![](https://upload-images.jianshu.io/upload_images/24435917-46b314558f67cddc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

用不同的颜色表示上图中不同的样本向量及其对应的输出可得到下图，更加直观地看出，当加入更多样本时，**仅相当于向矩阵$X$中加入更多列**。

![](https://upload-images.jianshu.io/upload_images/24435917-fbebc806ef5a084c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

而Python的广播机制对于这种矩阵与向量直接相加的处理方式是，将向量与矩阵的每一列相加。故矩阵$W^{[1]} X$与向量$b^{[1]}$相加得到的就是$Z^{[1]}$。

由此说明了为什么公式$Z^{[1]}=W^{[1]} X+b^{[1]}$是前向传播的第一步计算的正确向量化实现。

使用向量化的方法，可以不需要for循环，而直接通过矩阵运算就可由$X$直接计算出$A^{[1]}$，X也可以记为$A^{[0]}$，使用同样的方法就可以由神经网络中的每一层的输入$A^{[i-1]}$ 计算输出$A^{[i]}$。

### 3.4 激活函数

<font color=red>补充下列激活函数的pytorch实现、适用场景及优缺点</font>

pytorch在torch.nn.functional中实现了大部分激活函数。

#### 3.4.1 几种常用的激活函数

**1. sigmoid:** 
 $$ a=\sigma(z)=\frac{1}{1+e^{-z}} $$
![](https://upload-images.jianshu.io/upload_images/24408091-b46793f797ed2c38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

二分类问题中，由于**输出层**${y}$的值为0或1，为了让$\hat{y}$的数值介于0和1之间，需使用sigmoid激活函数。

pytorch实现：

```python
def sigmoid(input):
    r"""sigmoid(input) -> Tensor

    Applies the element-wise function :math:`\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}`

    See :class:`~torch.nn.Sigmoid` for more details.
    """
    warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
    return input.sigmoid()
```

**优点：**

- Sigmoid 函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层。
- 它在物理意义上最为接近生物神经元。求导容易。

**缺点：**

- 由于其软饱和性，容易产生梯度消失，导致训练出现问题。
- 其输出并不是以0为中心的。
- 计算量大，收敛缓慢。

**2. tanh:**
$$a=tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$
![](https://upload-images.jianshu.io/upload_images/24408091-af9bc1dc953720ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
效果总是优于sigmoid函数，因为其函数值域在-1和1之间，均值接近零，中心化数据，使下一层学习更简单。

>sigmoid函数和tanh函数的共同缺点：在z很大或很小的情况下，导数的梯度变得很小，最后接近于0，导致梯度下降速度降低。

pytorch实现：

```python
def tanh(input):
    r"""tanh(input) -> Tensor

    Applies element-wise,
    :math:`\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}`

    See :class:`~torch.nn.Tanh` for more details.
    """
    warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
    return input.tanh()
```

**优点：**

- 比Sigmoid函数收敛速度更快。
- 相比Sigmoid函数，其输出以0为中心。

**缺点：**

- 还是没有改变Sigmoid函数的最大问题——由于饱和性产生的梯度消失。

**3. 修正线性单元(ReLU):**
$$a=\max (0, z)$$
![](https://upload-images.jianshu.io/upload_images/24408091-3526749425eb736f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

ReLU的一个缺点是学习率过高时神经元可能会坏死（Dying ReLU），原理可参考：https://blog.csdn.net/weixin_42033436/article/details/105331510，可以通过选择较小的学习率或换用leaky ReLU、ELU、PReLU等激活函数以改进。<font color=red>这里的参考文献换一个吧，这个看的太难受了</font>

但在实际中，虽然在z为负值时ReLU的梯度一半为0，但有足够多的隐藏单元使z值大于0，故对大多数训练数据来说学习过程仍然很快。

pytorch实现：

```python
def relu(input, inplace=False):
    if inplace:
        return torch.relu_(input)
    return torch.relu(input) 
```

**优点：**

- 相比起Sigmoid和tanh，ReLU在SGD中能够快速收敛。据称，这是因为它线性、非饱和的形式。
- Sigmoid和tanh涉及了很多很expensive的操作（比如指数），ReLU可以更加简单的实现。
- 有效缓解了梯度消失的问题。
- 在没有无监督预训练的时候也能有较好的表现。
- 提供了神经网络的稀疏表达能力。

**缺点：**

- 随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。
- 如果发生这种情况，那么流经神经元的梯度从这一点开始将永远是0。也就是说，ReLU神经元在训练中不可逆地死亡了。

**4. leaky ReLU:**
$$a=\max (0.01 z, z)$$
![](https://upload-images.jianshu.io/upload_images/24408091-51c3d9fbc27ccbb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Leaky ReLU有两个好处：

- 它没有零斜率部分，因此可以解决“Dying ReLU”问题。
- 它加快了训练速度。有证据表明，“平均激活”接近0可使训练更快。与ReLU不同，Leaky ReLU更“平衡”，因此学习起来可能更快。

Leaky ReLU通常比ReLU激活函数效果好，但实际使用不多。

> ReLU和leaky ReLU的共同优点：1. 在z区间变动很大的情况下，其导数都会远大于0。使用ReLU激活函数神经网络通常比sigmoid或tanh激活函数学习的快。2.sigmoid和tanh函数的导数在正负饱和区梯度都接近于0，造成梯度弥散，而ReLU和leaky ReLU函数大于0部分导数都为常数，不会产生梯度弥散现象。

pytorch实现：

```python
def leaky_relu(input, negative_slope=0.01, inplace=False):
    r"""
    leaky_relu(input, negative_slope=0.01, inplace=False) -> Tensor

    Applies element-wise,
    :math:`\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)`

    See :class:`~torch.nn.LeakyReLU` for more details.
    """
    if inplace:
        return torch._C._nn.leaky_relu_(input, negative_slope)
    return torch._C._nn.leaky_relu(input, negative_slope)
```

**总结**
**sigmoid** 激活函数：一般只用于二分类问题的输出层。
**tanh** 激活函数：几乎在所有场合中都比sigmod函数更优越。
**ReLU** 激活函数：最常用的、默认的激活函数，若不确定使用哪个函数，就使用**ReLU** 或者 **leaky ReLU**。

#### 3.4.2 其他激活函数

**1. softmax函数：**

$$\sigma(z)_{j}=\frac{e^{Z_{j}}}{\sum_{k=1}^{K} e^{Z_{k}}}$$

Sigmoid函数实际上就是把数据映射到一个(0,1)的空间上，也就是说，Sigmoid函数如果用来分类的话，只能进行二分类，而这里的softmax函数可以看做是Sigmoid函数的一般化，可以进行多分类。

**2. softplus函数**

$a=log(1+e^z)$

softplus函数和ReLU函数的曲线如下：

![](https://upload-images.jianshu.io/upload_images/24408463-e2ee79abcd90f2e0.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

可以看到，softplus可以看作是ReLu的平滑。根据神经科学家的相关研究，softplus和ReLu与脑神经元激活频率函数有神似的地方。也就是说，相比于早期的激活函数，softplus和ReLu更加接近脑神经元的激活模型，而神经网络正是基于脑神经科学发展而来，这两个激活函数的应用促成了神经网络研究的新浪潮。

**3. PReLU函数**（参数化修正线性单元）

$$a=\max (\alpha z, z)$$

![](https://img-blog.csdnimg.cn/202004051921270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjAzMzQzNg==,size_16,color_FFFFFF,t_70)

PReLU函数也是针对ReLU的一个改进型，在负数区域内，PReLU有一个很小的斜率，这样也可以避免ReLU死掉的问题。相比于ELU，PReLU在负数区域内是线性运算，斜率虽然小，但是不会趋于0，这算是一定的优势吧。

PReLU的公式里面的参数α是根据数据来定的，一般是取0~1之间的数，而且一般还是比较小的，如零点零几。当α=0.01时，PReLU就成为Leaky ReLU。

pytorch实现：

```python
def prelu(input, weight):
    r"""prelu(input, weight) -> Tensor

    Applies element-wise the function
    :math:`\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)` where weight is a
    learnable parameter.

    See :class:`~torch.nn.PReLU` for more details.
    """
    return torch.prelu(input, weight)
```

**4.  RReLU函数**（随机修正线性单元）

![](http://p0.ifengimg.com/pmop/2017/0701/B3F2F3EA627EBB55D88C8F8FB36942C56B350A4B_size14_w740_h221.jpeg)



RReLU是Leaky ReLU的random版本，在训练过程中，α是从一个高斯分布 $U(l,u)$ 中随机出来的值，然后再在测试过程中进行修正。在测试阶段，把训练过程中所有的 $\alpha _{ij}$ 取个平均值。

下图是ReLU、Leaky ReLU、PReLU和RReLU的比较：

![](http://p0.ifengimg.com/pmop/2017/0701/C56E5C6FCBB36E70BA5EBC90CBD142BA320B3DF6_size19_w740_h217.jpeg)

**总结：**

（1）PReLU中的α是根据数据变化的；

（2）Leaky ReLU中的α是固定的；

（3）RReLU中的α是一个在给定范围内随机抽取的值，这个值在测试环节就会固定下来。

**5. ELU函数：**（指数线性单元）
$$a=\left\{\begin{array}{cc}
z & , z>0 \\
\alpha\left(e^{z}-1\right) & , z \leq 0
\end{array}\right.$$

类似于Leaky ReLU，ELU的负值斜率较小。它使用如下所示的对数曲线代替直线：

![](https://img-blog.csdnimg.cn/20200405192225656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjAzMzQzNg==,size_16,color_FFFFFF,t_70)

ELU函数是针对ReLU函数的一个改进型，设计目的是结合ReLU的线性部分和Leaky ReLU。相比于ReLU函数，在输入为负数的情况下，是有一定的输出的，而且这部分输出还具有一定的抗干扰能力，这样可以消除ReLU死掉的问题，不过还是有梯度饱和和指数运算的问题，在大的负数部分梯度为0，导致神经元不活跃。

pytorch实现：

```python
def elu(input, alpha=1., inplace=False):
    r"""Applies element-wise,
    :math:`\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))`.

    See :class:`~torch.nn.ELU` for more details.
    """
    if inplace:
        return torch._C._nn.elu_(input, alpha)
    return torch._C._nn.elu(input, alpha)
```

**6. SELU函数：**

$$a=\lambda \left\{\begin{array}{cc}
z & , z>0 \\
\alpha\left(e^{z}-1\right) & , z \leq 0
\end{array}\right.$$

经过该激活函数后使得样本分布自动归一化到0均值和单位方差(自归一化，保证训练过程中梯度不会爆炸或消失，效果比Batch Normalization 要好) 。
​其实就是ELU乘了个 $\lambda$，关键在于这个 $\lambda$ 是大于1的。前面的ReLU、PReLU、ELU这些激活函数，都是在负半轴坡度平缓，这样在activation的方差过大的时候可以让它减小，防止了梯度爆炸，但是正半轴坡度简单的设成了1。而SELU的正半轴大于1，在方差过小的的时候可以让它增大，同时防止了梯度消失。这样激活函数就有一个不动点，网络深了以后每一层的输出都是均值为0方差为1。

pytorch实现：

```python
def selu(input, inplace=False):
    r"""selu(input, inplace=False) -> Tensor

    Applies element-wise,
    :math:`\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))`,
    with :math:`\alpha=1.6732632423543772848170429916717` and
    :math:`scale=1.0507009873554804934193349852946`.

    See :class:`~torch.nn.SELU` for more details.
    """
    if inplace:
        return torch.selu_(input)
    return torch.selu(input)
```

**7. MaxOut函数**

![](https://upload-images.jianshu.io/upload_images/4155986-1b5bc20ed6ee1a1f.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

这里的W是3维的，尺寸为d×m×k，其中d表示输入层节点的个数，m表示隐含层节点的个数，k表示每个隐含层节点对应了k个”隐隐含层”节点，这k个”隐隐含层”节点都是线性输出的，而maxout的每个节点就是取这k个”隐隐含层”节点输出值中最大的那个值。因为激发函数中有了max操作，所以整个maxout网络也是一种非线性的变换。

maxout的拟合能力非常强，可以拟合任意的凸函数(包括ReLU)。

**8. Swish函数**

$a=z * sigmoid(z)$

其变形Swish-B激活函数的公式：$a=z * sigmoid(b * z)$

![](https://img-blog.csdn.net/20171126110946276)

Swish是Google提出的一种新型激活函数，拥有不饱和、光滑、非单调性的特征，而Google在论文中的多项测试表明Swish以及Swish-B激活函数的性能极佳，在不同的数据集上都表现出了要优于当前最佳激活函数的性能。

#### 3.4.3 如何选择激活函数

在实践过程中选择激活函数时更多还是需要结合实际情况，考虑不同激活函数的优缺点综合使用，这里提出一点在训练模型时选择激活函数的建议。

（1）首先尝试ReLU，速度快，但要注意训练的状态。

（2）如果ReLU效果欠佳，尝试Leaky ReLU或Maxout等变种。

（3）尝试tanh正切函数(以零点为中心，零点处梯度为1)。

（4）sigmoid/tanh在RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值。

（5）在浅层神经网络中，如不超过4层的，可选择使用多种激励函数，没有太大的影响。

#### 3.4.4 输出层可以不加激活函数吗？

​		输入层和隐藏层的激活函数参与了整个前向和反向传播，相当于加入了非线性因素，如果在输入和隐藏层没有激活函数，那么整个神经网络相当于只是一个矩阵的运算，只能得到一些线性组合。但对于输出层来说，加不加激活函数对前向和反向传播其实没有任何影响，激活函数的作用相当于整形，故一般输出层可以不加激活函数。但是，当需要控制输出在一定范围（sigmoid）或者希望输出满足一定的分布（softmax）再或者神经网络的输出层后面还要接其他的东西（比如GAN中生成器网络的输出还要接判别器），就必须在输出层加激活函数。

### 3.5 为什么需要非线性激活函数

#### 3.5.1 线性分类器和非线性分类器

**线性分类器**：只考虑二类的情形，所谓线性分类器即用一个**超平面**将正负样本分离开，模型是参数的线性函数，如$y=wx$，在线性分类器的基础上，用分段线性分类器可以实现更复杂的分类面。

常见的线性分类器：LR，贝叶斯分类，单层感知机、线性回归，SVM（线性核）等。
优缺点：算法简单并具有“学习”能力。线性分类器速度快、编程方便，但可能拟合效果不会很好。

**非线性分类器**：模型分界面可以是**曲面**或者**超平面的组合**。非线性判别函数解决比较复杂的线性不可分样本分类问题，解决问题比较简便的方法是采用多个线性分界面将它们分段连接，用分段线性判别划分去逼近分界的超曲面。

常见的非线性分类器：决策树、RF、GBDT、多层感知机、SVM（高斯核）等。
优缺点：非线性分类器编程复杂，但是效果拟合能力强。

#### 3.5.2 线性激活函数和非线性激活函数

线性激活函数也称为恒等激活函数，假设使用的线性激活函数为$g(z)=z$，即把$a^{[1]}=z^{[1]}$, $a^{[2]}=z^{[2]}$代入前面单隐层神经网络的计算公式中，有：

(1) $a^{[1]}=z^{[1]}=W^{[1]} x+b^{[1]}$
(2) $a^{[2]}=z^{[2]}=W^{[2]} a^{[1]}+b^{[2]}$ 
将(1)式带入(2)式中，则：
$$
a^{[2]}=z^{[2]}=W^{[2]}\left(W^{[1]} x+b^{[1]}\right)+b^{[2]}=W^{[2]} W^{[1]} x+W^{[2]} b^{[1]}+b^{[2]}
$$
化简得$a^{[2]}=z^{[2]}=W^{\prime} x+b^{\prime}$

由上式发现，若使用线性激活函数，神经网络只是把输入线性组合再输出，无论神经网络有多少层都一直只是在计算线性函数，线性隐藏层一点用也没有，模型复杂程度和没有任何隐藏层的标准 logistic 回归相同。

当要机器学习的是回归问题时，如果${y}$为实数，在输出层用线性激活函数也许可行，此时输出也是一个实数，从负无穷到正无穷。

**结论**
不能在隐藏层用线性激活函数，可用**ReLU** 、**tanh** 或 **leaky ReLU** 等非线性激活函数。唯一可用线性激活函数的就是输出层。

### 3.6 激活函数的导数

<font color=red>下面的对齐调整一下</font>

在神经网络中使用反向传播时，需要计算激活函数的导数。
1. **sigmoid:** $g(z)=\frac{1}{1+e^{-z}}$

![](https://upload-images.jianshu.io/upload_images/24408091-9a5fa6145ba979dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
求导公式：
$$
\frac{d}{d z} g(z)=\frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)=g(z)(1-g(z))
$$
在神经网络中
$$
a=g(z)\\
g^{\prime}(z)=\frac{d}{d z} g(z)=a(1-a)
$$

2. **tanh:** $g(z)=\tanh (z)$ $=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$
![](https://upload-images.jianshu.io/upload_images/24408091-4c16ab4cb118e005.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

求导公式：
$$
\frac{d}{d z} g(z)=1-(\tanh (z))^{2}
$$
在神经网络中
$$
a=g(z) \\
g(z)^{\prime}=\frac{d}{d z} g(z)=1-a^{2}
$$

3.  **ReLU:**$g(z)=\max (0, z)$
![](https://upload-images.jianshu.io/upload_images/24408091-7e9670d58cfa0e42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

$$
g(z)^{\prime}=\left\{\begin{array}{ll}
0 & \text { if } z<0 \\
1 & \text { if } z>0 \\
undefined & \text { if } z=0
\end{array}\right.
$$

通常在 $z = 0 $时，可以给定其导数为 1 或 0，一般$𝑧=0 $的情况很少。

4. **leaky ReLU:** $g(z)=\max (0.01 z, z)$
    ![](https://upload-images.jianshu.io/upload_images/24408091-508742c3269c915c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
$$
  g(z)^{\prime}=\left\{\begin{array}{ll}
  0.01 & \text { if } z<0 \\
  1 & \text { if } z>0 \\
  undefined & \text { if } z=0
  \end{array}\right.
$$
  通常在 $z = 0 $时，可以给定其导数为 1 或 0.01，一般$𝑧=0 $的情况很少。

### 3.7 神经网络的梯度下降法
首先，简单回顾一下单隐层网络的相关参数：$$W^{[i]}, b^{[i]}, n^{[i]}$$
分别表示神经网络第$i$层的权值矩阵、偏置和节点数。
其中权值矩阵和偏置的shape分别为：$$(n^{[i]}, n^{[i-1]}), (n^{[i]}, 1)$$
**cost函数**：
$$
J(W^{[1]}, b^{[1]}, W^{[2]}b^{[2]}) = \frac{1}{m} \sum_{i=1}^{n} L(\hat{y}, y)
$$
其中的 $\hat{y}$ 又等于 $a^{[2]}$。

**梯度下降**(训练参数)：
(1) 随机初始化
在训练神经网络时，随机初始化参数很重要，而不是全初始化为0。把参数初始化为某些值后，每个梯度下降循环都会计算预测值，所以基本上需要计算i=1到m的预测值$\hat y$。

(2) 计算导数：
$$
dW^{[i]}=\frac{\partial J}{\partial W^{[i]}} \\
db^{[i]}=\frac{\partial J}{\partial b^{[i]}}
$$
(3) 更新参数：
$$
W^{[i]}=W^{[i]}-\alpha \cdot d W^{[i]} \\
b^{[i]}=b^{[i]}-\alpha \cdot d b^{[i]}
$$
重复以上步骤(2)、(3)直至参数收敛。

单隐层神经网络的计算过程如下图所示：
![](https://upload-images.jianshu.io/upload_images/16793245-a1c9f6bc82c9b83a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 3.7.1 正向传播 ####
正向传播比较简单，直接代入已知数值按步骤计算就可以了。公式如下：
$$
Z^{[1]}=W^{[1]}X+b^{[1]},A^{[1]}=g^{[1]}(Z^{[1]})\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]},A^{[2]}=g^{[2]}(Z^{[2]})
$$
对于二分类模型来说，其中的$g^{[2]}(Z^{[2]})=\sigma(Z^{[2]})$
值得注意的是，此处用的是大写字母Z和A，和图片不一致。意味着这是将所有样本**向量化**堆叠在一起了。下文也应当注意到大小写的区别。

#### 3.7.2 反向传播 ####
反向传播则是指梯度的从后往前计算。公式如下：
$$
dZ^{[2]}=A^{[2]}-Y \\
dW^{[2]}=\frac{1}{m} dZ^{[2]}A^{[1]T} \\
db^{[2]}=\frac{1}{m} np.sum(dZ^{[ 2 ]}, axis=1, keepdims=True) \\

dZ^{[1]}=W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]}) \\
dW^{[1]}=\frac{1}{m} dZ^{[1]}X^{T} \\
db^{[1]}=\frac{1}{m} np.sum(dZ^{[1]}, axis=1, keepdims=True)
$$
其中$Y=[y^{(1)}, y^{(2)}, ..., y^{(m)}]$，$g^{[1]'}$为激活函数$g^{[1]}$的导数。

不难发现，$dW$和$db$的计算相当相似，可能有人想那是不是也可以向量化来减少代码量？但是向量化的基本前提是两个向量的shape要一致，但这里很明显不符合。

### 3.8 直观理解反向传播 ###
#### 3.8.1 单个样本的梯度

对于单个样本，$dz^{[2]}$的计算可以回顾前面的logistic回归，简单来说是先计算$a^{[2]}$，再计算$dz^{[2]}$，此处不再赘述。
如果你有微积分相关知识的基础，那理解$dW, db$的计算应该也不是很困难，而且无论哪一层都有着类似的计算。
那最难理解的部分可能是$dz^{[1]}$的计算。首先根据链式法则，易得：
$$
dz^{[1]}=\frac{dL}{dz^{[2]}}\frac{dz^{[2]}}{da^{[1]}}\frac{da^{[1]}}{dz^{[1]}}
$$
然后将已经计算得到结果的：
$$
\frac{dL}{dz^{[2]}}=dz^{[2]}
$$
和
$$
\frac{dz^{[2]}}{da^{[1]}}=W^{[2]} \\
\frac{da^{[1]}}{dz^{[1]}}=g^{[1]'}(z^{[1]})
$$
代入，就可得到目标式：
$$
dz^{[1]}=W^{[2]T}dz^{[2]}*g^{[1]'}(z^{[1]})
$$

#### 3.8.2 多样本向量化 ####
刚刚是针对单个样本，现在给出向量化后的**合理性**，不做过多解释。
向量化后的$Z$:
$$
Z^{[1]}=[z^{[1](1)}, z^{[1](2)}, ..., z^{[1](m)}]
$$
是将m个样本（本来是列向量）按“行向量”组合成了“矩阵”。
用shape来验证一下对不对：
$$
x:(n^{[0]}, 1), W^{[1]}:(n^{[1]}, n0), b^{[1]}:(n^{[1]}, 1) \\
W^{[1]}x+b^{[1]}=z^{[1]}:(n^{[1]}, 1)
$$

$$
X:(n^{[0]},m), W^{[1]}:(n^{[1]}, n0), b^{[1]}:(n^{[1]}, 1) \\
W^{[1]}X+b^{[1]}=Z^{[1]}:(n^{[1]}, m)
$$

看起来确实符合，由原来的1列堆叠成了m列。相应的，$A^{[1]}$也为堆叠之后的$(n^{[1]}, m)$。对于$Z^{[2]}, A^{[2]}$同理，可自行推导。

### 3.9 随机初始化 ###

<font color=red>写一个简单的测试代码测试一下不同的初始化方法对性能的影响</font>

在学习logistic回归时，我们初始化权值矩阵和偏置的时候都是初始为0的。在logistic回归中，这么做确实没什么问题；但在更复杂、更深的神经网络中，统一的初始化会导致学习效果差很多。接下来分析原因并介绍其他的初始化做法。

#### 3.9.1 原因 ####
在神经网络中，一个权值矩阵$W$，每一行都可以视为一个“特征提取器”。而由于对称性，如果每一行都初始化为一样的值，那会导致每一个特征提取器都在计算一模一样的特征，即每一行的结果都一致，每一次梯度更新也一致，最终导致这个权值矩阵只有一行是在进行有效计算的，其余都是**完全**重复的无用功。可以发现：
$$
\text { dW }=\left[\begin{array}{cc}
u & v \\
u & v
\end{array}\right]
$$
不难想象，这样下去会导致网络的学习效果很差。
#### 3.9.2 解决办法——随机初始化 ####
对于$W$，初始化必须要**破坏对称性**才能让权值矩阵发挥真正的作用，而对于偏置$b$则没有要求（原因下面讲）。
给出的代码用的是Python3，Python2可能会有所差别。

```python
# randn参数为对应的轴的维数，参数格式和zeros区分开开
W = np.random.randn(2, 2)*0.01
# zeros的参数为一个list或者tuple，相当于shape
b = np.zeros((2, 1))
```
乘0.01是因为要把W随机初始化到一个相对较小的值，因为如果X很大的话，W又相对较大，会导致Z非常大，这样如果激活函数是sigmoid，就会导致sigmoid的输出值1或者0，然后会导致一系列问题（比如cost function计算的时候，log里是0，这样会有点麻烦）。随机初始化后，cost function随着迭代次数的变化示意图为：

![](https://upload-images.jianshu.io/upload_images/24408463-e70635688309ffec.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

能够看出，cost function的变化是比较正常的。但是随机初始化也有缺点，np.random.randn()其实是一个均值为0，方差为1的高斯分布中采样。当神经网络的层数增多时，会发现越往后面的层的激活函数（使用tanH）的输出值几乎都接近于0，如下图所示：

![](https://upload-images.jianshu.io/upload_images/24408463-1479075150947769.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

激活函数输出值接近于0会导致梯度非常接近于0，因此会导致梯度消失。

关于$b$可以初始化为0的解释：经过特征提取后，每一行为一个特征，偏置的作用就是针对每个特征（每一行）进行一次“偏移修正”。那只要保证每一行计算出来的特征不一致，就可以保证每一行的更新也不一致。

#### 3.9.3 其他初始化方法

**1. Xavier 初始化 **

基本思想：尽可能的让输入和输出服从相同的分布，这样就能够避免后面层的激活函数的输出值趋向于0。虽然刚开始的推导基于线性函数，但是在一些非线性神经元也很有效。

Xavier 初始化代码：

```python
tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in)
```

来看下Xavier initialization后每层的激活函数输出值的分布：

![](https://upload-images.jianshu.io/upload_images/24408463-e8c8fc89b9f80cde.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

能够看出，深层的激活函数输出值还是非常漂亮的服从标准高斯分布。虽然Xavier initialization能够很好地配合tanH 激活函数输出，但是对于目前神经网络中最常用的ReLU激活函数，还是无能为力，请看下图：

![](https://upload-images.jianshu.io/upload_images/24408463-042b50c2ba6a8528.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

当达到5，6层后几乎又开始趋向于0，更深层的话很明显又会趋向于0。

**2. He initialization**

为了解决上面的问题，何恺明大神提出了一种针对ReLU的初始化方法，一般称作 He initialization。初始化代码为：

```python
tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in/2)
```

经过He initialization后，当隐藏层使用ReLU时，激活函数的输出值的分布情况：

![](https://upload-images.jianshu.io/upload_images/24408463-042b50c2ba6a8528.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

效果是比Xavier initialization好很多。**现在神经网络中，隐藏层常使用ReLU，权重初始化常用He initialization方法。**

> 参考文献：https://blog.csdn.net/u012328159/article/details/80025785?utm_medium=distribute.pc_relevant.none-task-blog-title-1&spm=1001.2101.3001.4242

#### 3.9.4 PBT算法（Population Based Training，基于群体的训练）

神经网络在某个特定应用中的成功通常取决于在研究初始时所做的一系列选择，包括选择要使用的网络类型以及用于训练的数据和方法。目前，这些选择（被称作超参数）是通过经验、随机搜索或计算密集型的搜索过程来实现的。

DeepMind在最新的一篇论文 *Population Based Training of Neural Networks* 中，提出了一种新的训练神经网络的方法PBT，这是一种异步优化算法，它同时训练和优化一个群体的网络，从而快速地为任务选择最佳的超参数集合和模型。最重要的是，这种方法不会增加计算开销，能够最大限度地提高性能，并且很容易集成到现有的机器学习流程中。DeepMind认为这一方法有很大潜力。

PBT技术是两种最常用的超参数优化方法的结合：**随机搜索**和**手动调优**。

在随机搜索中，神经网络群体(population) 被并行地独立训练，在训练结束时，选择性能最优的模型。通常情况下，这意味着只有群体的一小部分会得到良好的超参数训练，更多的部分得到不好的超参数训练，浪费计算资源。

![](https://upload-images.jianshu.io/upload_images/24408463-fac1cb46b2ac188a.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)



通过手动调优，研究人员必须猜测哪些是最好的超参数，使用它们来训练模型，然后对性能进行评估。这个过程一遍遍地重复，直到研究人员对网络的性能感到满意为止。尽管这可能会带来更好的性能，但缺点是需要很长时间，有时需要几周甚至几个月才能找到完美的设置。虽然有一些方法可以自动化这个过程，例如贝叶斯优化，但是仍然需要很长的时间，并且需要大量的连续训练才能找到最好的超参数。

![](https://upload-images.jianshu.io/upload_images/24408463-d2c3dcae339de693.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)



PBT，就像随机搜索，一开始是以随机的超参数并行地训练许多神经网络。但是，这些网络不是独立的，而是使用来自群体（population）中其余部分的信息来调优超参数，并且将计算资源引导到有希望的模型。这是从遗传算法中获得启发的，在遗传算法中，population中的每个成员（被称为worker）都可以利用其他成员的信息。例如，一个worker可以从一个性能更好的worker中拷贝模型参数，也可以通过随机改变当前的值来探索新的超参数。

随着神经网络population的训练继续进行，这种开发和探索的过程也周期性地进行，确保population中所有的worker都有一个良好的基础性能水平，并且一直不断地探索新的超参数。这意味着PBT可以快速利用良好的超参数，可以将更多训练时间分配给有希望的模型，并且，关键的是，可以在整个训练过程中调整超参数值，从而自动学习最佳配置。

![](https://upload-images.jianshu.io/upload_images/24408463-e72d1488633f19ae.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

实验表明，PBT在众多任务和领域都非常有效。DeepMind认为，这项技术潜力很大。他们还发现PBT对于训练**引入新的超参数的新算法和神经网络架构**特别有用。随着这个过程的不断改进，PBT有望寻找和开发更复杂、更强大的神经网络模型。

> 参考文献：https://www.sohu.com/a/207107606_473283

### 3.10 如何用神经网络实现XOR分类

**什么是异或(XOR)：**
在数字逻辑中，异或是对两个运算元的一种逻辑分析类型，符号为XOR或EOR或⊕。当两两数值相同时为否，而数值不同时为真。异或的真值表如下：
![](https://p0.ssl.img.360kuai.com/t016e9cf3413274f359.webp)
**如何用神经网络实现异或(XOR)：**
神经网络应用在分类问题中效果很好，逻辑回归可以通过**改变参数**来实现“与”、“或”、“非”等简单操作。
在吴恩达老师的【机器学习】课程第68、70课时中，使用了AND(与)、NOR(或非)和OR(或)的组合实现了XNOR(同或)，由于用神经网络实现异或(XOR)与同或(XNOR)的过程是相同的只是输出结果互为相反数，故在此讨论用神经网络实现XNOR(同或)的具体过程：

1. AND运算神经网络
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5rr5ZGbvHqAI1WYfIvA4DI1pYEgfvobysuD8rUI72542oT3oJnRb*HC9OZkABHupkvl35xh44XG8G2ACXjqIsHA!/b&bo=AgGLAAIBiwADCSw!&rf=viewer_4)
模型函数如下：
$$h_{\theta}(x)=g\left(-30+20 x_{1}+20 x_{2}\right)=\frac{1}{1+e^{-(-30+20 x_{1}+20 x_{2})}}$$
结果：
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcYiNBcz7No1IR.Drf6fnOs1vWtDIV.K05p50.fXdEN7av*d7HGoxmtNdwDBAoj.k0NvkTDwe3S8J5XXA6Ul2ycg!/b&bo=KQEhASkBIQEDGTw!&rf=viewer_4)
2. NOR(或非)运算神经网络
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcVqsKjONzu7mjtQrIpU3RbgoSvkv44FXSToJ8SM..0ZY7*5qY0MePsXbF6VRkzVcamaahE1OXfF.fKs3Rnjug8w!/b&bo=tQBoALUAaAADGTw!&rf=viewer_4)
模型函数如下：
$$h_{\theta}(x)=g\left(-10-20 x_{1}-20 x_{2}\right)=\frac{1}{1+e^{-(-10-20 x_{1}-20 x_{2})}}$$
结果：
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcVqsKjONzu7mjtQrIpU3Rbj.8JS7uBKTyX6zPflrK3t51TZK4G5rzx6rFv4oY3eHFeWCjenvQcpGpldmK50*R0A!/b&bo=2wHFANsBxQADGTw!&rf=viewer_4)
3. OR运算神经网络
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcVqsKjONzu7mjtQrIpU3RbhuhKzyCNPNpAg6rV9sby1qc.VbzbOKk3DIOf5IHjbiyAnT7B9RYRUYWOKJc3rRINw!/b&bo=9ACLAPQAiwADGTw!&rf=viewer_4)
模型函数如下：
$$h_{\theta}(x)=g\left(-10+20 x_{1}+20 x_{2}\right)=\frac{1}{1+e^{-(-10+20 x_{1}+20 x_{2})}}$$
结果：
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcVqsKjONzu7mjtQrIpU3RbgQ4l.G1orM5Rq7uuaYlc*frToz2g4hjiWbCQYHXqfiyyOMR3NCo99*Ci6WW0WVFkQ!/b&bo=2AG*ANgBvwADGTw!&rf=viewer_4)
4、构建XNOR网络(异或)：
$$A \odot B=A B+\overline{A B}=A B+\overline{A+B}$$
故可在输入层输入特征值A（$a_1^{(1)}$）和B（$a_2^{(1)}$），并假设$a_0^{(1)}=1$，三者同时输入隐藏层第一个神经元实现A和B的与运算得到$a_1^{(2)}$，同时输入隐藏层第二个神经元实现A和B的或非运算得到$a_2^{(2)}$，在隐藏层假设$a_0^{(2)}=1$，三者同时输入输出层神经单元实现或运算得到$a_1^{(3)}$，即神经网络实现同或运算的最终输出结果$h_{\theta}(x)$。具体神经网络结构如下图所示：
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mce5is5SjellblXQNkx859nYgVkKvVrckRF77OGAIcQS3FllFEcSPuf0jesfZWjJwl1lIvUv8XOTs1pjq26QdAOE!/b&bo=SARMAkgETAIDGTw!&rf=viewer_4)
运算结果如下：
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mce5is5SjellblXQNkx859na.LKI*vKzkMpkhfXIkSWz*2DRQe92cumPAgWbmFiGkoC07iWcDGPGuyDziN5Q4EHs!/b&bo=OgG.ADoBvgADGTw!&rf=viewer_4)
在同或神经网络后再加入实现“非”运算的神经元，如下图所示，即可实现异或运算。
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mce5is5SjellblXQNkx859nZyqSp5GVCoN64JCkhe5UFVWTYG4izLVPUbgG09Tu0Vf3mZRD6gTNjSNu.FrQZN3Bo!/b&bo=EQKlABECpQADGTw!&rf=viewer_4)



下面采用激活函数为tanh(x)的三层网络来解决异或问题(当激活函数为奇函数时，BP 算法的学习速度要快一些，最常用的奇函数是双曲正切函数)。

**代码实现：**

```python
import numpy as np

# 双曲正切函数,该函数为奇函数
def tanh(x):    
    return np.tanh(x)

# tanh导函数性质:f'(t) = 1 - f(x)^2
def tanh_prime(x):      
    return 1.0 - tanh(x)**2

# 构建神经网络
class NeuralNetwork:
    def __init__(self, layers, activation = 'tanh'):
        """
        :参数layers: 神经网络的结构(输入层-隐含层-输出层包含的结点数列表)
        :参数activation: 激活函数类型
        """
        if activation == 'tanh':    # 也可以用其它的激活函数
            self.activation = tanh
            self.activation_prime = tanh_prime
        else:
            pass

        # 存储权值矩阵
        self.weights = []

        # range of weight values (-1,1)
        # 初始化输入层和隐含层之间的权值
        for i in range(1, len(layers) - 1):
            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1     # add 1 for bias node
            self.weights.append(r)

        # 初始化输出层权值
        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1
        self.weights.append(r)


    def fit(self, X, Y, learning_rate=0.2, epochs=10000):
        # Add column of ones to X
        # This is to add the bias unit to the input layer
        X = np.hstack([np.ones((X.shape[0],1)),X])


        for k in range(epochs):     # 训练固定次数
            if k % 1000 == 0: print ('epochs:', k)

            # Return random integers from the discrete uniform distribution in the interval [0, low).
            i = np.random.randint(X.shape[0],high=None)
            a = [X[i]]   # 从m个输入样本中随机选一组

            for l in range(len(self.weights)):
                    dot_value = np.dot(a[l], self.weights[l])   # 权值矩阵中每一列代表该层中的一个结点与上一层所有结点之间的权值
                    activation = self.activation(dot_value)
                    a.append(activation)

            # 反向递推计算delta:从输出层开始,先算出该层的delta,再向前计算
            error = Y[i] - a[-1]    # 计算输出层delta
            deltas = [error * self.activation_prime(a[-1])]

            # 从倒数第2层开始反向计算delta
            for l in range(len(a) - 2, 0, -1):
                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))


            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]
            deltas.reverse()    # 逆转列表中的元素


            # backpropagation
            # 1. Multiply its output delta and input activation to get the gradient of the weight.
            # 2. Subtract a ratio (percentage) of the gradient from the weight.
            for i in range(len(self.weights)):  # 逐层调整权值
                layer = np.atleast_2d(a[i])     # View inputs as arrays with at least two dimensions
                delta = np.atleast_2d(deltas[i])
                self.weights[i] += learning_rate * np.dot(layer.T, delta) # 每输入一次样本,就更新一次权值

    def predict(self, x):
        a = np.concatenate((np.ones(1), np.array(x)))       # a为输入向量(行向量)
        for l in range(0, len(self.weights)):               # 逐层计算输出
            a = self.activation(np.dot(a, self.weights[l]))
        return a
    
    
if __name__ == '__main__':
    nn = NeuralNetwork([2,2,1])     # 网络结构: 2输入1输出,1个隐含层(包含2个结点)

    X = np.array([[0, 0],           # 输入矩阵(每行代表一个样本,每列代表一个特征)
                  [0, 1],
                  [1, 0],
                  [1, 1]])
    Y = np.array([0, 1, 1, 0])      # 期望输出

    nn.fit(X, Y)                    # 训练网络

    print ('w:', nn.weights)          # 调整后的权值列表

    for s in X:
        print(s, nn.predict(s))     # 测试
```

测试结果：

![](https://upload-images.jianshu.io/upload_images/24408463-67602774564737de.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)



- 
