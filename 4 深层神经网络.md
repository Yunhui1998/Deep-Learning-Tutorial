## 4. 深层神经网络

### 4.1 深层神经网络 ###

#### 4.1.1 深层神经网络的概念 ####

维基百科对深度学习的定义为“一类通过多层非线性变换对高复杂性数据建模算法的合集”。logistic回归（如图一），三层神经网络（如图二）都是比较浅的模型，如图三所示的五隐层神经网络是深度模型，属于深度神经网络，而深层，浅层是程度问题。*注：图二的2 hidden layers是不包括输出层的。*

<img src="https://upload-images.jianshu.io/upload_images/16793245-a7c1722f4978bea0.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图1" style="zoom:50%;" />

​                                                                                             图一

<img src="https://upload-images.jianshu.io/upload_images/16793245-18f7ad52cec1e2d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图2" style="zoom:150%;" />

​                                                                                             图二

![图3](https://upload-images.jianshu.io/upload_images/16793245-a770057aae45a3b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​                                                                                             图三

#### 4.1.2 深层神经网络的符号 ####

如下图所示的四层的有三个隐层的神经网络。输入特征用$x$表示，$x$也就是第0层的输出$a^{[0]}$。隐层中的单元数目是5，5，3；之后有一个输出单元。用$L$表示神经网络的层数，此时$L=4$；用$n^{[i]}$表示节点的数量，或者第$i$层上的单元数量。用$a^{[i]}$来表示第$i$层中的激活函数的输出，也是该层的输出。最后一层的输出$ a^{[4]}=\hat{y}$。

还有一些上标和下标的含义，如$a^{[i](j)}_{k}$，$j$表示此时为第$j$个样本在第$i$层的计算输出值，$k$表示此时为第$i$层的第$k$个结点。

![图4](https://upload-images.jianshu.io/upload_images/16793245-61ac36dd0a606912.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 4.2 深层神经网络的前向传递 ###

#### 前向传递的过程以及方式 ####

假设有一个训练样本$x$，网络结构如图五所示。在第一层里需要计算:
$$z^{[1]}=w^{[1]}x+b^{[1]}$$

此时$w^{[1]}、b^{[1]}$是会影响在第一层激活单元的参数。之后要计算这一层的激活值$a^{[1]}=g(z^{[1]})$，第一层计算完毕后，对于第二层来说:
$$\begin{array}{l}
z^{[2]}=w^{[2]}a^{[1]}+b^{[2]} \\
a^{[2]}=g(z^{[2]})
\end{array}$$

第三层类推，直到算到第四层，也就是输出层（激活函数用$\sigma$）:
$$\begin{array}{l}
z^{[4]}=w^{[4]}a^{[3]}+b^{[4]} \\
a^{[4]}=sigmoid(z^{[4]})
\end{array}$$

故所有正向传播的公式为：

$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}; a^{[l]}=g(z^{[l]})$。

![图5](https://upload-images.jianshu.io/upload_images/16793245-dcadcbe2e7dec756.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 4.3 核对矩阵的维数 ###

如图六所示，该神经网络$L=5$。$n^{[0]}=2; n^{[1]}=3; n^{[2]}=5; n^{[3]}=4; n^{[4]}=2; n^{[5]}=1$。$w^{[1]}=(n^{[1]},n^{[0]});w^{[2]}=(n^{[2]},n^{[1]})$,也就是（5,3）。故L层网络的核对矩阵维度$w^{[L]}:(n^{[L]},n^{[L-1]})$,根据向量加法的性质，$b^{[L]}:(n^{[L]},1)$。在分析方向传播时，$dw$与$w$维度一致，$dw^{[L]}:(n^{[L]},n^{[L-1]})$；$db$与$b$维度一致，$db^{[L]}:(n^{[L]},1)$。

![图6](https://upload-images.jianshu.io/upload_images/16793245-328493d549660a22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 4.4 为什么使用深层表示

#### 4.4.1 使用深层的原因	####

Hornik在1989年，就证明了一个定理：万能近似定理(Universal approximation theorem)。

> 只需**一个**包含足够多神经元的隐层，只要给予网络足够数量的隐藏单元，一个前馈神经网络就能以任意精度逼近任意复杂的连续函数。(Hornik et al., 1989;Cybenko, 1989)

那么既然一个隐层就能够表示出任意连续函数，我们为什么还需要多层神经网络呢？因为对于神经网络而言它有两个弊端：

1. 函数需要的神经单元可能大得不可实现，占用的资源呈指数增长。
2. 具有单层的前馈网络足以表示任何函数，但即使能够表示，网络层也可能无法正确地学习和泛化。

因此，我们选择使用深层神经网络。深度神经网络（Deep Neural Networks，DNN）可以理解为有很多隐藏层的神经网络。神经网络类似于拟合一个函数的过程，一个仅有一个隐藏层的神经网络就能拟合任意一个函数，但是它可能需要很多很多的神经元。而深层网络可以用**少得多的神经元**去拟合同样的函数。

我们可以用图7人脸识别系统为例，其中的步骤被当成黑箱，只简单讲解其中关于深层分析过程的原理。

![图7](https://upload-images.jianshu.io/upload_images/24435917-8375dfbbd04e6ff9.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于图1中含有三个隐层的神经网络，我们可以将它的功能理解为

- 第一层，把一张图片提取出边缘，形成低层次的简单特征（边缘特征）
- 第二层，将简单特征组合成复杂一点的特征，比如眼角，鼻子，嘴巴（器官）
- 第三层，将第二层的眼鼻口等特征组合起来，形成人脸（面部特征）

**所以深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来，去探测更加复杂的特征。**

它是一个简单到复杂的过程。每一层都比上一层复杂。

#### 4.4.2 与浅层的区别 ####

比起浅层，深层的网络隐藏单元数量相对较少（small），隐藏层数目较多（deep）。

而如果浅层的网络想要达到同样的计算结果，需要指数级增长的单元数量才能达到。

我们可以用一幅图来简单理解，如下图，假设想要对输入特征计算异或或是奇偶性，分别用树图和单层表示：

![图8](https://upload-images.jianshu.io/upload_images/24435917-b26458874584b410.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

左边，树图列举中首先计算相邻两个x的异或结果，此时得到 n/2 个结果；然后计算相邻的 n/2 个结果的异或结果，得到 n/4 个结果；以此类推，最后只剩下一个结果，即最终运算结果。容易得出，n 次运算变为了只需要 $n/2 + n/4 + ... + 1 = O(log_2 \space n)=O(log\space n)$次。

右边，暴力枚举中第一层每个结点计算一种情况（比如全0，全1，或者1010001），所以第一层需要$2^n$个结点，第二层一个结点进行输出，所以需要$O(2^n)$次运算。

可以看出右图比左边具有更少的隐含层，单元数却远比左边的深层要多得多。比起浅层网络，深层网络的表达能力更强，更能省资源。 

代码实现异或网络：

```python
import numpy as np

"""
实现XOR分类器。
输入：X shape (2, m)
输出：y shape (1, m)
y = x1 XOR x2 = ![x1x2 + !(x1+x2)]

a&b：sigmoid(-30+20a+20b)
!(a|b)：sigmoid(10-20a-20b)
"""


def sigmoid(x):
    y = 1 / (1+np.exp(-x))
    return y


def init_param():
    """
    返回异或网络的参数。

    :return: param_dict={
        "W1", "B1",
        "W2", "B2"
    }
    """
    W1 = np.array([[20, 20],
                   [-20, -20]])
    B1 = np.array([[-30],
                   [10]])

    W2 = np.array([[-20, -20]])
    B2 = np.array([[10]])

    param = {
        "W1": W1,
        "W2": W2,
        "B1": B1,
        "B2": B2
    }

    assert W1.shape == (2, 2)
    assert W2.shape == (1, 2)
    assert B1.shape == (2, 1)
    assert B2.shape == (1, 1)

    return param


def XOR(X):
    """
    用神经网络计算异或。

    :param X: shape (2, m)
    :return: Y, 异或的结果。shape (1, m)
    """
    param = init_param()
    W1 = param["W1"]
    W2 = param["W2"]
    B1 = param["B1"]
    B2 = param["B2"]
    m = X.shape[1]

    Z1 = np.dot(W1, X) + B1
    A1 = sigmoid(Z1)
    assert A1.shape == (2, m)

    Z2 = np.dot(W2, A1) + B2
    A2 = sigmoid(Z2)
    assert A2.shape == (1, m)

    return A2


X = np.array([[1, 1, 0, 0],
              [1, 0, 1, 0]])
Y = XOR(X)
print("X:\n", X)
print("Y:\n", Y)
"""
结果：
X:
 [[1 1 0 0]
 [1 0 1 0]]
Y:
 [[4.54391049e-05 9.99954520e-01 9.99954520e-01 4.54391049e-05]]
"""

```



### 4.5 搭建深层神经网络块

#### 4.5.1 深层神经网络的反向传递 ####

本小节我们先分析深度神经网络中的其中一层，看看反向传播是如何实现的，它和正向传播有什么关联。

在正向传播中我们已经知道，对于神经网络第$l$层，有：

输入：$a^{[l-1]}$
输出：$a^{[l]}$
已知项：$W^{[l]}$，$b^{[l]}$
缓存：$z^{[l]}$

而在反向传播中，同样是对第$l$层计算，有：

输入：$da^{[l]}$，缓存值$z^{[l]}$
输出：$da^{[l-1]}$，$dW^{[l]}$，$db^{[l]}$
已知项：$W^{[l]}$，$b^{[l]}$

需要注意，在反向传播期间，除了已知$W^{[l]}$，$b^{[l]}$，为了求导相应的反向传播函数，也需要知道第$l$层的激活函数是什么。


![图9](https://upload-images.jianshu.io/upload_images/24435917-8826a9d0702d48e2.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个过程可以从图9中直观地看出。对于反向传播，我们需要获取从正向中得到的$W^{[l]}$，$b^{[l]}$，以及预先储存的值$z^{[l]}$。因此实现了这两个函数（正向和反向），接下来我们将具体讨论深层神经网络块的搭建过程。

#### 4.5.2 搭建深层神经网络块 ####

对于完整网络正向反向过程，每一层都有前向传播和反向传播。将所有层结合起来，我们可以搭建一个深层神经网络块。

**1. 正向传播的步骤**

根据前向传播计算公式：

$$\begin{array}{l}
z^{[L]}=W^{[L]}a^{[L-1]}+ b^{[L]} \\
a^{[L]}=g^{[L]}\left(z^{[L]}\right) \\
\end{array}$$

![图10](https://upload-images.jianshu.io/upload_images/24435917-7b77f05e27786c6f.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

推算的步骤如上图所示。

1. 把输入特征$a^{[0]}$，放入第一层并计算第一层的激活值$a^{[1]}$，由此需要$W^{[1]}$和$b^{[1]}$来计算，也缓存$z^{[1]}$值。
2. 之后喂到第二层，第二层里，需要用到$W^{[2]}$和$b^{[2]}$，计算第二层的激活值$a^{[2]}$。
3. 后面几层以此类推，直到最后算出$a^{[L]}$，第L层的最终输出值为$\hat{y}$。

在这些过程里我们缓存了所有的$z$值，这就是正向传播的步骤。

**2. 反向传播的步骤**

根据后向传播计算公式：

$$\begin{array}{l}
d z^{[L]}=da^{[L]}g^{\prime[L]}\left(z^{[L]}\right) \\
d W^{[L]}=d z^{[L]} a^{[L-1]} \\
d b^{[L]}=d z^{[L]}\\
d a^{[L-1]}=W^{[L]T}d z^{[L]} \\
d z^{[L]}=W^{[L+1]T}d z^{[L+1]}g^{\prime[L]}\left(z^{[L]}\right)
\end{array}$$

![图11](https://upload-images.jianshu.io/upload_images/24435917-a7fba24b7ba04e84.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对反向传播的步骤而言，我们需要算一系列的反向迭代，步骤如上图所示

1. 反向传播的的一步从$a^{[L]}$开始，也就是经过一系列正向传播计算得到$\hat{y}$，之后再用输出值计算$da^{[L]}$（第二行最后方块）。
2. 将$da^{[L]}$作为反向传播的输入值，得到$da^{[L-1]}$的值，以此类推，直到我们得到$da^{[2]}$和$da^{[1]}$。对于最后的$da^{[0]}$，其实是输入特征的导数，并不重要，可以止步于此。
3. 反向传播步骤中也会输出$dW^{[L]}$和$db^{[L]}$，这些就是目前为止算好的所有需要的导数。

正向反向的向量化实现过程可以写成：

正向传播：

$$\begin{array}{l}
Z^{[1]}=W^{[1]} X+b^{[1]} \\
A^{[1]}=g^{[1]}\left(Z^{[1]}\right) \\
Z^{[2]}=W^{[2]} A^{[1]}+b^{[2]} \\
A^{[2]}=g^{[2]}\left(Z^{[2]}\right) \\
\vdots \\
A^{[L]}=g^{[L]}\left(Z^{[L]}\right)=\hat{Y}
\end{array}$$

反向传播：

$$\begin{array}{l}
d Z^{[L]}=A^{[L]}-Y \\
d W^{[L]}=\frac{1}{m} d Z^{[L]} A^{[L]^{T}} \\
d b^{[L]}=\frac{1}{m} n p \cdot \operatorname{sum}\left(\mathrm{d} Z^{[L]}, \text {axis}=1, \text { keepdims }=\text { True }\right) \\
d Z^{[L-1]}=d W^{[L]^{T}} d Z^{[L]} g^{[L]\prime}\left(Z^{[L-1]}\right) \\
\vdots \\
d Z^{[1]}=d W^{[L]^{T}} d Z^{[2]} g^{\prime[1]}\left(Z^{[1]}\right) \\
d W^{[1]}=\frac{1}{m} d Z^{[1]} A^{[1]^{T}} \\
d b^{[1]}=\frac{1}{m} n p \cdot \operatorname{sum}\left(\mathrm{d} Z^{[1]}, \text {axis}=1, \text {keepdims}=\text {True}\right)
\end{array}$$

现在我们就有所有的导数项了，$W$也会在每一层被更新为$W=W-\alpha d W$，$b$也一样，$b=b-\alpha d b$，反向传播就都计算完毕，我们有所有的导数值，于是就实现了神经网络一个梯度下降循环。


### 4.6 前向传播和反向传播 ###

前向传播：令层数为第$l$层，输入是$a^{[l−1]}$，输出是$a^{[l]}$，缓存变量是$z^{[l]}$，有：
$$\begin{array}{l}
z^{[l]}=W^{[l]} a^{[l-1]}+b^{[l]} \\
a^{[l]}=g^{[l]}\left(z^{[l]}\right)
\end{array}$$

对m个训练样本进行向量化：
$$\begin{array}{l}
Z^{[l]}=W^{[l]} A^{[l-1]}+b^{[l]} \\
A^{[l]}=g^{[l]}\left(Z^{[l]}\right)
\end{array}$$

反向传播：输入是$da^{[l]}$，输出是$da^{[l−1]},dw^{[l]},db^{[l]}$，有：
$$\begin{array}{l}
d z^{[l]}=d a^{[l]} * g^{[l]\prime}\left(z^{[l]}\right) \\
d W^{[l]}=d z^{[l]} \cdot a^{[l-1]} \\
d b^{[l]}=d z^{[l]} \\
d a^{[l-1]}=W^{[l] T} \cdot d z^{[l]} \\
\end{array}$$

将$d a^{[l]}=W^{[l+1] T} \cdot d z^{[l+1]}$，代入$d z^{[l]}=d a^{[l]} * g^{[l]\prime}\left(z^{[l]}\right)$可得：
$$d z^{[l]}=W^{[l+1] T} \cdot d z^{[l+1]} * g^{[l]\prime}(z^{[l]})$$

从而得出$d z^{[l]}$与$d z^{[l+1]}$的递推关系。
对m个训练样本进行向量化：
$$\begin{array}{l}
dZ^{[l]}=dA^{[l]} * g^{[l] \prime} \left(Z^{[l]} \right) \\
dW^{[l]}= \frac{1}{m} d Z^{[l]} \cdot A^{[l-1] T} \\
db^{[l]}= \frac{1}{m} np. \operatorname{sum} \left(dZ^{[l]}, axis=1, \right.keepdim=True) \\
dA^{[l-1]}=W^{[l] T} \cdot d Z^{[l]} \\
\end{array}$$

#### 4.6.1 反向传播与梯度下降法的关系 ####

神经网络通过前向传播可以求得预测值，为了比较预测值和真实值的差距，构建了以$w$和$b$为变量的损失函数，而我们希望损失函数越小越好。
梯度下降法就是用以寻找最合适的$w$和$b$，使得损失函数尽可能小。运用梯度下降法，我们需要知道梯度的方向和大小。
反向传播就是用来求出梯度的方法。即：反向传播是梯度下降法的一种实现方法。

#### 4.6.2 梯度向量和$Jacobian$矩阵的关系 ####

梯度向量：当函数$f$为关于自变量向量$x=(x_{1},x_{2},\ldots,x_{n})^{T} \in \mathbb{R}^{n \times 1}$的标量函数时（$f:\mathbb{R}^{n} \rightarrow R$），$f$对$x$求梯度，则得到与$x$同维度的向量，称为梯度向量$g$：

$$g(x)=\nabla f(x)=\left(\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \ldots, \frac{\partial f}{\partial x_{n}}\right)^{T}=\frac{\partial f}{\partial x}^{T}$$

不难发现，我们的$z_{i}$的计算与标量函数$f$对应，即可以写成：$z_{i}=f(x)=w_{i}x+b$。其中$w_{i} \in \mathbb{R}^{1 \times n},b \in R$。$w_{i}$对应与某一层的第$i$个结点。

$Jacobian$矩阵：当函数$f$为向量函数（$f(x)=(f_{1}(x),f_{2}(x),\ldots,f_{m}(x))$，其中$f_{i}(x)$是标量函数），自变量为向量$x$时，函数向量$f$对$x$求梯度，结果为一个矩阵；且行数为$f$的维数$m$，列数位$X$的维度$n$，称之为$Jacobian$矩阵。

$$\nabla f(x)=\left[\begin{array}{cccc}
\frac{\partial f_{1}(x)}{\partial x_{1}} & \frac{\partial f_{1}(x)}{\partial x_{2}} & \cdots & \frac{\partial f_{1}(x)}{\partial x_{n}} \\
\frac{\partial f_{2}(x)}{\partial x_{1}} & \frac{\partial f_{2}(x)}{\partial x_{2}} & \cdots & \frac{\partial f_{2}(x)}{\partial x_{n}} \\
\cdots & \cdots & \cdots & \cdots \\
\frac{\partial f_{m}(x)}{\partial x_{1}} & \frac{\partial f_{m}(x)}{\partial x_{2}} & \cdots & \frac{\partial f_{m}(x)}{\partial x_{n}}
\end{array}\right]_{m \times n}=\left[\begin{array}{c}
g_{1}(x)^{T} \\
g_{2}(x)^{T} \\
\ldots \\
g_{m}(x)^{T}
\end{array}\right]$$

其每一行都是由相应函数的梯度向量转置构成的；梯度向量可以视为是$Jacobian$矩阵的一个特例，即$m=1$的时候，或者说函数$f$为标量函数时。

同样不难发现，这里的向量函数与我们的$z=(z_{1},z_{n},\ldots,z_{m})^{T} \in \mathbb{R}^{m \times 1}$对应，即可以写成：$z=f(x)=Wx+b$。其中$W=  \left(\begin{array}{c}
w_{1} \\
w_{2} \\
\vdots \\
w_{m}
\end{array}\right) \in \mathbb{R}^{m \times n}$,$b \in \mathbb{R}^{m \times 1}$。注意这里的$m$不是指样本数。可得：

$$\frac{\partial z}{\partial x}=\nabla f(x)=\left[\begin{array}{cccc}
\frac{\partial z_{1}(x)}{\partial x_{1}} & \frac{\partial z_{1}(x)}{\partial x_{2}} & \cdots & \frac{\partial z_{1}(x)}{\partial x_{n}} \\
\frac{\partial z_{2}(x)}{\partial x_{1}} & \frac{\partial z_{2}(x)}{\partial x_{2}} & \cdots & \frac{\partial z_{2}(x)}{\partial x_{n}} \\
\cdots & \cdots & \cdots & \cdots \\
\frac{\partial z_{m}(x)}{\partial x_{1}} & \frac{\partial z_{m}(x)}{\partial x_{2}} & \cdots & \frac{\partial z_{m}(x)}{\partial x_{n}}
\end{array}\right]_{m \times n}$$

为什么要讲这个？我的理解是因为这个是更底层的知识，可以理解为是求解梯度时，为什么矩阵的梯度是这样计算（当然，结果和我们的直觉得出的结果差不多）。虽然现在大部分的深度学习甚至都需要太多数学知识就可以做，但是我认为，在求学道路上，如果真的想要做出一番成就，越是底层的越不能忘、越是应该深入了解。

#### 4.6.3 矩阵求导

此章参考文章[这里](https://zhuanlan.zhihu.com/p/24709748)，如果看不懂我写的可移步。

我们的损失函数$J(W^{[1]},b^{[1]},\dots)$是一个标量函数，然而无论是$Z,W,b$还是$A$都是矩阵，所以我们需要舍弃我们高数学习的对标量的求导，开始学习对矩阵求导。

通过梯度向量向量我们可以得知，标量$f(x)$对向量$x$的导数与微分有联系：

$$df=\frac{\partial f}{\partial x}^{T}dx$$

其中$\frac{\partial f}{\partial x}^{T}dx$相当于$\frac{\partial f}{\partial x}^{T}$与$dx$的内积（简单来说是相同shape的矩阵逐元素相乘并求和：$\sum \sum A_{ij}B_{ij}$）。受此启发，我们可以定义标量$f(X)$对矩阵$X$的导数与微分的联系：

$$df=tr(\frac{\partial f}{\partial X}^{T}dX)$$。

其中$tr$代表迹（$trace$），是方阵对角线元素之和；$tr(A^{T}B)$可以表示相同shape的矩阵$A,B$的内积。是不是和上式对应上了？但是这只建立的联系，还是没讲如何计算梯度（导数）$\frac{\partial f}{\partial x}$。不过先别急，我们先讲部分会用上的矩阵微分运算规则：

* $d(X \pm Y)=d X \pm d Y$，$X,Y$的shape相同。
* $d(X Y)=(d X) Y+X d Y$；$d\left(X^{T}\right)=(d X)^{T}$；$d \operatorname{tr}(X)=\operatorname{tr}(d X)$。
* $d(X*Y)=(dX)*Y+X*dY$，$*$是我们比较熟悉的逐元素相乘写法，正规的是$\odot$。尤其逐元素除法也适用。
* $df(X)=f^{\prime}(X)*dX$，其中$f(X)$是逐元素函数，可以对应到激活函数。

除了这些运算规则，我们还需要一些迹技巧（trace trick）：

* 对于标量$a$有：$a=tr(a)$。
* $tr(A^T)=tr(A)$。
* $tr(A \pm B)=tr(A) \pm tr(B)$，尤其对逐元素运算都适用（$A,B$的shape相同）。
* $tr(AB)=tr(BA)$，矩阵乘法符合交换律。

然后我们可以开始计算梯度$\frac{\partial f}{\partial x}$了。假设有$f=f(Z),Z=g(A)$，其中$Z,A$均为矩阵，$\frac{\partial f}{\partial Z}$已知：

1. 列出$df=tr(\frac{\partial f}{\partial Z}^{T}dZ)$。
2. 将$Z=g(A)$代入得$df=tr(\frac{\partial f}{\partial Z}^{T}dg(A))$。
3. 展开$dg(A)$，利用矩阵微分的运算规则，将所有项移到$dA$的左边，得$df=tr([\dots]dA)$。
4. 利用德摩根律$(AB)=(B^TA^T)^T$，得$df=tr([\dots]^TdA)$。
5. 其中的$[\dots]$便是$\frac{\partial f}{\partial A}$。

开始实战求求$\frac{\partial J}{\partial A^{[l-1]}}$，假设$\frac{\partial J}{\partial Z^{[l]}}$已知，$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$，下略上标：

1. $dJ=tr(\frac{\partial J}{\partial Z}^TdZ)$
2. 代入：$dJ=tr(\frac{\partial J}{\partial Z}^Td(WA+b))$
3. 展开：$dJ=tr(\frac{\partial J}{\partial Z}^T[d(WA)+db])$
4. $b$对于$X$来说是常数，$db=0$，继续展开：$dJ=tr(\frac{\partial J}{\partial Z}^T[(dW)A+WdA])$
5. 同上，$dW=0$：$dJ=tr(\frac{\partial J}{\partial Z}^TWdA)$
6. 德摩根律：$dJ=tr([W^T\frac{\partial J}{\partial Z}]^TdA)$
7. 结果：$\frac{\partial J}{\partial A}=W^T \frac{\partial J}{\partial Z}$ 

是不是终于明白为什么$W$会在左边而且要加转置了（虽然可能直觉告诉你的结果也是这样）？其他的矩阵梯度$\frac{\partial J}{\partial Z},\frac{\partial J}{\partial W}$等的计算也可以用类似方法求得。一定要记住的是，**不能随便沿用标量的链式法则**，矩阵梯度的计算是没有链式法则的，计算结果只是**有相似性而已**。

#### 4.6.4 代码

手动求导是非常麻烦、辛苦且容易出错的，就此我们需要代码帮我们实现自动求导。下面用pytorch实现：

设 $y=x^2$，用torch实现这个函数并且求导。若$x=3$，则求导可知$x$的梯度为$2*3=6$。

```python
x=torch.tensor(3.0,requires_grad=True)
y=torch.pow(x,2)

#判断x,y是否是可以求导的
print(x.requires_grad)
print(y.requires_grad)

#求导，通过backward函数来实现
y.backward()

#查看导数，也即所谓的梯度
print(x.grad)

"""结果
True
True
tensor(6.) #这和我们自己算的是一模一样的
"""
```

tensor是张量的意思。在torch中，tensor对象的定义：`tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor`。各参数含义如下：

* data： (array_like): tensor的初始值. 可以是列表，元组，numpy数组，标量等；并且只有浮点数才能求导，所以要用3.0代替3
* dtype： tensor元素的数据类型
* device： 指定CPU或者是GPU设备，默认是None
* requires_grad：是否可以求导，即求梯度，默认是False，即不可导的。可以通过这个属性判断该变量是否可导。

对于函数$y$，只有当其所有的leaf variable（叶子变量，比如在$z=f(y),y=g(x)$中，$x$是叶子变量，$y$不是）都是不可导的，它才是不可导的。

`y.backward()`是自动求导的关键，backward函数定义：`backward(gradient=None, retain_graph=None, create_graph=False)`。各参数含义如下：

* gradient：各梯度的权重。如果输入和输出都是张量，则需要使用这个参数，因为输出的每个变量都要求一次导数，会得到多个梯度。比如有函数$Y=X^2+X$，求导的代码实现如下：

  ```python
  x = torch.tensor([[1.,2.,3.],[4.,5.,6.]],requires_grad=True)
  y = torch.add(torch.pow(x,2),x)
  
  gradient=torch.tensor([[1.0,1.0,1.0],[1.0,1.0,1.0]])
  
  y.backward(gradient)
  
  print(x.grad)
  
  """结果
  tensor([[ 3., 5., 7.],
  [ 9., 11., 13.]])
  """
  ```

  实例代码中的gradient，shape和 Y 一致，每个元素都为1表示不加权。可以改为其他数字进行加权。

* retain_graph：保留图运算。在如下图的复合函数中，如果执行了`p.backward()`，那么之后就无法执行`p.backward()`或`q.backward()`,因为图运算中为了节省资源，求过一次导的x已经被销毁了。如果通过retain_graph保留图运算的结点而不释放。

  ![](https://img-blog.csdnimg.cn/20200717210212404.png)

在调用`y.backward()`求导后，用`x.grad`获得导数。至此已完成自动求导。

#### 4.6.5 多层感知机

多层感知的模型如下图：

![多层感知机](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4ua2VzY2kuY29tL3VwbG9hZC9pbWFnZS9xNWhvNjg0am1oLnBuZw?x-oss-process=image/format,png)

使用多层感知机进行分类，数据样本随机生成。代码如下：

```python
import torch
# M是样本数量，input_size是输入层大小
# hidden_size是隐含层大小，output_size是输出层大小
M, input_size, hidden_size, output_size = 64, 1000, 100, 10

# 生成随机数当作样本
x = torch.randn(M, input_size) #size(64, 1000)
y = torch.randn(M, output_size) #size(64, 10)

# 参数初始化
def init_parameters():
    w1 = torch.randn(input_size, hidden_size)
    w2 = torch.randn(hidden_size, output_size)
    b1 = torch.randn(1, hidden_size)
    b2 = torch.randn(1, output_size)
    return {"w1": w1, "w2":w2, "b1": b1, "b2": b2}

# 定义模型
def model(x, parameters):
    Z1 = x.mm(parameters["w1"]) + parameters["b1"] # 线性层
    A1 = Z1.clamp(min=0) # relu激活函数
    Z2 = A1.mm(parameters["w2"]) + parameters["b2"] #线性层
    # 为了方便反向求导，我们会把当前求得的结果保存在一个cache中
    cache = {"Z1": Z1, "A1": A1}
    return Z2, cache

# 计算损失
def loss_fn(y_pred, y):
    loss = (y_pred - y).pow(2).sum() # 我们这里直接使用 MSE(均方误差) 作为损失函数
    return loss

# 反向传播，求出梯度
def backpropogation(x, y, y_pred, cache, parameters):
    m = y.size()[0] # m个样本
    # 以下是反向求导的过程：
    d_y_pred = 1/m * (y_pred - y)
    d_w2 = 1/m * cache["A1"].t().mm(d_y_pred)
    d_b2 = 1/m * torch.sum(d_y_pred, 0, keepdim=True)
    d_A1 = d_y_pred.mm(parameters["w2"].t())
    # 对 relu 函数求导: start
    d_Z1 = d_A1.clone()
    d_Z1[cache["Z1"] < 0] = 0
    # 对 relu 函数求导: end
    d_w1 = 1/m * x.t().mm(d_Z1)
    d_b1 = 1/m * torch.sum(d_Z1, 0, keepdim=True)
    grads = {
        "d_w1": d_w1, 
        "d_b1": d_b1, 
        "d_w2": d_w2, 
        "d_b2": d_b2
    }
    return grads

# 更新参数
def update(lr, parameters, grads):
    parameters["w1"] -= lr * grads["d_w1"]
    parameters["w2"] -= lr * grads["d_w2"]
    parameters["b1"] -= lr * grads["d_b1"]
    parameters["b2"] -= lr * grads["d_b2"]
    return parameters

## 设置超参数 ##

learning_rate = 1e-2
EPOCH = 400

# 参数初始化
parameters = init_parameters()

## 开始训练 ##
for t in range(EPOCH):    
    # 向前传播
    y_pred, cache = model(x, parameters)
    
    # 计算损失
    loss = loss_fn(y_pred, y)
    if (t+1) % 50 == 0:
        print(loss)
    # 反向传播
    grads = backpropogation(x, y, y_pred, cache, parameters)
    # 更新参数
    parameters = update(learning_rate, parameters, grads)

```



### 4.7 超参数 ###

定义：在神经网络中，需要人为设置的参数，并用这些参数求得$w$和$b$的值，即为超参数，如：学习率$α$，激活函数的选择……
与普通参数关系：普通参数是需要通过对数据集的训练才能得出的，而超参数是在训练一开始就进行人为设置的，所以，超参数在某种程度上决定了普通参数（$w$,$b$,…）的取值.
寻找最优超参数:超参数的选择是需要一定经验的，在没有足够经验的情况下，通常可以选取一个一定的范围，并不断尝试让超参数取到这个范围内的值，看损失函数的变化情况，通过不断的尝试，令损失函数尽可能取得更合适，从而找到最优的超参数。鉴于超参数有可能会随着时间发生改变，所以我们要勤于检验。

### 4.8 神经网络和大脑的关系 ###

当神经网络和大脑都是由许多单元组成，即逻辑单元（神经元），当神经网络进行运算时，只进行单层（只含有一个逻辑单元）的运算是不足以解决问题的，（上世纪70年代，AI发展的寒冬，正是因为人们发现单个逻辑单元无法进行简单的异或运算引起的）通常神经网络在解决问题时，需要动用多个逻辑单元所构成的网络。同样的，在大脑中，单个神经元也是无法解决问题的，大脑对信息的处理，也是要通过多个神经元的。
正因神经网络和大脑都无法只通过一个逻辑单元（神经元）解决问题，所以人们经常将两者进行类别。
人们一直认为大脑和神经网络一样，无法只通过一个神经元进行异或运算，可最新的研究发现，虽然神经网络中，至少需要两个逻辑单元才可以进行异或运算，但在大脑中，只需要一个神经元，就可以进行异或运算。这或许将重塑人们对神经网络的看法。

引用论文[Dendritic action potentials and computation in human layer 2/3 cortical neurons](https://science.sciencemag.org/content/367/6473/83)

### 4.9 编程作业

本程序参考 https://blog.csdn.net/u013733326/article/details/79767169 进行部分的修改或者进行更加细致化的注释。

#### 4.9.1 引入相关数据集和库

**编程目的**是实现一个两层和一个多层的神经网络，用于对猫的图片进行识别。
相关数据集和库的下载，[点击此处](http://pan.baidu.com/s/1I4MBm7QwRGuQDp-IZc9P1Q&shfl=sharepset)；密码：2u3w

	import numpy as np
	import h5py
	import matplotlib.pyplot as plt
	import testCases 
	from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward 
	import lr_utils 

#### 4.9.2 初始化 ####

首先进行网络的节点和参数的创建和初始化。

**两层网络的初始化**

```python
np.random.seed(1)
def initialize_parameters(n_x,n_h,n_y):
    """
    此函数是为了初始化两层网络参数而使用的函数。即对每个节点进行创建和初始化
    参数：
        n_x - 输入层节点数量
        n_h - 隐藏层节点数量
        n_y - 输出层节点数量
    
    返回：
        parameters - 包含你的参数的python字典：
            W1 - 权重矩阵,维度为（n_h，n_x）
            b1 - 偏向量，维度为（n_h，1）
            W2 - 权重矩阵，维度为（n_y，n_h）
            b2 - 偏向量，维度为（n_y，1）


	    使用的函数：
	         np.random.randn(x,y)：返回一个服从“0~1”均匀分布的随机样本值的维度为x行y列的矩阵
	         np.zeros(shape, dtype=float, order=‘C’)：返回来一个给定形状和类型的用0填充的数组；
	             shape:形状
	             dtype:数据类型，可选参数，默认numpy.float64
	             order:可选参数，c代表与c语言类似，行优先；F代表列优先
	         assert()：断言函数，用于判断一个表达式，在表达式条件为 false 的时候触发异常。
	         .shape：获取矩阵的维度数据
	    """
	#初始化各种参数
	W1 = np.random.randn(n_h, n_x) * 0.01
	b1 = np.zeros((n_h, 1))
	W2 = np.random.randn(n_y, n_h) * 0.01
	b2 = np.zeros((n_y, 1))
	
	#使用断言确保我的数据格式是正确的
	assert(W1.shape == (n_h, n_x))
	assert(b1.shape == (n_h, 1))
	assert(W2.shape == (n_y, n_h))
	assert(b2.shape == (n_y, 1))
	
	#生成一个字典输出。
	parameters = {"W1": W1,
	              "b1": b1,
	              "W2": W2,
	              "b2": b2}
	
	return parameters  

```

**多层网络的初始化**

```python
def initialize_parameters_deep(layers_dims):
    """
    此函数是为了初始化多层网络参数而使用的函数。
    参数：
        layers_dims - 包含我们网络中每个图层的节点数量的列表
    
    返回：
        parameters - 包含参数“W1”，“b1”，...，“WL”，“bL”的字典：
                     W1 - 权重矩阵，维度为（layers_dims [1]，layers_dims [1-1]）,即为了保证z = wx + b时，z维度为（n【t】，），x的维度为（n【t-1】，1）所所以w的维度必须为（n【t】，n【t-1】）；
                     bl - 偏向量，维度为（layers_dims [1]，1）
    使用的函数：
        np.sqrt（x）：对x开方
    """
    np.random.seed(3)
    parameters = {}
    #图层数
    L = len(layers_dims)
    
    #对每一层初始化参数，并保存在字典当中
    for l in range(1,L):
        parameters["W" + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) / np.sqrt(layers_dims[l - 1])
        parameters["b" + str(l)] = np.zeros((layers_dims[l], 1))
        
        #确保我要的数据的格式是正确的
        assert(parameters["W" + str(l)].shape == (layers_dims[l], layers_dims[l-1]))
        assert(parameters["b" + str(l)].shape == (layers_dims[l], 1))
        
    return parameters
```

在初始化各个参数和节点之后，则需要实现前向传播的计算过程
这个过程分为三个步骤或者说功能

1. 线性函数（wx + b）
2. 线性函数 -> 激活函数（ReLU或Sigmoid）
3. 线性函数 -> 激活函数（ReLU） -> 线性函数 -> 激活函数（Sigmoid）
   这三个步骤的搭建都是基于上一步搭建的函数为基础的，如第二步需要用到第一步的函数实现线性部分的功能。

对于ReLU和Sigmoid函数，python并没有专门的函数进行计算，所以需要自己定义（不过实现很简单，此处不提供具体实现）。
下面分别进行这三个步骤的功能函数的搭建

#### 4.9.3 模型搭建

线性函数

```python
def linear_forward(A,W,b):
    """
    实现前向传播的线性部分。

    参数：
        A - 来自上一层（或输入数据）的激活，维度为(上一层的节点数量，示例的数量）
        W - 权重矩阵，numpy数组，维度为（当前图层的节点数量，前一图层的节点数量）
        b - 偏向量，numpy向量，维度为（当前图层节点数量，1）

    返回：
         Z - 激活功能的输入，也称为预激活参数
         cache - 一个包含“A”，“W”和“b”的字典，存储这些变量以有效地计算后向传递
    函数：
        np.dot(x,y) ———x是m*n 矩阵 ，y是n*g矩阵，则得到m*g矩阵。（即实现矩阵相乘）
    """
    #线性计算
    Z = np.dot(W,A) + b
    assert(Z.shape == (W.shape[0],A.shape[1]))
    #保存相关参数作为缓冲值，方便后向传播的实现
    cache = (A,W,b)
     
    return Z,cache
```

线性 -> 激活函数（分为两种情况）

```python
def linear_activation_forward(A_prev,W,b,activation):
    """
    实现LINEAR-> ACTIVATION 这一层的前向传播

    参数：
        A_prev - 来自上一层（或输入层）的激活，维度为(上一层的节点数量，示例数）
        W - 权重矩阵，numpy数组，维度为（当前层的节点数量，前一层的大小）
        b - 偏向量，numpy阵列，维度为（当前层的节点数量，1）
        activation - 选择在此层中使用的激活函数名，字符串类型，【"sigmoid" | "relu"】

    返回：
        A - 激活函数的输出，也称为激活后的值
        cache - 一个包含“linear_cache”和“activation_cache”的字典，我们需要存储它以有效地计算后向传递
    """
    #对不同参数施行不同函数的操作
    if activation == "sigmoid":
        #首先进行线性计算
        Z, linear_cache = linear_forward(A_prev, W, b)
        #调用sigmoid函数进行处理
        A, activation_cache = sigmoid(Z)
    elif activation == "relu":
        #首先进行线性计算
        Z, linear_cache = linear_forward(A_prev, W, b)
        #调用relu函数进行处理
        A, activation_cache = relu(Z)
    
    assert(A.shape == (W.shape[0],A_prev.shape[1]))
    cache = (linear_cache,activation_cache)
    
    return A,cache
```

线性 -> 激活函数（ReLU） -> 线性 -> 激活函数（Sigmiod）

```python
def L_model_forward(X,parameters):
    """
    实现[LINEAR-> RELU] *（L-1） - > LINEAR-> SIGMOID计算前向传播，也就是多层网络的前向传播，为后面每一层都执行LINEAR和ACTIVATION
    
    参数：
        X - 数据，numpy数组，维度为（输入节点数量，示例数）
        parameters - initialize_parameters_deep（）的输出,也就是初始化的数据
    
    返回：
        AL - 最后的激活值
        caches - 包含以下内容的缓存列表：
                 linear_relu_forward（）的每个cache（有L-1个，索引为从0到L-2）
                 linear_sigmoid_forward（）的cache（只有一个，索引为L-1）
    """
    caches = []
    A = X
    #每一层参数向量有两个，所以除2能够得到神经网络的层数
    #// 进行的是除法，会舍去小数部分
    L = len(parameters) // 2
    for l in range(1,L):
        A_prev = A 
        #进行线性->激活（relu）函数的调用
        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], "relu")
        #保存每一层参数，直接插入到缓冲向量caches的末尾
        caches.append(cache)
    
    #进行线性->激活（sigmoid）函数的调用，即输出层的计算
    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], "sigmoid")
    caches.append(cache)
    
    assert(AL.shape == (1,X.shape[1]))
    
    return AL,caches
```

成本函数用于计算结果和实际的偏差值，并用于计算各种参数的导数
成本函数的公式为：

$$-\frac{1}{m} \sum_{i=1}^{m} \left(y^{(i)} \log \left(a^{[L](i)} \right)+ \left(1-y^{(i)}\right) \log \left(1-a^{[L](i)} \right) \right)$$

```python
def compute_cost(AL,Y):
    """
    实施等式（4）定义的成本函数。

    参数：
        AL - 与标签预测相对应的概率向量，维度为（1，示例数量）
        Y - 标签向量（例如：如果不是猫，则为0，如果是猫则为1），维度为（1，数量）

    返回：
        cost - 交叉熵成本
    使用函数：
        Y.shape，输出Y的行和列
        Y.shape[0]:输出Y的行
        Y.shape[1]:输出Y的列
        np.squeeze(cost):从数组cost的形状中删除单维度条目，即把shape中为1的维度去掉,并返回这个删除的维度数组
            eg:
            array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])
            >>> np.squeeze(a)
            array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    """
    #获取样本的个数，也就是样本的标签向量的列数
    m = Y.shape[1]
    #成本函数计算
    cost = -np.sum(np.multiply(np.log(AL),Y) + np.multiply(np.log(1 - AL), 1 - Y)) / m
    
    #将结果去向量化    
    cost = np.squeeze(cost)
    assert(cost.shape == ())

    return cost
```


为实现反向传播需要对应于前向传播有几个步骤

1.  线性计算的反向计算
2.  线性->激活函数（两种激活函数）的反向计算
3.  线性->激活函数（relu）-> 线性 -> 激活函数（sigmoid）的反向计算
    反向传播用于计算出各个参数的对应于成本函数的导数值，以方便我们后面进行参数的更新。
    反向传播的相关导数计算公式为
    $$\begin{array}{l}
    d W^{[l]}= \frac{\partial \mathcal{L}}{\partial W^{[l]}}= \frac{1}{m} d Z^{[l]} A^{[l-1] T} \\
    d b^{[l]}=\frac{\partial \mathcal{L}}{\partial b^{[l]}}=\frac{1}{m} \sum_{i=1}^{m} d Z^{[l](i)} \\
    d A^{[l-1]}=\frac{\partial \mathcal{L}}{\partial A^{[l-1]}}=W^{[l] T} d Z^{[l]}
    \end{array}$$

进行线性计算的反向传播

```python
def linear_backward(dZ,cache):
    """
    为单层实现反向传播的线性部分（第L层）

    参数：
         dZ - 相对于（当前第l层的）线性输出的成本梯度
         cache - 来自当前层前向传播的值的元组（A_prev，W，b）

    返回：
         dA_prev - 相对于激活（前一层l-1）的成本梯度，与A_prev维度相同
         dW - 相对于W（当前层l）的成本梯度，与W的维度相同
         db - 相对于b（当前层l）的成本梯度，与b维度相同
    """
    #从缓冲字典中获取各个参数的向量值
    A_prev, W, b = cache
    #获取样本的个数
    m = A_prev.shape[1]
    #进行dW，db，dA_prev的计算，也就是求导计算
    dW = np.dot(dZ, A_prev.T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m
    dA_prev = np.dot(W.T, dZ)
    
    assert (dA_prev.shape == A_prev.shape)
    assert (dW.shape == W.shape)
    assert (db.shape == b.shape)
    
    return dA_prev, dW, db
```

进行线性 ->激活 的反向传播

对应于两种激活函数，python自有一个反向传播的计算公式，分别对应为 ```relu_backward()```和```sigmoid_backward()```

```python
def linear_activation_backward(dA,cache,activation="relu"):
    """
    实现LINEAR-> ACTIVATION层的后向传播。
    
    参数：
         dA - 当前层l的激活后的梯度值
         cache - 我们存储的用于有效计算反向传播的值的元组（值为linear_cache，activation_cache）
         activation - 要在此层中使用的激活函数名，字符串类型，【"sigmoid" | "relu"】
    返回：
         dA_prev - 相对于激活（前一层l-1）的成本梯度值，与A_prev维度相同
         dW - 相对于W（当前层l）的成本梯度值，与W的维度相同
         db - 相对于b（当前层l）的成本梯度值，与b的维度相同
    """
    #从缓冲字典中获取参数
    linear_cache, activation_cache = cache
    #对应于不同的函数进行不同的计算
    if activation == "relu":
        #计算顺序和前向传播的顺序相反，先进行激活函数反向计算，再进行线性函数的反向计算
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
    
    return dA_prev,dW,db
```

线性->激活函数（relu）->线性->激活函数（sigmoid）的反向计算

```python
def L_model_backward(AL,Y,caches):
    """
    对[LINEAR-> RELU] *（L-1） - > LINEAR - > SIGMOID组执行反向传播，就是多层网络的向后传播
    
    参数：
     AL - 概率向量，正向传播的输出（L_model_forward（））
     Y - 标签向量（例如：如果不是猫，则为0，如果是猫则为1），维度为（1，数量）
     caches - 包含以下内容的cache列表：
                 linear_activation_forward（"relu"）的cache，不包含输出层
                 linear_activation_forward（"sigmoid"）的cache
    
    返回：
     grads - 具有梯度值的字典
              grads [“dA”+ str（l）] = ...
              grads [“dW”+ str（l）] = ...
              grads [“db”+ str（l）] = ...
    使用的函数：
        Y.reshape(X.shape):将一个矩阵更新成X矩阵的维度和形状（即改变为和X相同的行和列）
    """
    #建立保存计算出的每一层的参数的导数的结果值的字典，用于后面的更新参数
    grads = {}
    #获取层数
    L = len(caches)
    #获取正向传播的输出的列数，也就是样本的个数
    m = AL.shape[1]
    
    Y = Y.reshape(AL.shape)
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    
    #获取前一层的参数
    current_cache = caches[L-1]
    #进行sigmoid()函数的反向求导计算，并保存相关参数的计算结果
    grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, "sigmoid")
    
    #对剩下的隐藏层进行线性-》激活函数（relu）的反向计算，并且保存计算的参数对应于成本函数的求导
    for l in reversed(range(L-1)):
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(l + 2)], current_cache, "relu")
        grads["dA" + str(l + 1)] = dA_prev_temp
        grads["dW" + str(l + 1)] = dW_temp
        grads["db" + str(l + 1)] = db_temp
    
    return grads
```


#### 4.9.4 更新参数 ####

通过使用反向传播计算出的参数导数，使用更新公式来更新对应的各个节点的参数
使用的公式为：
$$\begin{array}{l}
W^{[l]}=W^{[l]}-\alpha d W^{[l]} \\b^{[l]}=b^{[l]}-\alpha d b^{[l]}
\end{array}$$
其中α是学习率。

```python
def update_parameters(parameters, grads, learning_rate):
    """
    使用梯度下降更新参数
    
    参数：
     parameters - 包含你的参数的字典
     grads - 包含梯度值的字典，是L_model_backward的输出
    
    返回：
     parameters - 包含更新参数的字典
                   参数[“W”+ str（l）] = ...
                   参数[“b”+ str（l）] = ...
    """
    #从参数字典中获取神经网络的层数
    L = len(parameters) // 2 #整除
    #通过计算更新参数字典
    for l in range(L):
        parameters["W" + str(l + 1)] = parameters["W" + str(l + 1)] - learning_rate * grads["dW" + str(l + 1)]
        parameters["b" + str(l + 1)] = parameters["b" + str(l + 1)] - learning_rate * grads["db" + str(l + 1)]
        
    return parameters
```


#### 4.9.5 搭建构建函数 ####

**构建两层神经网络的函数**

```python
	def two_layer_model(X,Y,layers_dims,learning_rate=0.0075,num_iterations=3000,print_cost=False,isPlot=True):
	    """
        实现一个两层的神经网络，【LINEAR->RELU】 -> 【LINEAR->SIGMOID】
	    参数：
	        X - 输入的数据，维度为(n_x，例子数)
	        Y - 标签，向量，0为非猫，1为猫，维度为(1,数量)
	        layers_dims - 层数的向量，维度为(n_y,n_h,n_y)
	            n_x - 输入层节点数量
	            n_h - 隐藏层节点数量
	            n_y - 输出层节点数量
	        learning_rate - 学习率
	        num_iterations - 迭代的次数
	        print_cost - 是否打印成本值，每100次打印一次
	        isPlot - 是否绘制出误差值的图谱
	    返回:
	        parameters - 一个包含W1，b1，W2，b2的字典变量
	    """
	    np.random.seed(1)
	    grads = {}
	    costs = []
	    (n_x,n_h,n_y) = layers_dims
	    
	    #调用初始化函数，进行初始化参数
	
	    parameters = initialize_parameters(n_x, n_h, n_y)
	    #建立参数对象
	    W1 = parameters["W1"]
	    b1 = parameters["b1"]
	    W2 = parameters["W2"]
	    b2 = parameters["b2"]

    #开始进行迭代
    
    for i in range(0,num_iterations):
        #前向传播
        A1, cache1 = linear_activation_forward(X, W1, b1, "relu")
        A2, cache2 = linear_activation_forward(A1, W2, b2, "sigmoid")
        
        #计算成本
        cost = compute_cost(A2,Y)
        
        #后向传播
        ##初始化后向传播
        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))
        
        ##向后传播，输入：“dA2，cache2，cache1”。 输出：“dA1，dW2，db2;还有dA0（未使用），dW1，db1”。
        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, "sigmoid")
        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, "relu")
        
        ##向后传播完成后的数据保存到grads
        grads["dW1"] = dW1
        grads["db1"] = db1
        grads["dW2"] = dW2
        grads["db2"] = db2
        
        #更新参数
        parameters = update_parameters(parameters,grads,learning_rate)
        W1 = parameters["W1"]
        b1 = parameters["b1"]
        W2 = parameters["W2"]
        b2 = parameters["b2"]
        
        #打印成本值，如果print_cost=False则忽略
        if i % 100 == 0:
            #记录成本
            costs.append(cost)
            #是否打印成本值
            if print_cost:
                print("第", i ,"次迭代，成本值为：" ,np.squeeze(cost))
    #迭代完成，根据条件绘制图
    if isPlot:
        plt.plot(np.squeeze(costs))
        plt.ylabel('cost')
        plt.xlabel('iterations (per tens)')
        plt.title("Learning rate =" + str(learning_rate))
        plt.show()
    
    #返回parameters
    return parameters
```

**数据集调用并测试**

```python
train_set_x_orig , train_set_y , test_set_x_orig , test_set_y , classes = lr_utils.load_dataset()

train_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T 
test_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

train_x = train_x_flatten / 255
train_y = train_set_y
test_x = test_x_flatten / 255
test_y = test_set_y

n_x = 12288
n_h = 7
n_y = 1
layers_dims = (n_x,n_h,n_y)

parameters = two_layer_model(train_x, train_set_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True,isPlot=True)
```

测试结果：

![代价](https://upload-images.jianshu.io/upload_images/16793245-4ca39d4e8834b808.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


**搭建多层神经网络**

```python
def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False,isPlot=True):
    """
    实现一个L层神经网络：[LINEAR-> RELU] *（L-1） - > LINEAR-> SIGMOID。
    
    参数：
	    X - 输入的数据，维度为(n_x，例子数)
        Y - 标签，向量，0为非猫，1为猫，维度为(1,数量)
        layers_dims - 层数的向量，维度为(n_y,n_h,···,n_h,n_y)
        learning_rate - 学习率
        num_iterations - 迭代的次数
        print_cost - 是否打印成本值，每100次打印一次
        isPlot - 是否绘制出误差值的图谱
    
    返回：
     parameters - 模型学习的参数。 然后他们可以用来预测。
    """
    np.random.seed(1)
    costs = []
    
    parameters = initialize_parameters_deep(layers_dims)
    
    for i in range(0,num_iterations):
        AL , caches = L_model_forward(X,parameters)
        
        cost = compute_cost(AL,Y)
        
        grads = L_model_backward(AL,Y,caches)
        
        parameters = update_parameters(parameters,grads,learning_rate)
        
        #打印成本值，如果print_cost=False则忽略
        if i % 100 == 0:
            #记录成本
            costs.append(cost)
            #是否打印成本值
            if print_cost:
                print("第", i ,"次迭代，成本值为：" ,np.squeeze(cost))
    #迭代完成，根据条件绘制图
    if isPlot:
        plt.plot(np.squeeze(costs))
        plt.ylabel('cost')
        plt.xlabel('iterations (per tens)')
        plt.title("Learning rate =" + str(learning_rate))
        plt.show()
    return parameters
```

两层和多层的区别在于两层只使用了反向传播时的第二步的函数，而多层使用的是第三层的函数。

![代价](https://upload-images.jianshu.io/upload_images/16793245-093444107622c79b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
