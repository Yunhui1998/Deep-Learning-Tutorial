# 1.1 为什么是ML策略

如果你训练了一段时间自己的神经网络，准确率到达了一定水平（比如90%）但却还不能满足你的要求，那这个时候必然需要对网络做出一些调整。你可能想到了几个解决思路，比如收集更多的数据、用不同的优化算法、训练得更久一些、加入正则化甚至修改网络架构等等。

但是这些办法并不是总是有效的，可能收集了6个月的数据却发现对网络的性能没有任何帮助。就是说如果你作出了错误的选择，那很可能浪费大量的时间和精力而得不到任何有效进展。

于是接下来希望教学一些策略：ML策略，以帮助我们向着有希望的方向前进。

# 1.2 正交化

这里的正交化可以简单地理解为，每个自变量的改变只会引起一个因变量发生改变。举个反例，假设有$y_1=x_1+0.9x_2$,$y_2=0.1x_2$，则自变量$x_2$的改变会同时引起因变量$y_1,y_2$的改变，同时若想要通过$x_2$调整$y_1$时会不可避免的也调整了$y_2$。

在ML策略中，正交化是一种设计方法，以保证你在修改超参的时候不会引起两个以上的参数的变化。

## 四个参数

假设你需要正交化而确保不同时改变的四个参数是：

* 收敛后的训练集的代价（cost）
* 交叉验证集的代价
* 测试集的代价
* 真实应用的效果

当然，以上的参数是“按顺序”得到的。即只有在当前参数算优的情况下你才会做下一步而得到下一个参数的值。

对于第一点，如果训练集上的效果不好甚至无法收敛，可以考虑更大的神经网络或其他优化算法比如$Adam$。

对于第二点，可能原因是过拟合了，那可以尝试加入正则化或采用更大的训练集。

对于第三点，可以考虑采用更大的交叉验证集。

对于第四点，则考虑改变交叉验证集的图源或改变损失函数。

举个不正交的例子是 early stopping。因为 early stopping 会基于开发集的效果而提早停止训练，所以带来的影响是两方面的：训练集的代价还没收敛至最优的同时降低了在开发集上的代价。

# 1.3 单一数字评估指标

对于一个网络的性能评估，我们有时候会采用多个指标以便更了解自己搭建的网络的效果。但是当你需要从多个网络中选出最好的一个时，多评估指标可能会导致你无法轻易选出优胜者——有的网络某一些指标最好，另一些指标则比不上其他网络。这个时候，单一数字评估指标就能很方便地帮你评价出最优的网络。

## 1.3.1常用指标

参考文章[在这](https://zhuanlan.zhihu.com/p/73569538)。

## 1.3.2混淆矩阵：

为了方便表达各指标如何计算，先介绍混淆矩阵：

![1.3.jpg](https://upload-images.jianshu.io/upload_images/16793245-a59cc3b9b306533b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

阴阳可理解为真假、1/0。a是预测为真真实也为真的样本数，其余类比。

## 1.3.3指标

1. Precision查准率/精度：评价网络做出的预测的准确率。对于二分类来说（比如猫/非猫分类器），   $$P= \frac{预测为1且正确的样本数}{所有预测为1的样本数} = \frac{a}{a+b}$$。

2. 对于进行one-hot热值编码的多分类来说同样适用。

3. Recall查全率/True Positive真阳率/Sensitivity敏感性：评价网络对于目标类的识别率。对于二分类来说，

   $$R= \frac {所有预测为1且正确的样本数}{所有真实值为1的样本数} \frac{a}{a+c}$$。

   对于进行了one-hot热值编码的多分类来说同样适用。

4. Accuracy准确率：评价网络所有类预测的准确率。对于二分类来说，$Acc= \frac{所有预测正确的样本数}{总的样本数}= \frac{a+d}{a+b+c+d}$。

5. 对于进行了one-hot热值编码的多分类同样适用。

6. False Positive误检率/假阳率：评价网络对阴性预测的错误率，为

   $$\frac{预测为1且错误的样本数}{真实值为0的样本数}=\frac{b}{b+d}$$。

7. F1分数：$$\frac{2}{\frac{1}{P}+\frac{1}{R}}$$

   P 和 R 分别代指 Precision 和 Recall。是 Precision 和 Recall 的调和平均数。

# 1.4满足和优化指标

如果你想考虑的多个指标取值范围是相同的，那取平均值就可以简单地将多个指标“整合”成单一评估指标。但是如果其中一个是准确率P（%），另一个是单张图的预测时间T（ms），那取平均值的做法是几乎不合理的。线性组合$$aP+bT$$这样的做法或许可行，但是参数a、b的值却又可能难以设计。

所以，可以从另一个角度考虑：其中一个（甚至多个）指标只用于淘汰、筛选模型；另一个指标作为评估指标，是判断哪一个更优的指标。这分别对应满足指标和优化指标。

## 1.4.1满足指标

该指标是你关心的指标之一，但是又并不那么重要：该指标只需要到达一定范围即可，不是你**最想优化**的指标。比方说你很关心模型的准确率，同时也希望运行速度足够快，那将 T 视为满足指标是合适的。

满足指标的用意在于如果你有多个指标想要兼顾，但是大部分指标又并不是那么重要，那作为用于筛选模型的指标是很不错的选择。比如唤醒 Siri 你最关注的是 Recall，这能确保有人真的想唤醒 Siri 时能及时做出反应；但你也关注在没人想唤醒 Siri 时，Siri 被错误唤醒的概率。那将误检率/假阳率作为满足指标是可行的，比如设置为每24小时只有一次错误唤醒。

## 1.4.2优化指标

优化指标应该是你最关心的指标，是你最想优化的指标，比如唤醒 Siri 时的Recall。

由于单一数字评估指标的原因，优化指标应该只有一个，而其余你关心的指标都应该设为满足指标，或者和优化指标合理地整合为一个指标。

# 1.5测试集、交叉验证集、测试集的划分

## 1.5.1 训练集、交叉验证集、测试集的作用

- **训练集**：用以训练模型；

- **交叉验证集** (也叫dev 集、开发集)：衡量训练效果的同时进行超参数调优；

- **测试集**：评测模型训练效果（测定泛化误差）。

  以课后作业、期末考试、高考三者间的关系类比训练、开发、测试集的作用：

  - 课后作业（训练集）：让你学会知识；
  - 期末考试（交叉验证集）：让你检验自己是否真的学会了知识，发现了问题后，你会调整学习方法（调整超参数）；
  - 高考（测试集）：最终的测试，测试你在**没见过的**题上究竟能做到什么程度（泛化能力），且不再给你调整的机会。

## 1.5.2 如何设立交叉验证集和测试集

​		建议在设立交叉验证集和测试集时，要选择能够反映未来你希望得到的并认为做好很重要的数据作为交叉验证集和测试集。

>  Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on.

​		交叉验证集和测试集最好来自**同一分布**：不管未来会得到什么样的数据，一旦你的算法效果不错，要尝试收集类似的数据且不管那些数据是什么，都要**随机分配**到开发集和测试集中，这样才能瞄准想要的目标，让团队高速迭代来逼近同一个目标。

## 1.5.3 如此设立交叉验证集和测试集的原因

​		设立交叉验证集以及一个单实数评估指标就像是定下目标，告诉你的团队那就是你要瞄准的靶心，一旦建立了这样的开发集和指标，团队就可以快速迭代，可以很快地使用交叉验证集和指标去评估不同分类器，然后尝试选出最好的那个，针对开发集上的指标优化。

​		如果交叉验证集和测试集来自不同的分布，就像设定了一个目标，结果在团队逼近这个目标工作几个月后，想要把成果应用到另一个目标。为了避免这种情况，建议将所有数据随机洗牌放入开发集和测试集，所以交叉集和测试集都有来自所有不同分类的数据并且开发集和测试集都来自同一分布，这个分布就是所有的数据混在一起。

# 1.6 交叉验证集和测试集的大小

## 1.6.1 交叉验证集和测试集大小设置的原则

1、经验法则：把取得的所有数据划分为 **70%训练集+30%测试集** 或 **60%训练集+20%交叉验证集+20%测试集**。

​		这种经验法则使用于机器学习的早期，即数据集规模较小的情况下。

2、在现代机器学习中，数据集规模很大，可分为**98%训练集+1%交叉验证集+1%测试集**。

​		训练集的大小：深度学习算法对数据的胃口很大，故要把占更高比例的数据划分到训练集中；

​		测试集的大小：测试集的目的是完成系统开发后用测试集评估系统性能，故要令测试集足够大，能够以高置信度评估系统整体性能。所以除非需要对最终系统有一个很精确的指标，一般来说，测试集不需要上百万个例子，有10000或100000个大概就足够了，当数据量很大时，测试集就会远远小于数据集的30%或20%。

## 1.6.2 什么情况下可以没有测试集

​		对于不需要对系统性能有置信度很高的评估的应用，可以不用划分测试集，只需要训练集和交叉验证集，只要有数据去训练、有数据去调试就够了，但是并不建议在系统搭建时省略测试集，有个单独的测试集会更好，因为可以用这组不带偏差的数据来测量系统的性能，但如果交叉验证集非常大，就不会对交叉验证集过拟合得太厉害，在这种情况下只有训练集和交叉验证集也不是完全不合理。

​		在实际实践中，有时只把数据集划分为训练集和测试集，实际上这是在测试集上迭代，所以这里的测试集其实是交叉验证集，即只把数据分为训练集和交叉验证集，没有测试集。

# 1.7 什么时候该改变交叉验证/测试集和指标

## 1.7.1 如何定义一个指标去评估分类器

​		评估指标的意义在于，准确告诉你已知的两个分类器哪一个更适合你的应用。

​		当已设定的评估指标无法正确衡量算法之间的优劣排序时，就应该改变评估指标或者改变交叉验证集或测试集。

​		例如，若对旧的误差指标 $Error = \frac{1}{m_{dev}} \sum_{i=1}^{m_{dev}}I[y_{pred}^{(i)}≠y^{(i)}]$ 不满意，可以尝试定义一个新的能够更加符合你的偏好的指标，从而定义出更适合的算法，如在 $I$ 项前加一个权重项 $\omega^{(i)}$，即

​                                                  $Error = \frac{1}{m_{dev}} \sum_{i=1}^{m_{dev}} \omega^{(i)} I[y_{pred}^{(i)}≠y^{(i)}]$ 

​		其中，

​                                                              $\omega^{(i)}=\left\{\begin{array}{ll}1 & \text { if }   x^{(i)}... \\ 10 & \text { if } x^{(i)}...\end{array}\right.$，

当 $x^{(i)}$ 是预期分类中的数据时，赋予较小的权重1，当 $x^{(i)}$ 不是预期分类中的数据时，赋予较大的权重10，误差项迅速变大，即把非预期分类数据分类成某一类的惩罚权重加大10倍。

## 1.7.2 机器学习的两个独立步骤（正交化）

1. 设定目标：弄清楚如何定义一个指标来衡量你想做的事情的表现（**定义指标**）；
2. 瞄准/射击目标：分开考虑如何改善系统在这个指标上的表现（**调整指标**以优化系统）。

## 1.7.3 需改变评价标准的情况

​		如果系统在当前指标、当前交叉验证集或者交叉验证集和测试集分布中表现很好，但在实际应用程序即你真正关注的地方表现不好（当前的指标和当前用来评估的数据和你真正关心必须做好的事情关系不大），就需要修改指标或交叉验证/测试集，让它们能够更好地反映算法需要处理好的数据。

​		举例：假设有两个猫分类器A和B，用开发集评估分别得到3%和5%的误差，做评估时用的是高质量图片的交叉验证集和测试集但在实际应用时上传的图片取景不专业，没有把猫完整拍下来或猫的表情很古怪，或者图像很模糊，在实际测试算法时可能会发现算法B比算法A更好，此时就应该修改指标或交叉验证/测试集。

# 1.8为什么是人的表现

机器学习模型的准确性有两个层级，分别为

1.human-level performance（人的表现）

2.Bayes optimal error（贝叶斯最优误差）

机器学习模型表现常与人的学习表现相比较![image-20200925164341635](C:\Users\fan\Desktop\image-20200925164341635.png)

如图，随着训练时间的不断增长，模型的准确性会不断接近“人的表现”并最终超过。当超过“人的表现”后，准确性的上升会比较缓慢，并不断靠近理想的最优情况，即贝叶斯最优误差。而实际上，“人的表现”，在许多如语音识别、图像识别的部分是非常具有优势的，让机器学习模型的准确性不断接近“人的表现”非常有必要。

1.Get labeled data from humans.

2.Gain insight from manual error analysis: Why did a person get this right?

3.Better analysis of bias/variance.

以上方法可以让模型不断接近人的表现。

# 1.9可避免偏差

在实际应用中，要看human-level error，training error和dev error的相对值。例如猫类识别的例子中，

![image-20200926143758650](C:\Users\fan\Desktop\image-20200926143758650.png)



对于物体识别这类CV问题，human-level error是很低的，很接近理想情况下的贝叶斯最优误差。因此，上面例子中的1%和7.5%都可以近似看成是两种情况下对应的bayes optimal error。实际应用中，我们一般会用human-level error代表bayes optimal error。



通常，我们把training error与human-level error之间的差值称作avoidable bias（可避免偏差）；把dev error与training error之间的差值称为方差（variance）。根据bias和variance值的相对大小，可以知道算法模型是否发生了欠拟合或者过拟合。

若相比于方差（variance），bias较大，则我们对模型的调整主要在于减小偏差

若相比于方差（variance），bias较小，则可采用正则化、采集更多数据等方法减小方差

# 1.10理解人的表现

human-level performance有时能够代表bayes optimal error。但是，human-level performance如何定义呢？举个医学图像识别的例子，不同人群的error有所不同：

![image-20200926144037591](C:\Users\fan\Desktop\image-20200926144037591.png)

由于bayes optimal error是最优的理想情况，一般来说，我们将表现最好的那一组，即Team of experienced doctors作为human-level performance，以此来代替bayes optimal error。



但在实际应用时，human-level performance的选择也会因人而异，不同取值的human-level performance会在一定程度上影响bias和variance的相对大小。如下图



![image-20200926144113294](C:\Users\fan\Desktop\image-20200926144113294.png)

在方案C中，由于Training error和Development error数值差只为0.1，即variance只有0.1，此时不同取值的human-level performance会影响到bias和variance的相对大小，影响到了后续对模型的调整与改善。

而这种情况一般只在模型表现很好，接近bayes  optimal error 时才会出现，此时，由于human-level performance较模糊难以定义，而影响到了模型的优化。

# 1.11超过人的表现

![image-20200926144243272](C:\Users\fan\Desktop\image-20200926144243272.png)

对于自然感知类问题，例如视觉、听觉等，机器学习的表现还不及人类。但是在很多其它方面，如



- Online advertising（在线广告）
- Product recommendations（产品推荐）
- Logistics(predicting transit time)（预测交通时间）
- Loan approvals（贷款批准）

这些方面，机器的表现优于人类。

机器学习模型超过human-level performance是比较困难的。但是如果有足够多的样本，训练更加复杂的神经网络，模型预测准确性会不断得到提高，最终接近并超过human-level performance。然而当机器模型的表现超过human-level performance时，难以直觉来继续提高算法模型性能。

# 1.12改善模型表现

提高机器学习模型性能主要要解决两个问题：avoidable bias和variance。

解决avoidable bias的常用方法包括：

- Train bigger model（训练更大的模型）

- Train longer/better optimization algorithms: momentum, RMSprop, Adam

  （使用更好的算法：momentum、RMSprop、Adam）

- NN architecture/hyperparameters search（选用不同的神经网络结构）

解决variance的常用方法包括：

- More data（使用更多数据）
- Regularization: L2, dropout, data augmentation（使用正则化方法）
- NN architecture/hyperparameters search（选用不同的神经网络结构）

