# 14序列模型

### 14.1 为什么选择序列模型 

#### 14.1.1 序列模型应用领域

序列模型广泛应用于语音识别，音乐生成，情感分析，DNA序列分析，机器翻译，视频行为识别，命名实体识别等众多领域。

- 语音识别：将输入的语音信号直接输出相应的语音文本信息。无论是语音信号还是文本信息均是序列数据。

- 音乐生成：生成音乐乐谱。只有输出的音乐乐谱是序列数据，输入可以是空或者一个整数。

- 情感分类：将输入的评论句子转换为相应的等级或评分。输入是一个序列，输出则是一个单独的类别。

- DNA序列分析：输入的DNA序列，找到匹配的蛋白质序列。

- 机器翻译：两种不同语言之间的翻译。输入和输出均为序列数据。

- 视频行为识别：识别输入的视频帧序列中的人物行为。

- 命名实体识别：从输入的句子中识别实体的名字。

  ![image](https://img-blog.csdn.net/20180724084849379?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb3l1dGlhbmdhbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

#### 14.1.2 输入x和输出y的关系

上面那些问题可以看成使用标签数据$(x,y)$作为训练集的监督学习，但是输入与输出不一定都是序列，对应关系有非常多的组合，比如一对一，多对多，一对多，多对一，多对多（个数不同）等情况来针对不同的应用。

### 14.2 数学符号

#### 14.2.1 符号含义

- $x^{<t>}$ 表示输入数据 x​ 中的第 t 个符号(eg:输入的英文句子中的单词)
- $y^{<t>}$ 表示输出 y 中的第 t 个符号(单词)
- ${x^{(i)<t>}}$  表示第 i 个输入样本中的第 t 个符号(单词)
- ${y^{(i)<t>}}$ 表示第 i 个输出样本中的第 t 个符号(单词)
- $T_x$ 表示输入 x 的长度
- $T_y$ 表示输出 y 的长度
- $T_x^{(i)}$ 表示第 i 个训练样本的输入序列长度
- $T_y^{(i)}$ 表示第 i 个训练样本的输出序列长度

#### 14.2.2 字典编码

- 利用一个字(词)典向量，通常有3-5万个字(词)，来表示$x^{<t>}$的符号含义

- 可以利用one-hot编码表示词典里的每个单词，构造一个输入的序列中每个单词$x^{<t>}$的向量

  - 与字典向量大小一致（3-5万行）
- $x^{<t>}$代表的单词对应到字典里的单词，在对应索引位置（行数）是1，其余位置都是0。比如“and”在词典里排第367，所以相应的$x^{<t>}$就是第367行是1，其余值都是0的向量。
  - 如果单词不在字典里，则创造一个新的“Unknow Word”伪单词，用<**UNK**>作为标记。


### 14.3 循环神经网络模型

#### 14.3.1 为什么不用普通神经网络

试想如果将输入拆成每个字的 One-Hot 编码输入传统的深度神经网络中，经过一些隐藏层的计算得到输出 Y。

存在的问题：

- 输入和输出数据在不同的例子中可以有不相等的长度

- 不能共享从文本不同位置所学习到的特征：能学习到“and”在$x^{<1>}$位置的普通神经网络不能识别“and”在$x^{<4>}$时的情况。

- 参数数量过多

循环神经网络可以解决以上问题

#### 14.3.2 循环神经网络

循环神经网络，从左到右一个单词计算一步，每一步的计算不仅来自这一步的输入，也来自上一步的激活函数值。

![image](https://img-blog.csdn.net/20180724084907159?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhb3l1dGlhbmdhbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- 最左侧第一层假设了一个来自第零层的激活值向量（通常设计为零向量或随机向量）
- 有的研究论文上将循环神经网络的结构写成图片右边的形式，其和左边的形式是一致的。

循环神经网络从左到右扫描数据，同时共享每个时间步的参数。

- $W_{ax}$ 管理从输入$x^{<t>}$到隐藏层连接的一系列参数
- $W_{aa}$ 管理激活值$a^{<t>}$到隐藏层连接的一系列参数
- $W_{ya}$ 管理隐藏层到输出结果 $y$ 连接的一系列参数
- 每个时间步都使用相同的 $W_{ax}$ ，$W_{aa}$ ，$W_{ya}$ 

#### 14.3.3 循环神经网络的前向传播

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvOTQ5MjQxLzIwMTgwOC85NDkyNDEtMjAxODA4MjAxNDMwMTEzMzItODc0OTU2MTYuanBn?x-oss-process=image/format,png)

如图，首先输入$a^{<0>}$，接着是前向传播过程，先计算$a^{<1>}$，再计算$y^{<1>}$

$$
\begin{array}{c}a^{<1>}=g_1\left(W_{a a} a^{<0>}+W_{a x} x^{<1>}+b_{a}\right) \\y^{<1>}=g_2\left(W_{y a} a^{<1>}+b_{y}\right)\end{array}
$$
计算每步的激活函数 a 通常为**Tanh** ，也可由输出判断改用**ReLu**，**sigmoid**（二分类），**softmax**（k分类）

在 t 时刻：

$$
\begin{array}{c}
a^{<t>}=g\left(W_{a a} a^{<t-1>}+W_{a x} x^{<t>}+b_{a}\right) \\
y^{<t>}=g\left(W_{y a} a^{<t>}+b_{y}\right)
\end{array}
$$

> 1. 每一步都有一个激活函数 $a^t$ 和一个输出函数 $y^{<t>}$
> 2. 激活函数$a^t$来自于输入 $x^t$ 和上一步的激活函数 $a^{t−1}$, 输出函数 $y^{<t>}$ 来自于激活函数 $a^t$ ，因此实现了利用这个序列中之前的所有序列信息来做出预测
> 3. 激活函数或者输出函数都是 $g(WX+b)$ 这一点没有改变
> 4. 参数W，b 的第一个下标表示是激活函数a 的参数还是输出函数 y 的参数， W的第二个下标表示这个参数是用来乘以哪个输入

#### 14.3.4 前向传播参数的简化

下面参数可以精简合并为$WX+b$形式，以便建立更复杂的模型。

$$
\begin{array}{c}
a^{<t>}=g\left(W_{a}[ a^{<t-1>}, x^{<t>}]+b_{a}\right) \\
y^{<t>}=g\left(W_{y} a^{<t>}+b_{y}\right)
\end{array}
$$

其中 $W_{a}=[W_{a a}:W_{a x}]$ ，表示两个矩阵并列放置，维度为[ len(a) , len(a) + len(x) ]。

$[ a^{<t-1>}, x^{<t>}]=\left[\begin{array}{l}
a^{<t-1>}\\
x^{<t>} \\
\end{array}\right]$，表示建立成一个列向量，维度为[ len(a) + len(x) , 1 ]。

这时 $[W_{a a}:W_{a x}]$ 乘以 $\left[\begin{array}{l}
a^{<t-1>}\\
x^{<t>} \\
\end{array}\right]$ 正好等于 $W_{a a} a^{<t-1>}+W_{a x} x^{<t>}$。

而对$y^{<t>}$简化为：
$$
W_{y}=W_{y a}
$$



### 14.4 通过时间的反向传播

#### 14.4.1 序列模型中的反向传播

在一些编程框架里实现RNN时，编程框架会自动为我们处理关于RNN的反向传播，但还是有必要粗略的了解一下反向传播在RNN中的实现。

如下图的前向传播

![image-20201106104909166.png](https://upload-images.jianshu.io/upload_images/24439865-f341d7473e54f9b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于$a^{<t>}$的计算，需要参数$W_{aa}$ ；对于$y^{<t>}$的计算，需要参数$W_{ya}$；与之前的对神经网络的训练相类似，在RNN中，进行反向传播的目的，是为了对这些参数进行更新，直到找到最为合适的参数。



![image-20201106105525897.png](https://upload-images.jianshu.io/upload_images/24439865-13cf4a1dfb973183.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

同样的，如上图，在RNN中实现反向传播仍然需要定义cost function。不同的在于，它需要对于每一个输出的$y^{<t>}$进行概率值的计算，并最终将所有计算得出的$y^{<t>}$进行求和，计算整体的cost function。



![image-20201106105616950.png](https://upload-images.jianshu.io/upload_images/24439865-fe46da7fcf01c61e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

计算出整体的cost function后，再进行求导、梯度下降的方法进行处理，将前向传播的箭头都反过来，最终更新出合适的参数。这就是大致的RNN中反向传播的过程。



#### 14.4.2 反向传播中的损失函数

在反向传播中，定义的对于单个输出的cost function为

​                                                                      $L^{<t>}\left(\hat{y}^{<t>}, y^{<t>}\right)=-y^{<t>} \log \hat{y}^{<t>}-\left(1-y^{<t>}\right) \log \left(1-\hat{y}^{<t>}\right)$

称为交叉熵损失函数，用以计算序列中某一个特定的词为某一类别（如名字、地点等）的词的概率，如若计算该词为人名的概率为0.1，则输出0.1.



对于所有输出的预测值的求和，定义的cost function为

​                                                                                        $L(\hat{y}, y)=\sum_{t=1}^{T_{y}} L^{<t>}\left(\hat{y}^{<t>}, y^{<t>}\right)$

即将每一个timestep的输出预测值进行求和。

### 14.5 不同类型的循环神经网络

以上介绍的，都是当$T_x$和$T_y$ 相同的情况下，RNN的实现。更多时候，$T_x$和$T_y$ 是不同的，面对两者不同时，我们要用不同的RNN模型。个人认为主要的调整在于，在不同的timestep进行输入和输出，以及将一些输出的信息作为输入，作用到下一个单元中。

大致如下图

![image-20201106134337398.png](https://upload-images.jianshu.io/upload_images/24439865-7a00f278e34efba5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



### 14.6 语言模型和序列生成

语言模型是NLP中比较重要的一个模块，它能够计算一句话被翻译成各种不同意思的概率，并选择最大概率的翻译作为正确翻译。

#### 14.6.1 语言模型的构建

构建语言模型，首先需要建立一个足够大的training set，即一个包含很多句子的corpus，然后对corpus中的每句话进行tokenize操作，即将一整句话切分成每一个单词，利用one-hot，将单词转换成词向量。其中，句子结尾可以用符号<EOS>表示；而对没有出现在corpus中的单词可以用<UNK>表示。再利用RNN来计算一句话被翻译成具体意思的概率，如下图

![image-20201106145545285.png](https://upload-images.jianshu.io/upload_images/24439865-f17fea3a4e7fc8e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

$x^{<1>}$和$a^{<0>}$均为零向量，softmax层会根据$x^{<1>}$和$a^{<0>}$来预测输出$y^{<1>}$，它会计算第一个词分别为字典中的每一个词的概率具体为多少（如预测为“猫”、“狗”的概率分别为多少）；而$y^{<2>}$则表示在出现第一个单词的基础上，出现第二个单词的概率，即为条件概率，以此类推，直到计算出出现<EOS>的概率

#### 14.6.2 概率计算

在softmax层中，计算单个元素的cost function为

​                                                                     $L^{<t>}\left(\hat{y}^{<t>}, y^{<t>}\right)=-\sum_{i} y_{i}^{<t>} \log \hat{y}_{i}^{<t>}$

计算所有元素的cost function为

​                                                                             $L(\hat{y}, y)=\sum_{t} L^{<t>}\left(\hat{y}^{<t>}, y^{<t>}\right)$

计算一句话被翻译正确的概率公式为                    

​                                                                               $P\left(y^{<1>}, y^{<2>}, \cdots, y^{<T_{y}>}\right)$

即对每个元素的cost function进行求和。

最后，整个语句出现的概率为语句中所有元素出现的条件概率的乘积，如，某个语句中包含$y^{<1>}$、$y^{<2>}$ 、  $y^{<3>}$，则整个语句出现的概率为

​                                                         $P\left(y^{<1>}, y^{<2>}, y^{<3>}\right)=P\left(y^{<1>}\right) \cdot P\left(y^{<2>} \mid y^{<1>}\right) \cdot P\left(y^{<3>} \mid y^{<1>}, y^{<2>}\right)$



### 14.7 新序列采样

在训练完一个序列模型之后，如果想了解这个模型学到了什么，那就可以选择进行一次新序列采样。不过这是非正式做法。

新序列采样可以理解为先给模型一个输入，然后从其预测结果中随机选取一个作为下一次的输入；直至满足不再采样的要求，这样就可以得到一连串的字符。这些字符构成一段文本，可以帮你了解你的模型学到了什么。接下来看具体怎么做。

#### **14.7.1 生成随机语句**

首先要理解的是，你的模型会干什么：给定一个单词（也不一定是单词），预测下一个单词是词典中某个单词的概率。

1. 采样的第一个时间步，是输入$x^{<1>}=0,a^{<0>}=0$。然后得到输出向量，这个向量表示了词典中每个词出现的概率。然后根据给出的概率，随机选取一个词作为$\hat{y}^{<1>}$。具体实现可以用:

   ```python
   y_1 = np.random.choice(dictionary, p=y)
   ```

   其中`y_1`表示$\hat{y}^{<1>}$，`dictionary`表示词典，`p=y`表示词典中的各值以对应`y`中的概率选取。`y`是模型输出的向量。

2. 第二个时间步，将$\hat{y}^{<1>}$作为这一步的输入$x^{<2>}$，然后得到预测$\hat{y}^{<2>}$。然后依次类推，重复若干次。

3. 当$\hat y$为结束符***EOS***或者已经获得足够的单词后，就可以停止采样了。

执行流程大概如图：

![图片1](https://upload-images.jianshu.io/upload_images/16793245-c69fcb198b93cf1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这样下来，就可以得到由$\hat{y}^{<i>}$组成的一句话或者一段文本了。

#### **14.7.2 词典为字符与词典为单词的优缺点**

**基于单词**的词典的优缺点

* 优点：不会出现不存在的单词；序列很短就能构成句子、段落；
* 缺点：可能出现未知标识***UNK***；字典会很大；

**基于字符**的词典的优缺点

* 优点：不会出现未知标识；字典小很多；
* 缺点：可能出现不存在的单词；序列会太多太长，前面的单词会被遗忘；训练成本高昂；

### 14.8 带有神经网络的梯度消失

#### 14.8.1 为什么会梯度消失

梯度消失并不只是存在于很深的卷积神经网络中，同样还存在于RNN中——因为RNN其实也可以看成是一个很深的网络，只不过深度由训练样本的长度决定。不过还要意识到，导致RNN能“有很多层”的原因是激活值$a^{<t>}$的传递，所以如果要进行优化也是针对$a^{<t>}$进行。

举个例子：对于样本“The **cat**, which already ate ……, **was** full.”，前面的单数**cat**决定了后面用的是**was**，而中间的字符可以任意长。如果想要 RNN 能发现这一点，就需要反向传播时能将**was**处的损失传播到**cat**处。但是实际情况下很可能由于已经传播了很多层（两者间单词过多）而导致梯度消失，最终对**cat**的影响微乎其微。

更普遍来说，对于长句子样本，句子前后的关联性会很难被发现、建立起来。

#### 14.8.2 梯度修剪

其实RNN也存在梯度爆炸问题。但是相对于梯度爆炸，梯度消失更难发现。因为梯度消失只会让你的参数基本不变，而不像梯度爆炸会让你的参数出现很多的**NaN**，这意味着数值出现了溢出。

而且不同于梯度消失，梯度爆炸解决起来其实更容易一些。解决办法就是**梯度修剪**：

观察梯度向量，通过人为设定一个阈值（比较大的数），如果超过了阈值就缩放梯度向量，从而保证梯度不会过大。

### 14.9 GRU单元

为了解决梯度消失的问题，学者提出了GRU单元（门控循环单元网络）的概念。这个网络能有效解决梯度消失的问题，并且能够使你的神经网络捕获更长的长期依赖。

#### 14.9.1 GRU单元实现

![2.png](https://upload-images.jianshu.io/upload_images/16793245-ee36b30f3bb4042c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

回顾一下RNN单元的工作模式如上。记住公式
$$
a^{<t>}=g\left(W_{a}\left[a^{<t-1>}, x^{<t>}\right]+b_{a}\right)
$$
在处理一个句子的时候，如果想要记住一个以前出现过的单词，那就需要一个额外的变量保存：***c***（cell），代表了记忆细胞。记忆细胞负责记忆前面的**cat**是单数还是复数。记 $c^{<t>}$ 为记忆细胞在时间 $t$ 时的取值。对于GRU来说，其实$c^{<t>} = a^{<t>}$。但还是会使用不同的标记，因为在**LSTM**中会有所不同。

记忆细胞在更新时，需要考虑是否忘记已经记住了的值并更新为需要记住的值。所以需要先计算候选值
$$
\tilde{c}^{<t>}=\tanh \left(W_{c}\left[c^{<t-1>}, x^{<t>}\right]+b_{c}\right)
$$
然后重点来了，GRU真正重要的思想是有一个**更新门**：
$$
\Gamma_{u}=\sigma\left(W_{u}\left[c^{<t-1>}, x^{<t>}\right]+b_{u}\right)
$$
这个门的作用在于决定什么时候更新$c^{<t>}$。但是在介绍怎么决定是否更新前，先明白$\Gamma_{u}$的含义：大写希腊字母$\Gamma$很像门，代表门的含义；下标$u$表示 updata 更新；然后计算用的是sigmoid函数，意味着取值处在 (0, 1) 间，并且大概率接近0或1。0/1的含义就很清楚了，可以用来表示更新/不更新。所以$c^{<t>}$的更新公式可以写成：
$$
c^{<t>}=\Gamma_{u} * \tilde{c}^{<t>}+\left(1-\Gamma_{u}\right) * c^{<t-1>}
$$
其中的运算符$*$表示逐元素乘法。

这就是门的作用，可以决定是否更新记忆细胞。对于 cat 的例子，可以理解为判断是否已经过了要用到 was/were的地方。架构大致如下：

![3.png](https://upload-images.jianshu.io/upload_images/16793245-76407435f8184c13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



理解了GRU的思想之后，还有一个完整的版本，会多一个**相关门**：
$$
\Gamma_{r}=\sigma\left(W_{r}\left[c^{<t-1>}, x^{<t>}\right]+b_{r}\right)
$$
$r$表示相关性（relevance），其作用在于计算候选值$\tilde{c}^{<t>}$与$c^{<t-1>}$的关联性。这需要更改$\tilde{c}^{<t>}$的计算公式为：
$$
\tilde{c}^{<t>}=\tanh \left(W_{c}\left[\Gamma_{r}*c^{<t-1>}, x^{<t>}\right]+b_{c}\right)
$$

#### 14.9.2 工作原理

上面只讲了GRU的实现，现在回到我们一开始想要解决的问题：梯度消失。

$c^{<t>}$的更新公式告诉我们，只有在需要用到这个记忆的时候才可能更新。因此$\Gamma_{u}$会长时间为接近0的数，近乎保证了$c^{<t>}=c^{<t-1>}$，也因此让梯度消失出现的可能性下降了。

### 14.10 长短期记忆（LSTM（long short term memory）unit）

#### 14.10.1 LSTM用到的公式

​	在GRU的基础上做出如下修改

$\tilde{c}^{\langle t\rangle}=\tanh \left(\omega_{i}\left[a^{(t-1\rangle}, x^{(t)}\right]+b_{c}\right)$

$\Gamma_{u}=\sigma\left(\omega_{n}\left[a^{(t-1\rangle}, x^{(t\rangle}\right]+b_{u}\right)$

$\Gamma_{f}=\sigma\left(\omega_{t}\left[a^{(t-1)}, x^{\langle t\rangle}\right]+b_{f}\right)$

$\Gamma_{o}=-\left(\omega_{o}\left[a^{(t-1)}, x^{(t)}\right]+b_{o}\right)$

$c^{(t\rangle}=\Gamma_{u} \times \tilde{c}^{\langle t\rangle}+\Gamma_{f} * c^{\langle t-1\rangle}$

$a^{\langle t\rangle}=\Gamma_{o} * c^{\langle t\rangle}$

​	LSTM中我们不再有和GRU相同的的情况，在GRU更新门控制公式中，我们将用不同的项来代替它们，要用$\Gamma_{f}$来取代GRU中的$1-\Gamma_{u}$，这个$\Gamma_{f}$也被称为遗忘门，也就是说，和GRU由两个门控制相比，LSTM的最大区别就在于其有三个门，分别为更新门和遗忘门以及输出门来进行控制。（$\Gamma_{o}$是输出门）。

#### 14.10.2LSTM单元图

![image-20201102204204363.png](https://upload-images.jianshu.io/upload_images/24439865-343141a04e7941bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​		多个LSTM单元连接图：

![image-20201102204702303.png](https://upload-images.jianshu.io/upload_images/24439865-9b08a844bd07dfc7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​		如上图所示，把多个多个LSTM单元连接起来，就是把它们按时间次序连起来，每一个单元的输出了上一个时间的a，会作为下一个时间步的输入，c也是同理。这条线显示了只要你正确地设置了遗忘门和更新门，LSTM是相当容易把的值$c^{\langle t\rangle}$（上图编号11所示）一直往下传递到右边。这就是为什么***\*LSTM\****和***\*GRU\****非常擅长于长时间记忆某个值，对于存在记忆细胞中的某个值，即使经过很长很长的时间步。

​		这里和一般使用的版本会有些不同，最常用的版本可能是门值不仅取决于$a^{\langle t\rangle-1}$和$x^{\langle t\rangle}$有时候也可以偷窥一下$c^{\langle t\rangle-1}$的值（上图编号13所示），这叫做"窥视孔连接”）。偷窥孔连接其实意思就是门值不仅取决于$a^{\langle t\rangle-1}$和$x^{\langle t\rangle}$，也取决于上一个记忆细胞的$c^{\langle t\rangle-1}$，然后“偷窥孔连接”就可以结合这三个门来计算了。

​		总的来说，GRU比LSTM少一个门，更加简单，而LSTM则由于有三个门而更加强大和灵活。





### 14.11 双向循环神经网络（Bidirectional RNN）：

#### 14.11.1 BRNN作用

​	这个模型可以让你在序列的某点处不仅可以获取之前的信息，还可以获取未来的信息

#### 14.11.2 BRNN原理

如下图所示，我们需要对下列语句进行分析

![image-20201102211439847.png](https://upload-images.jianshu.io/upload_images/24439865-7ca4657c08cc924a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​	在判断第三个词Teddy是不是人名的一部分时，光看句子前面部分是不够的，为了判断$y^{\langle t\rangle}$是0还是1，除了前3个单词，你还需要更多的信息，因为根据前3个单词无法判断他们说的是Teddy，还是其他的词。

​	为了解决上述问题，需要使用双向RNN进行实现。

![image-20201102212346484.png](https://upload-images.jianshu.io/upload_images/24439865-495514eb13eafb6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​	如上图所示，我们用四个输入或者说一个只有4个单词的句子，这样输入只有4个，$x^{\langle 1\rangle}$到$x^{\langle 4\rangle}$。从这里开始的这个网络会有一个前向的循环单元叫做$a^{\langle 1\rangle}$到$a^{\langle 4\rangle}$，我在这上面加个向右的箭头来表示前向的循环单元，并且他们互相连接。这四个循环单元都有一个输入进去，得到预测的$y^{\langle 1\rangle}$到$y^{\langle 4\rangle}$。

![image-20201102213027606.png](https://upload-images.jianshu.io/upload_images/24439865-aab74c812af016d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​	我们想要增加一个反向循环层，这里有个左箭头代表反向连接，如图中绿色的$a^{\langle 1\rangle}$到$a^{\langle 4\rangle}$，所以这里的左箭头代表反向连接。同样，我们把网络这样向上连接，这个a反向连接就依次反向向前连。这样，这个网络就构成了一个无环图。

​	给定一个输入序列，$x^{\langle 1\rangle}$到$x^{\langle 4\rangle}$，这个序列从左到由计算$a^{\langle 1\rangle}$到$a^{\langle 4\rangle}$。而反向序列从右到左反向进行。你计算的是网络激活值，这不是反向，而是前向的传播，而图中这个前向传播一部分计算是从左到右，一部分计算是从右到左。把所有这些激活值都计算完了就可以计算预测结果了。

![image-20201102214221377.png](https://upload-images.jianshu.io/upload_images/24439865-3cc0aceb497b3838.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​	举个例子，为了预测结果，你的网络会有如$\hat{y}^{<t>}, \hat{y}^{<t>}=g\left(W_{g}\left[\vec{a}^{<t>}, \bar{a}^{<t>}\right]+b_{y}\right)$。在结合了左右两个方法的预测值带入到前面的式子当中，即可得到预测结果（即对$y^{\langle t\rangle}$的计算方式结合了左右两个方向运算而来的$a^{\langle t\rangle}$）。

​	这就是双向循环神经网络，并且这些基本单元不仅仅是标准***\*RNN\****单元，也可以是***\*GRU\****单元或者***\*LSTM\****单元。



### 14.12 深层循环神经网络（Deep RNNs）

​		我们首先对一个简单的单元进行分析。

​		一个标准的神经网络，首先是输入x，然后堆叠上隐含层，所以这里应该有激活值，比如说第一层是$a^{\langle 1\rangle}$，接着堆	叠上下一层，激活值$a^{\langle 2\rangle}$，可以再加一层，$a^{\langle 3\rangle}$然后得到预测值$\hat{y}$。

![image.PNG](https://upload-images.jianshu.io/upload_images/24439865-cbae308f32a5727d.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​		深层神经网络也是同理，只是多个单元的连接来进行实现，具体如下图所示：

![图片1.png](https://upload-images.jianshu.io/upload_images/24439865-5340e434daf184ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​		这是我们一直见到的标准的RNN（上图边框中），这里的符号有点不同，不再用原来的$a^{\langle 0\rangle}$表示0时刻的激活值了，而是用$a^{\langle 1\rangle}$来表示第一层，所以我们现在用$a^{[l]\langle t\rangle}$，来表示第l层的激活值，这个<t>表示第t个时间点，如上图所示，构建了三层隐层，并且堆叠在一起，进而得到了这个新的网络。

​		对于各个节点都有一个算法进行计算，我们看个具体的例子，看看$a^{[2]\langle 3\rangle}$这个值（上图编号5所示）是怎么算的，激活值$a^{[2]\langle 3\rangle}$有两个输入，一个是从下面过来的输入（上图编号6所示），还有一个是从左边过来的输入（上图编号7所示），$a^{[2]<3>}=g\left(\omega_{a}^{[2]}\left[a^{[2]<2>}, a^{[1]<3>}\right]+b_{a}^{[2]}\right)$这就是这个激活值的计算方法。参数w和b则是这一层的参数。

​		你可能见过很深的网络，甚至于100层深，而对于RNN来说，有三层就已经不少了。由于时间的维度，RNN网络会变得相当大，即使只有很少的几层，很少会看到这种网络堆叠到100层。但有一种会容易见到，就是在每一个上面堆叠循环层，把输出去掉，然后换成一些深的层，这些层并不水平连接，只是一个深层的网络，来分别预测各个y的值，这几个深层网络没有水平方向上的连接。

​	我们前面所说的单元，没必要非是标准的RNN，最简单的RNN模型，也可以是***\*GRU\****单元或者***\*LSTM\****单元，并且，你也可以构建深层的双向RNN网络。由于深层的RNN训练需要很多计算资源，需要很长的时间，尽管看起来没有多少循环层，这个也就是在时间上连接了三个深层的循环层，你看不到多少深层的循环层，不像卷积神经网络一样有大量的隐含层。



补充：

seq 2 seq：https://www.jianshu.com/p/b2b95f945a98

bert：https://zhuanlan.zhihu.com/p/48612853

transformer：https://www.jianshu.com/p/923c8b489604

code 2 seq：https://zhuanlan.zhihu.com/p/301058441?utm_source=wechat_session

ALBERT(albert) ：https://blog.csdn.net/u012526436/article/details/101924049

