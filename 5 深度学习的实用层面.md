## 5 深度学习的实用层面

### 5.1 训练集(Train sets)

在机器学习中，通常将训练数据分为**训练集**(train sets)、简单交叉验证集（简称**验证集**，dev sets）和**测试集**(test sets)。对训练集执行训练算法，通过验证集选择最好的模型，经过充分验证选定最终模型后，在测试集上进行评估。
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5vvIkID5DOHuzIh1uab4y2MX3Mhupz6LidRyc8PMkqRkY6N6.MQBfLExrzd.t6iFTjcCVCeJ*6Evjj1rp2wrAXw!/b&bo=7wNXAAAAAAADB5k!&rf=viewer_4)

**数据集划分比例：**

- **对数据集规模较小的：**
   如果没有明确设置验证集，常见做法是将所有数据**三七分**（70%训练集和30%测试集）；或者划分为60%训练集、20%验证集和20%测试集。
- **对数据集规模较大的：**
  **验证集和测试集占数据总量的比例会趋向于变得更小（低于数据总量的20%甚至10%）**。因为验证集的目的是验证不同的算法中哪一种更有效，故验证集要足够大才能评估，比如现有100万条数据，则取1万条便足以评估算法性能。同样地，根据最终选择的分类器，测试集的主要目的是正确评估分类器的性能，若拥有百万数据，只需1万条数据便足以评估单个分类器。所以倘若有100万条数据，可以将其中1万条作为验证集1万条作为测试集，即98%训练集、1%验证集和1%测试集。对于数据量过百万的应用，训练集可以达到99.5%，验证集和测试集各占0.25%或者验证集占0.4%测试集占0.1%。

**对验证集和测试集的要求：**
1. 确保**验证集和测试集的数据来自同一分布**。因为要用验证集评估不同的模型以尽可能地优化性能，如果验证集和测试集来自同一分布会更好，机器学习算法会训练得更快。
2. **可以没有测试集**。测试集的目的是对最终选定的神经网络系统做出无偏评估，若不需要无偏评估，就可以不设置测试集。在只有训练集和验证集的情况下，需要在训练集上训练、尝试不同的模型框架，在验证集上评估这些模型，迭代并选出最适合的模型，训练集仍称为训练集，而验证集被称为测试集。

搭建训练测试集和验证集能够加速神经网络的集成，更有效地衡量算法的偏差和方差，从而更高效地选择合适的方法优化算法。



### 5.2 偏差和方差(Bias/Variance)

我们先定义好我们要用到的公式中变量的含义：

![preview](https://i.loli.net/2021/04/11/wUqc69PosKVTpgZ.jpg)

#### **5.2.1 偏差**

​		描述的是预测值的期望与真实值之间的差距。偏差越大，越偏离真实数据，拟合度越低，如下图第二行所示。故偏差度量的是学习算法的期望预测与真实结果的偏离程度，即**拟合能力**。

​        期望输出与真实标记的差别称为偏差，即：
$$
\operatorname{bias}^{2}(\boldsymbol{x})=(\bar{f}(\boldsymbol{x})-y)^{2}
$$

#### **5.2.2 方差**

​		描述的是预测值的变化范围，即离散程度。方差越大，数据的分布越分散，如下图右列所示。故方差度量的是同样大小的训练集的变动所导致的学习性能的变化，即刻画了**数据扰动**造成的影响。

​        使用样本数相同的不同训练集产生的**方差**为：
$$
\operatorname{var}(\boldsymbol{x})=\mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x}))^{2}\right]
$$
​       

![](https://pic4.zhimg.com/162bbe3ae6c8f46da4f4e05edea2d9fc_r.jpg?source=1940ef5c)

#### **5.2.3 泛化**

​		学习后的模型对未知数据的预测能力。在实际情况中，通常通过测试误差来评价学习方法的泛化能力，泛化误差（generalization error）是新输入的误差期望，为偏差、方差与噪声之和，给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

​         泛化误差 $=$ 错误率 $($ error $)=\operatorname{bias}^{2}(x)+\operatorname{var}(x)+\varepsilon^{2}$

​         也就是说，泛化误差可以通过一系列公式分解运算证明：泛化误差为偏差、方差与噪声之和。**证明过程如下：**为了便于讨论，我们假定噪声期望为零，即$E_{D}\left[y_{D}-y\right]=0$。通过简单的多项式展开合并，可对算法的期望泛化误差进行分解：
$$
\begin{aligned}
E(f ; D)=& \mathbb{E}_{D}\left[\left(f(\boldsymbol{x} ; D)-y_{D}\right)^{2}\right] \\
=& \mathbb{E}_{D}\left[\left(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x})+\bar{f}(\boldsymbol{x})-y_{D}\right)^{2}\right] \\
=& \mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x}))^{2}\right]+\mathbb{E}_{D}\left[\left(\bar{f}(\boldsymbol{x})-y_{D}\right)^{2}\right] \\
&+\mathbb{E}_{D}\left[2(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x}))\left(\bar{f}(\boldsymbol{x})-y_{D}\right)\right] \\
=& \mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x}))^{2}\right]+\mathbb{E}_{D}\left[\left(\bar{f}(\boldsymbol{x})-y_{D}\right)^{2}\right] \\
=& \mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x}))^{2}\right]+\mathbb{E}_{D}\left[\left(\bar{f}(\boldsymbol{x})-y+y-y_{D}\right)^{2}\right] \\
=& \mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x}))^{2}\right]+\mathbb{E}_{D}\left[(\bar{f}(\boldsymbol{x})-y)^{2}\right]+\mathbb{E}_{D}\left[\left(y-y_{D}\right)^{2}\right] \\
&+2 \mathbb{E}_{D}\left[(\bar{f}(\boldsymbol{x})-y)\left(y-y_{D}\right)\right] \\
=& \mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\bar{f}(\boldsymbol{x}))^{2}\right]+(\bar{f}(\boldsymbol{x})-y)^{2}+\mathbb{E}_{\bar{D}}\left[\begin{array}{ll}
\boldsymbol（{y}_{D}-y）^2 &
\end{array}\right] \\
\end{aligned}
$$
于是, 最终得到:
$$
E(f ; D)=\operatorname{bias}^{2}(\boldsymbol{x})+\operatorname{var}(\boldsymbol{x})+\varepsilon^{2}
$$
"偏差-方差分解" 说明, 泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度 所共同决定的。给定学习任务, 为了取得好的泛化性能, 则需使偏差较小, 即能够充分拟合数据 并且使方差较小, 即使得数据扰动产生的影响小

#### 5.2.4 拟合

- **欠拟合**(Underfitting)：常在模型学习能力较弱而数据复杂度较高的情况下出现。由于模型的学习能力不足，无法学习到数据集中的“一般规律”，因而导致模型的泛化能力较弱。
- **过拟合**(Overfitting)：常在模型学习能力较强而数据复杂度较弱的情况下出现。此时由于模型的学习能力太强，以至于将训练集中单个样本自身的特点都能捕捉到，并将其认为是“一般规律”，这种情况同样会导致模型的泛化能力下降。
- **适度拟合**(just right)：介于欠拟合和过拟合之间，数据拟合适度，复杂程度适中，泛化能力强。

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcZuFHkMq09r1sktWqx9yvDjP06bfhJCxUhKSer0w7GaQ3aZPIxnbtp02*J6w3RaYXwgwnhB9Bixcxn6oyC4ofM8!/b&bo=RQPlAAAAAAADF5E!&rf=viewer_4)

#### 5.2.5 偏差和方差与训练集和验证集误差的关系

理解偏差和方差的两个关键数据是**训练集误差**(Train set error)和**验证集误差**(Dev set error)。假设人眼辨别错误率接近0（即最优误差或称为贝叶斯误差接近0），训练集和验证集数据来自相同分布，则可通过查看训练集误差判断数据拟合情况，从而判断是否有偏差问题（训练集误差越大，拟合度越低，偏差越高）；然后查看错误率(验证集误差-训练集误差)有多高，判断方差是否过高（错误率越大，方差越高）。
下表是在不同的训练集和验证集的误差组合情况下对偏差和方差的分析：
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcZuFHkMq09r1sktWqx9yvDh0p1FGR..g7BePGDEynjRT64TPDl9fyA47S6Tr9qgjq5xkw*7D7AZxmi.CF4hjNUI!/b&bo=FgWhAQAAAAADF4E!&rf=viewer_4)




![](https://upload-images.jianshu.io/upload_images/24408091-57c2c99bc7f69034.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 5.2.6 **偏差、方差与bagging、boosting的关系**

Bagging算法是对训练样本进行采样，产生出若干不同的子集，再从每个数据子集中训练出一个分类器，**取这些分类器的平均，所以是降低模型的方差（variance）**。Bagging算法和Random Forest这种并行算法都有这个效果。

Boosting则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，**所以随着迭代不断进行，误差会越来越小**，所以模型的偏差（bias）会不断降低。

#### 5.2.7 **偏差、方差和K折交叉验证的关系**

K-fold Cross Validation的思想：将原始数据分成K组 $($ 一般是均分 $)$, 将每个子集数据分别做一次 验证集, 其余的K-1组子集数据作为训练集, 这样会得到K个模型, 用这K个模型最终的验证集的分 类准确率的平均数作为此K-CV下分类器的性能指标。
对于一系列模型 $F(\hat{f}, \theta)$, 我们使用Cross Validation的目的是获得预测误差的无偏估计量CV, 从而可以用来选择一个最优的Theta*,使得CV最小。假设K-folds cross validation, CV统计量定 义为每个子集中误差的平均值, 而K的大小和CV平均值的bias和variance是有关的:
$C V=\frac{1}{K} \sum_{k=1}^{K} \frac{1}{m} \sum_{i=1}^{m}\left(\hat{f}^{k}-y_{i}\right)^{2}$
其中， $\mathrm{m}=\mathrm{N} / \mathrm{K}$ 代表每个子集的大小, $\quad$N是总的训练样本量, K是子集的数目。当K较大时, $\mathrm{m}$ 较小，模型建立在较大的N-m上, 经过更多次数的平均可以学习得到更符合真实数 据分布的模型, Bias就小了，**但是这样一来模型就更加拟合训练数据集, 再去测试集上预测的时候 预测误差的期望值就变大了，从而Variance就大了**; $\mathrm{k}$ 较小的时候, 模型不会过度拟合训练数据, 从而Bias较大，**但是正因为没有过度拟合训练数据, Variance也较小**。

### 5.3 处理偏差和方差

#### 5.3.1 针对不同偏差的不同处理

评估训练集或训练数据的性能，若偏差较高，可以选择以下几种方法：
- 选择更大的网络，比如含有更多隐藏层或隐藏单元的网络
- 花费更多时间训练网络（不一定有用，但没坏处）
- 尝试更先进的优化算法
- 选择更适合解决此问题的神经网络架构（可能有用，可能没用）

训练学习算法时，不断尝试这些方法，直到可以拟合数据，解决掉偏差问题为止，至少能够拟合训练集。
#### 5.3.2 针对不同方差的不同处理
偏差降低到可以接受的数值，查看验证集性能来评估方差，如果方差高，可以选择下列方法：
- 采用更多数据（最好的解决办法）
- 正则化
- 尝试更适合的神经网络框架（有时可以同时减少偏差和方差）

解决高方差最优方法是准备更多的数据，但可能无法实时准备足够多的数据或数据成本很高时，**正则化**可帮助避免过拟合，减少网络误差。

通过不断重复尝试，找到一个低偏差、低方差的框架。
> 高偏差和高方差尝试的方法可能完全不同，明确这点来选择最有效的方法。
只要正则适度，通常构建更大网络便可以在不影响方差的同时减少偏差。采用更多数据通常可以在不过多影响偏差的同时减少方差。我们有很多的选择可以减少偏差或方差而不影响另一方。


### 5.4 正则化

**定义**：一种为了减小方差（测试误差）的行为
**作用**：使用比较复杂的模型比如神经网络，去拟合数据时，容易出现过拟合现象，导致模型的泛化能力下降，这时我们使用正则化，降低模型的复杂度，让模型在面对新数据的时候，可以有很好的表现。

#### 5.4.1 logistic 回归的正则化
- **L2 正则化**
                                                                                             $J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m}\|w\|_{2}^{2}$
每次更新w时：

$$w:=w-\alpha d w=\left(1-\frac{\alpha \lambda}{m}\right) w-\frac{\partial J}{\partial w}$$

从上式看出，每次更新对特征系数做一个**比例的缩放**而不是像L1正则化减去一个固定值。这使得系数趋向变小而不会变为0，因此L2正则化**让模型变得更简单，防止过拟合**。
- **L1 正则化**
                                                                                            $$J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m}\|w\|_{1}$$

每次更新w时：
$$\begin{array}{c}
w:=w-\alpha d w\\
=w-\frac{\alpha \lambda}{2 m} \operatorname{sign}\left(w\right)-\frac{\partial J}{\partial w}
\end{array}$$
若 w 为正数，则每次更新减去一个常数；若 w 为负数，则每次更新加上一个常数，故容易产生特征的系数为0的情况，特征系数为 0 表示该特征不会对结果有任何影响，因此L1正则化让w变得稀疏，起到**特征选择**的作用。
> - 参数 w 向量的L2 范数：$\|w\|_{2}^{2}=\sum_{j=1}^{n_{x}} w_{j}^{2}=w^{T} w$ 
参数 w 向量的L1 范数：$\|w\|_{1}=\sum_{j=1}^{n_{x}}\left|w_{j}\right|$
> - $w\in \mathbb{R}^{n_{x}}, b \in \mathbb{R}$，习惯只正则化w,省略参数b。因为w通常是一个高维参数矢量，b只是单个数字，即w几乎涵盖所有参数，b只是众多参数中的一个，b通常省略不计，加上也没问题。
>- $\lambda$是正则化参数，是一个需要调整的超参数，Python 编程语言中，lambda 是一个保留字段，编写代码时，删掉a，写成lambd,以免冲突。

**总结**
两者都通过加上一个和项来限制参数大小，效果却不同：L1正则化适用于**特征选择**，L2正则化适用于**防止模型过拟合**。
在训练网络时，倾向于使用**L2正则化**。



#### 5.4.2 神经网络中实现L2正则化

​                                                                           $$J\left(w^{[1]}, b^{[1]}, \ldots, w^{[L]}, b^{[L]}\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m} \sum_{l=1}^{L}\|w\|_{F}^{2}$$
**Frobenius 范数**：
下标F标注，表示一个矩阵中所有元素的平方和。
​                                                                                                     $$\|w\|_{F}^{2}=\sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}}\left(w_{i j}^{[l]}\right)^{2}$$

其中，$w:\left(n^{[l-1]}, n^{[l]}\right)$ $n^{[l]}$表示第$l$层单元数量，$n^{[l-1]}$表示第$l-1$层单元数量

实现梯度下降：
                                                                                                     $$\begin{aligned} w^{[l]}: &=w^{[l]}-\alpha\left[(\text { from backprop })+\frac{\lambda}{m} w^{[l]}\right] \\ &=\left(1-\frac{\alpha \lambda}{m}\right) w^{[l]}-\alpha(\text { from backprop }) \end{aligned}$$
实际上，相当于给矩阵$w^{[l]}$乘上$1-\frac{\alpha \lambda}{m}$倍的权重，而该系数小于1，因此L2正则化也被称为**权重衰减**



### 5.5 正则化有效的原理
- 正则化参数$\lambda$增加到足够大，权重矩阵$w^{[l]}$会接近于0，大量隐藏单元的影响变得更小了，神经网络越来越简单，接近逻辑回归，这样更不容易发生过拟合。

举例：
假设使用$\tanh (z)$激活函数，如果$z$的值始终在接近原点的小范围内，激活函数大致呈线性，每层几乎都是线性的，和线性回归一样。

![](https://upload-images.jianshu.io/upload_images/24408091-e494de17a5ddc9af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
当正则化参数$\lambda$足够大，参数$w$很小,$z$也会相对变小，激活函数$\tanh (z)$会相对呈线性，整个神经网络会计算接近线性函数的值，即一个很简单的函数，而不是极复杂的高度非线性函数，不会发生过拟合。

参考文章：[机器学习中的正则化](https://www.jianshu.com/p/569efedf6985)





### 5.6 Dropout正则化 ###
Dropout正则化是较常用的避免过拟合的方法之一。

**Dropout**

首先介绍一下Dropout正则化：在**训练**神经网络时，对于每一层神经网络中的所有节点，每个结点都有keep_prob的概率保留，1 - keep_prob的概率丢弃。这里的丢弃指的是**每一次**前向传播的时候随机丢弃部分结点，每次前向传播都重新计算要丢弃的结点。要注意在更新权值和真正应用已经训练好的网络时，是不会随机丢弃的。
![Dropout. 左图为丢弃前的网络，右图为丢弃后的网络](https://upload-images.jianshu.io/upload_images/16793245-49d0d6f5c9e2441e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**原理**

有没有觉得很神奇，竟然随机丢掉一些结点反而能“更好”地训练神经网络，也太“玄乎”了。当然，能生效的正则化肯定有它成功的理由，下面给出解释：
1. 随机删除一些结点，本质上相当于缩小网络。而过拟合的原因之一是网络太大而导致的拟合效果“好过头”了，那缩小网络自然能一定程度上避免过拟合。
2. 由于所有节点都可能被丢弃，所以训练时不会过渡依赖任何一个结点（特征），表现为尽可能缩小所有权值。那就有点接近L2正则化那种收缩权值的效果了。

**代码实现**

    # 操作 < keep_prob 可使本来的随机向量变为True、False组成的向量
    d3 = np.random.randn(a3.shape[0], a3.shape[1]) < keep_prob
    # 实际运算时，True、False会变为1和0
    a3 *= d3
    # 修正a3的均值，保持和Dropout前不变
    a3 /= keep_prob

技术细节上，用了一个小于运算符自动将随机值转换为0、1；在将部分值归0后，a3的均值也会变为原来的keep_prob倍，所以最后对所有数除以keep_prob以保证a3的均值不变，保证$z3=wa3+b$的均值不变




### 5.7 其他正则化方法 ###
#### 5.7.1 数据增强/Data Augmentation ####
前文提到解决high variance（高方差）/ 过拟合的解决办法之一是收集更多的数据，更大的训练集。但有时候并不是那么容易就能短时间内收集的。所以提出了一个基于已有数据的增加数据量的方法：数据增强。
具体方法：

1. 翻转（水平、上下等）
2. 随机裁剪。指放大后旋转、裁剪
3. 扭曲变形，有很多更具体的成熟的方法。

但是数据增强并不是万用的方法，有时候可能会对网络的训练造成负面影响：比如从图片中提取文字的网络不适应翻转等等。甚至识别人脸的网络也不适合翻转，原因是“视觉手性”，感兴趣的可以阅读[Visual Chirality](https://arxiv.org/abs/2006.09512
)。

#### 5.7.2 Early Stopping ####
Early stopping顾名思义，提早结束（训练）。具体操作是：

1. 训练足够多的次数，并且记录train set和dev set的error曲线。如图
![训练过程中的error曲线.jpg](https://upload-images.jianshu.io/upload_images/16793245-496e3c48695686bf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

2. 找到dev error曲线的最低点，该点的迭代次数（附近）作为最佳迭代次数。

原理也很简单，在网络过拟合之前停止训练，不就能阻止网络过拟合了嘛。操作起来也很简单，训练一次就知道下次要训练多久了。不过简单的做法自然带来偷懒的弊端。过早停止训练，意味着网络可能还没得到充分的训练，过早停止优化cost函数，虽然低方差但是可能高偏差。


### 5.8 正则化输入 ###
**命名问题**

首先提一下有关归一化和标准化的命名问题。如果你去百度一下很容易就发现吴恩达老师教的归一化（Normalizing）和百度的不一样，倒是和百度的标准化（Standardization）差不多。大名鼎鼎的吴恩达老师竟然会犯这种低级错误？笔者一开始也这么认为，但是想了想我只是刚入门的怎么可以那么轻易怀疑大师。于是在多百度了几次（无果）、查几篇国内外论文、最后一次百度（知乎）后，得出了结果：
归一化和标准化都是一种特征放缩（Feature scaling）。特征放缩有很多种方法，这里只讲涉及到的这两种。

1. min-max normalizing(Rescaling)，也就是百度的归一化normalizing：
                                                                                         $$
                                                                                             x^{\prime}=\frac{x-\min (x)}{\max (x)-\min (x)}
                                                                                         $$
                                                                                         
2. Z-score normalization(Standardization)，也就是百度的标准化:
                                                                                             $$
                                                                                             x^{\prime}=\frac{\left(x-\mu\right)}{\sigma}
                                                                                             $$

猜测吴恩达老师是简称Z-score normalization为normalization了？也可能是其他原因吧。彻底帮我解决问题的[回答](https://www.zhihu.com/question/20467170)。

**为什么要Z-score normalization**

有时候我们的数据的分布并不是“很好看”（图1、2），不是很利于网络的训练。这时候我们就可以考虑处理一下数据，使得数据的分布“更好看”（图3、4）。**注意**：图片中的名称用的是吴恩达老师课程中的名称。
![图1](https://upload-images.jianshu.io/upload_images/16793245-ecb059ac267799e1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![图2](https://upload-images.jianshu.io/upload_images/16793245-9c3ee55b4cdf4f32.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![图3](https://upload-images.jianshu.io/upload_images/16793245-0a028cf5ef1f0b16.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![图4](https://upload-images.jianshu.io/upload_images/16793245-5bc29e124bf0c4f4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
什么时候数据会不好看？当输入的特征$x_{1}$和$x_{2}$取值范围有很大差别时就会，比如：$x_{1}$在[0,1]内，$x_{2}$在[1, 100]内。这样就会导致数据的分布为椭圆形，那在梯度下降时如果学习率不够小很容易就会一直震荡而艰难收敛；而更对称的分布，学习率就算大一点也很容易就能收敛。
所以一般当输入特征的取值范围差距较大时就考虑Z-score normalization，不过无脑使用也没什么坏事，所以是比较推荐的方法。

**实现**

首先要注意的一点是，尽量别分多次计算均值$\mu$和方差$\sigma$，比如这两个值都由训练集得出就不更改了。
Python代码：

    # 计算均值
    x_mean = np.mean(x)
    # 计算标准差；计算方差用np.var
    x_std = np.std(x)
    # 标准化
    x = (x - x_mean) / x_std

多提一句，对于图片，如果想实现min-max normalizing，一般只需要默认0为最小值，255为最大值即可：

    # x = (x-0) / (255-0)
    x = x / 255




### 5.9 梯度消失和梯度爆炸 ###
#### 5.9.1 介绍 ####
英文称：vanishing and exploding gradients。
首先简单“展示”一下梯度爆炸是怎么样的：
假设有$L$层，$\text W^{[l]}=\left[\begin{array}{cc}
1.5 & 0 \\
0 & 1.5
\end{array}\right],
b^{[l]}=0$（$l∈[1, L]$），且激活函数全部采用线性激活（恒等函数），则

​                                                                                                          $$\begin{array}{l}
\hat{y}=W^{[L]}W^{[L-1]} \ldots W^{[1]}x \\
=\left[\begin{array}{cc}
1.5 & 0 \\
0 & 1.5
\end{array}\right]^{L}x \\
=\left[\begin{array}{cc}
1.5^{L} & 0 \\
0 & 1.5^{L}
\end{array}\right]x
\end{array}$$

可见，$W^{[l]}$中大于1的元素，将会因指数级增长而“爆炸”；同理，小于1的元素，则会以指数级递减而“消失”（真实情况会更复杂一些，但上述例子足以说明这种可能性）。
这不仅仅适用于$\hat{y}$的计算，还适用于梯度$dW、db$的计算——过深的网络在梯度下降时计算的梯度很有可能接近0或过大。这会直接导致网络的学习停滞不前或者发散，严重影响学习效果。

#### 5.9.2 权重初始化 ####
既然梯度消失和爆炸都是由于$L$过大而形成的指数级缩小/放大，那在不减少$L$的前提下比较有限的方法就是让每次输出尽可能接近1了。$z$的计算可写成：
$$z=w_{1}a_{1}+w_{2}a_{2}+\ldots+w_{n}a_{n}$$那么当$n$增大时，想让$z$减小则可以通过减小$w_{i}$做到；而$w_{i}$是符合高斯分布（标准正态分布，np.random.randn()）的，所以$z$可**看似**服从分布$N(0, n)$，那$w_{i}$除以$n$就可以让$z$近似服从分布$N(0, 1)$，已达到。
具体做法是初始化时，用以下语句：

    w[l] = np.random.randn(n[l], n[l-1])*np.sqrt(2 / (n[l-1]))

其中*sqrt*中的表达式和使用的激活函数有关。ReLU采用上式（$\frac{2}{n^{l-1}}$），tanh则采用$\frac{1}{n^{[l-1]}}$，其他形式的有$\frac{2}{n^{[l+1]}+n^{[l]}}$。
这么做的目的主要是修改$W^{[l]}$的分布，将方差除以输入

#### 5.9.3 梯度逼近 ####
我们在计算梯度时用的是“手写”的公式（自己推导的公式），可能有时候会犯一点小错误。那我们就需要对梯度的计算进行验算，看看有没有错。但是不可能用同样的式子进行验算，因为计算机不会产生“计算失误”。所以我们需要不同的，计算结果很接近的方法来重新计算一次，那就是梯度逼近。
回顾高数/高中知识，梯度（导数）是切线斜率。近似地我们可以通过计算一个点附近的割线的斜率来代替该点的切线斜率。如图，可以用$\frac{ f(\theta+\varepsilon)-f(\theta-\varepsilon)}{2\varepsilon}$代替$f^{\prime}(\theta)$。当然，$\varepsilon$越小越接近。图来自吴恩达课程的1.12梯度的数值逼近。
![割线代替切线](https://upload-images.jianshu.io/upload_images/16793245-3737cd36a20a50e2.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
可能会有人疑惑为什么不采用单边，比如$\frac{ f(\theta+\varepsilon) - f(\theta)}{\varepsilon}$，而是要这样两边都取。抽象地你可以理解为单边计算出来的斜率更接近$\theta$的左右偏移一点点的$\theta^{\prime}$的值；而双边计算$\theta$正好位于割线的“正中间”，计算出来的斜率也自然要更接近一些。

#### 5.9.4 梯度检验 ####
进入正题，介绍如何进行检验。首先对于所有参数$W^{[l]}$和$b^{[l]}$，按顺序链接成一个参数集$\theta$。对于$dW^{[l]}$和$db^{[l]}$也同样链接成$d\theta$。这里分别对应前向传播和反向传播。$\theta$和$d\theta$有一样的shape，$\theta[i]$表示第$i$个参数。然后计算$d\theta$的近似值$d\theta_{approx}$：
1. 遍历参数集$\theta$中的$\theta[i]$。
2. 计算$$d\theta_{approx}[i] = \frac{J(\theta[1], \theta[2], \ldots, \theta[i]+\varepsilon, \ldots)-J(\theta[1], \theta[2], \ldots, \theta[i] - \varepsilon, \ldots )} { 2\varepsilon} $$
3. 计算$\Delta = \frac{\|d\theta_{approx}-d\theta\|} {\|d\theta_{approx}\|+\|d\theta\|}$欧几里得范数，实际上就是根号下误差平方之和。
4. 如果$\Delta > \varepsilon$（如$\varepsilon=10^{-7}$），则视为误差超出阈值，否则为正常。

#### 5.9.5 其他解决办法

- relu、leakrelu等激活函数

  Relu:思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度。

  relu的优点： 解决了梯度消失、爆炸的问题；计算方便，计算速度快，加速了网络的训练

  relu的缺点：由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）；输出不是以0为中心的

  **尽管relu也有缺点，但是仍然是目前使用最多的激活函数。**

  leakrelu激活函数在前面介绍激活函数时已经讲过了，这里就不再赘述了。

- 梯度裁剪

- batch normalization

- 残差结构

  上面的方法在下面的内容里面我们还会讲，这里就不再赘述了。

#### 5.9.6 注意细节 ####

讲一下代码实现有几个细节：
1. 要实现cost function，并且接受参数$\theta$。
2. 欧几里得范数可以用 np.linalg.norm(array) 计算。
3. 不要和Dropout一起使用。
4. 只在debug时开启，正常训练无需启动，否则会花费大量的时间。
5. 应用了L2正则化的cost function和一般的不一样，梯度计算也会不一样。



### 5.10 参考文献

https://zhuanlan.zhihu.com/p/38853908








