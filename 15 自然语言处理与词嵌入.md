### 15.1 词汇表征

在前面学习的内容中，我们表征词汇是直接使用英文单词来进行表征的，但是对于计算机来说，是无法直接认识单词的。为了让计算机能够能更好地理解我们的语言，建立更好的语言模型，我们需要将词汇进行表征。

#### 15.1.1 词嵌入

词嵌入(Word Embedding)是一种将文本中的词转换成数字向量的方法，为了使用标准机器学习算法来对它们进行分析，就需要把这些被转换成数字的向量以数字形式作为输入。词嵌入过程就是把一个维数为所有词数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量，词嵌入的结果就生成了词向量。

词向量是各种NLP任务中文本向量化的首选技术，如词性标注、命名实体识别、文本分类、文档聚类、情感分析、文档生成、问答系统等。引自[词嵌入 - 简书 ](https://www.jianshu.com/p/2fbd0dde8804)。

#### 15.1.2 one-hot 表征

在前面的一节课程中，已经使用过了one-hot表征的方式对模型字典中的单词进行表征，对应单词的位置用1表示，其余位置用0表示，如下图所示：

<img src="https://ae04.alicdn.com/kf/H848f5915e1be418d904955d968a82ef7u.png" alt="image.png" title="image.png" />

one-hot表征的缺点：这种方法将每个词孤立起来，使得模型对相关词的泛化能力不强。每个词向量之间的距离都一样，乘积均为0，所以无法获取词与词之间的相似性和关联性。

#### 15.1.3 特征表征：词嵌入

用不同的特征来对各个词汇进行表征，相对与不同的特征，不同的单词均有不同的值。如下例所示：

<img src="https://ae03.alicdn.com/kf/H80bd6d36a84b45178f5185fd8095f4014.png" alt="image.png" title="image.png" />

这种表征方式使得词与词之间的相似性很容易地表征出来，这样对于不同的单词，模型的泛化性能会好很多。下面是使用t-SNE算法将高维的词向量映射到2维空间，进而对词向量进行可视化，很明显我们可以看出对于相似的词总是聚集在一块儿：

<img src="https://ae05.alicdn.com/kf/H2d4c33e61d1f4ad69b53a51ff28dfc94L.png" alt="image.png" title="image.png" />

#### 15.1.4 编码原则

词嵌入本质就是对自然语言的一种编码方式，那自然也需要考虑一些编码原则。下列给出一些常见的编码原则：

* **最佳编码**：平均编码长度最小，用最少的内存表达相同的内容，尽可能逼近香农极限。比如让出现频率高的编码更短，出现频率低的编码更长。
* **正交**：用多维向量进行编码，且各维相互正交。比如PCA。
* **无偏见**：在词嵌入中，有时候会学到一些带有偏见的知识。因此如何消除这个偏见也是很重要的。

### 15.2 使用词嵌入

Word Embeddings对不同单词进行了实现了特征化的表示，那么如何将这种表示方法应用到自然语言处理的应用中呢？

#### **15.2.1 名字实体识别的例子：**

如下面的一个句子中名字实体的定位识别问题，假如我们有一个比较小的数据集，可能不包含durain（榴莲）和cultivator（培育家）这样的词汇，那么我们就很难从包含这两个词汇的句子中识别名字实体。但是如果我们从网上的其他地方获取了一个学习好的word Embedding，它将告诉我们榴莲是一种水果，并且培育家和农民相似，那么我们就有可能从我们少量的训练集中，归纳出没有见过的词汇中的名字实体。

<img src="https://ae03.alicdn.com/kf/H7605b9e517834dc5aa0389295e459dc2E.png" alt="image.png" title="image.png" />

#### **15.2.2  词嵌入的迁移学习：**

有了词嵌入，就可以让我们能够使用迁移学习，通过网上大量的无标签的文本中学习到的知识，应用到我们少量文本训练集的任务中。下面是做词嵌入迁移学习的步骤：

- 第一步：从大量的文本集合中学习word Embeddings（1-100B words），或者从网上下载预训练好的词嵌入模型；
- 第二步：将词嵌入模型迁移到我们小训练集的新任务上；
- 第三步：可选，使用我们新的标记数据对词嵌入模型继续进行微调。

#### **15.2.3  词嵌入和人脸编码：**

词嵌入和人脸编码之间有很奇妙的联系。在人脸识别领域，我们会将人脸图片预编码成不同的编码向量，以表示不同的人脸，进而在识别的过程中使用编码来进行比对识别。词嵌入则和人脸编码有一定的相似性。

<img src="https://ae03.alicdn.com/kf/H29b86e22958e4ad6b37c6b3857aaaa768.png" alt="image.png" title="image.png" />

但是不同的是，对于人脸识别，我们可以将任意一个没有见过的人脸照片输入到我们构建的网络中，则可输出一个对应的人脸编码。而在词嵌入模型中，所有词汇的编码是在一个固定的词汇表中进行学习单词的编码以及其之间的关系的。

### 15.3 词嵌入的特性

#### **15.3.1 类比推理特性：**

词嵌入还有一个重要的特性，它还能够帮助实现类比推理。如下面的例子中，通过不同词向量之间的相减计算，可以发现不同词之间的类比关系，man——woman、king——queen，如下图所示：

<img src="https://ae02.alicdn.com/kf/Ha6b472a5774041f4ae294b7f5b76673fN.png" alt="image.png" title="image.png" />

这种思想帮助研究者们对词嵌入建立了更加深刻的理解和认识。

计算词与词之间的相似度，实际上是在多维空间中，寻找词向量之间各个维度的距离相似度。

<img src="https://ae05.alicdn.com/kf/H6c2c1903f146420f894c4627166a4be45.png" alt="image.png" title="image.png" />

#### **15.3.2 相似度函数：**

- 曼哈顿距离（Manhattan Distance）

  顾名思义，在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离也称为“城市街区距离”(City Block distance)。

  ![曼哈顿距离](https://static.oschina.net/uploads/img/201611/14200240_ayRA.jpg)

  * 二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离：

  $$
  d_{12}=\left|x_{1}-x_{2}\right|+\left|y_{1}-y_{2}\right|
  $$

  - n维空间点a(x11,x12,…,x1n)与b(x21,x22,…,x2n)的曼哈顿距离：
    $$
    \mathrm{d}_{12}=\sum_{k=1}^{n}\left|\mathrm{x}_{1 k}-x_{2 k}\right|
    $$
    

- 切比雪夫距离 (Chebyshev Distance)

  国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？这个距离就叫切比雪夫距离。

  ![距离及相似性度量| Cheng Wei's Blog](https://scm_mos.gitlab.io/algorithm/distance-similarity-measures/chebyshev_distance.jpg)

  * 二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离：
    $$
    d_{12}=\max \left(\left|x_{1}-x_{2}\right|,\left|y_{1}-y_{2}\right|\right)
    $$
    

  - n维空间点a(x11,x12,…,x1n)与b(x21,x22,…,x2n)的切比雪夫距离：
    $$
    d_{12}=\max _{i}\left(\left|x_{1 i}-x_{2 i}\right|\right)
    $$
    

- 欧氏距离

  * 二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离（拓展到n维同理）
    $$
    d_{a b}=\sqrt{\left(x_{1}-x_{2}\right)^{2}}
    $$
    

两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离
$$
d_{a b}=\sqrt{\sum_{k=1}^{n}\left(x_{1 k}-x_{2 k}\right)^{2}}
$$

* 向量运算的形式：

$$
  d_{a b}=\sqrt{(a-b)(a-b)^{T}}
$$

- 余弦相似度函数（Cosine similarity）

  余弦相似度，又称为余弦相似性，是通过计算两个向量的夹角余弦值来评估他们的相似度。余弦相似度将向量根据坐标值，绘制到向量空间中，如最常见的二维空间。

  余弦相似度衡量的是2个向量间的夹角大小，通过夹角的余弦值表示结果，因此2个向量的余弦相似度为：
$$
  \cos \theta=\frac{A \cdot B}{\|A\| *\|B\|}
$$
  分子为向量A与向量B的点乘，分母为二者各自的L2相乘，即将所有维度值的平方相加后开方。 余弦相似度的取值为[-1,1]，值越大表示越相似。

  <img src="https://img-blog.csdn.net/20170411164251296" alt="img" style="zoom:80%;" />

  python代码实现：

  ```python
  import numpy as np
  def cosine_dis(x,y):
      num = sum(map(float,x*y))
      denom = np.linalg.norm(x)*np.linalg.norm(y)
      return round(num/float(denom),3)
  print(cosine_dis(np.array([3,45,7,2]),np.array([2,54,13,15])))
  ```

  PS: 函数参数: linalg=linear（线性）+algebra（代数），norm则表示范数。

  ```python
  x_norm=np.linalg.norm(x, ord=None, axis=None, keepdims=False)
  ```

  - x: 表示矩阵（也可以是一维）
  - ord：范数类型

| 参数       | 说明                                                         | 计算方法                                                     |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 默认       | 二范数：![l_{2}](https://private.codecogs.com/gif.latex?l_%7B2%7D) | ![\sqrt{x_{1}^{2} + x_{2}^{2} + ... + x_{n}^{2}}](https://private.codecogs.com/gif.latex?%5Csqrt%7Bx_%7B1%7D%5E%7B2%7D%20&plus;%20x_%7B2%7D%5E%7B2%7D%20&plus;%20...%20&plus;%20x_%7Bn%7D%5E%7B2%7D%7D) |
| ord=2      | 二范数：![l_{2}](https://private.codecogs.com/gif.latex?l_%7B2%7D) | 同上                                                         |
| ord=1      | 一范数：![l_{1}](https://private.codecogs.com/gif.latex?l_%7B1%7D) | ![\left \| x_{1} \right \| + \left \| x_{2} \right \| + ... + \left \| x_{n} \right \|](https://private.codecogs.com/gif.latex?%5Cleft%20%7C%20x_%7B1%7D%20%5Cright%20%7C%20&plus;%20%5Cleft%20%7C%20x_%7B2%7D%20%5Cright%20%7C%20&plus;%20...%20&plus;%20%5Cleft%20%7C%20x_%7Bn%7D%20%5Cright%20%7C) |
| ord=np.inf | 无穷范数：![l_{\infty}](https://private.codecogs.com/gif.latex?l_%7B%5Cinfty%7D) | ![MAX \left \| x_{i} \right \|](https://private.codecogs.com/gif.latex?MAX%20%5Cleft%20%7C%20x_%7Bi%7D%20%5Cright%20%7C) |

  结果：

  <img src="https://ae04.alicdn.com/kf/H409f22cdd94b49d780052efafa581271v.png" alt="image-20201128154107639.png" title="image-20201128154107639.png" />

### 15.4 嵌入矩阵

在学习词嵌入时，实际上学习的是一个嵌入矩阵，这个矩阵用于将原来的one-hot编码转化为用特征表示的向量。

#### 15.4.1 定义

假设词典共有10,000个单词，嵌入特征为300维。

$o_i$：表示一个one-hot向量（10000$\times$1），且第$i$位为1，其余位为0。比如orange在词典中的编号为6257，则对应$o_{6257}$。

$e_i$：表示一个嵌入向量（300$\times$1），对应由one-hot向量$o_i$得到的嵌入向量。

$E$：表示嵌入矩阵，shape为300$\times$10000。

有如下关系：
$$
e_i = E \cdot o_i
$$
由（3）式可知，嵌入矩阵$E$的作用在于将one-hot向量映射为嵌入向量。因此，学习词嵌入的本质是学习嵌入矩阵。

#### 15.4.3 实现技巧

观察下图，可以发现由于one-hot向量的特殊性，对于$o_i$，嵌入矩阵实际上会用影响到$e_i$的值的只有第$i$列。

![image-20201128145513666.png](https://upload-images.jianshu.io/upload_images/16793245-0feede24c6ce738f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

所以在使用代码实现的时候，我们只需要抽取出对应的列即可，无需真的将如此大的矩阵与向量做矩阵乘法。

下面是代码示例：

```
# 定义one-hot向量o_2, shape为(4, 1)
o = np.array([[0, 0, 1, 0]]).T

# 定义嵌入矩阵, shape为(2, 4)
E = np.array([
	[10, 11, 12, 13],
	[20, 21, 22, 23]
])

# 获得编号i
i = np.where(o==1)[0]

# 计算e_i, shape为(2, 1), 结果为 [ [12], [22] ]
e = E[:, i]
```

### 15.5 学习词嵌入

#### 15.5.1 简单的语言模型

假设搭建了一个用神经网络来实现的语言模型，用于预测“I want a glass of orange \_\_\_\_”。

![image-20201128155105180.png](https://upload-images.jianshu.io/upload_images/16793245-08e9bda8bdc415dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

预测的具体方法如上图：

1. 对每个单词进行one-hot编码，得到一系列$o_i$
2. 映射为嵌入向量$e_i$
3. 将这些嵌入向量都放到神经网络中，该神经网络有自己的参数$W^{[1]}$和$b^{[1]}$，输入为 (6$\times$300)=1800维
4. 最后通过softmax层激活得到预测词的one-hot向量，也有自己的参数$W^{[2]}$和$b^{[2]}$。

实际应用上，会选择每次预测只向前看4个单词（一个超参数，可以修改），则神经网络的输入会变为(4$\times$300)=1200维。

#### 15.5.2 反向传播

在搭建模型后，我们明确了这个模型的参数有$E,W^{[1]},W^{[2]},b^{[1]},b^{[2]}$，因此可以开始用反向传播进行训练。这样这个网络会发现要想最好地拟合训练集，就要使apple（苹果）、orange（橘子）、grape（葡萄）和pear（梨）等等，还有像durian（榴莲）这种很稀有的水果都拥有相似的特征向量。

#### 15.5.3 采用不同的上下文

除了刚刚提到的每次预测只使用前4个单词，还可以选择目标词左右各4个词作为上下文；也可以仅使用目标词的前一个单词进行预测。

不同的上下文会得到不同的效果，但是学习的嵌入矩阵仍会是相似的。

#### 15.5.4 相关的重要人物

Yoshua Bengio，Rejean Ducharme，Pascal Vincent，Rejean Ducharme，Pascal Vincent还有Christian Jauvin。

### 15.6 Word2Vec

Word2Vec有两个版本，其中Skip-Gram是更常用的那个，另一个是CBOW。下面先介绍Skip-Gram。

#### 15.6.1 Skip-Gram模型

Skip-Gram模型的思想是，学习给定嵌入向量$e_c$预测对应的$e_t$。但是由于一个$e_c$是可以对应多个$e_t$的（因为一个词附近出现的词有很多个，不可能一直只有一个），所以最后学习的结果会将$e_c$附近的词都认为拥有相似的特征。

![image-20201128161538680.png](https://upload-images.jianshu.io/upload_images/16793245-7c2790047b3a942f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在该模型中，上下文不一定总是目标单词之前离得最近的四个单词，或最近的n个单词。在给定一个训练样本（如“I want a glass of orange juice to go along with my cereal.”）该模型的算法如下：

1. **随机**选一个词作为上下文 $c$（比如orange）

2. 然后基于 $c$，随机在一定词距内选另一个词作为目标词 $t$（比如juice，也可能是后方的my或者前方的glass）

3. 根据$c,t$分别得到one-hot向量$o_c,o_t$以及嵌入向量$e_c$。那一个训练样本就是（$e_c$，$o_t$）

4. 然后将$e_c$喂给softmax单元，该单元用于预测不同目标词的概率，公式如下：
   $$
   \operatorname{Softmax}: p(t \mid c)=\frac{e^{\theta_{t}^{T} e_{c}}}{\sum_{j=1}^{10,000} e^{\theta_{j}^{T} e_{c}}}
   $$
   其中$\theta_t$是与输出$t$相关的参数（可训练），即某个词$t$与标签相符的概率。$\theta_{t}^{T} e_{c}$描述了两者的关联性。
   
5. 用$y$表示目标词（one-hot向量），$\hat y$表预测词（可能目标词的概率），均为10,000维的向量。损失函数如下：
   $$
   L(\hat{y}, y)=-\sum_{i=1}^{10,000} y_{i} \log \hat{y}_{i}
   $$

最后，通过优化这个损失函数就可以得到一个不错的词嵌入。

#### 15.6.2 分级的Softmax

其实上述的Skip-Gram模型有一个很明显的问题：计算速度。在计算softmax单元时，需要对10,000个词做求和运算，甚至更多的词。这会导致速度会变得相当慢，扩大词汇表（词典）更是几乎不可能的。

所以提出了分级的（hierarchical）的softmax，结构如下图：

![image-20201128165953663.png](https://upload-images.jianshu.io/upload_images/16793245-0c8ef7edb45b30b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这样每个结点都是一个二分类器，给定一个词，第一个分类器判断这个词是前5000个词中还是后5000个词中；第二个分类器判断是前2500中还是后2500个至前5000个词中……以此类推，最后到达的叶子结点就是目标类别，并给出对应的概率。

实际应用时，常用词会更接近树的根部，罕见词会在很深的地方；因此这个树并非一颗平衡树。

#### 15.6.3 Skip-Gram与CBOW

***CBOW***，即连续词袋模型（Continuous Bag-Of-Words Model），它获得中间词两边的的上下文，然后用周围的词去预测中间的词。在训练时相当于是用缺省的词（预测词）来影响周围的词。

这个模型也很有效，也有一些优点和缺点。结构如下图。

![image-20201128171707628.png](https://upload-images.jianshu.io/upload_images/16793245-d2e9dbf4edcec9a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对应的，Skip-Gram是用中间词去预测周围的词，训练时则相当于用一个词周围的词来影响中间词。结构如下图：

![image-20201128171754115.png](https://upload-images.jianshu.io/upload_images/16793245-9f7c4e0ff60a2cd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好；CBOW是从原始语句推测目标字词，而Skip-Gram正好相反，是从目标字词推测出原始语句。

cbow是 1个老师 VS K个学生，K个学生（周围词）都会从老师（中心词）那里学习知识，但是老师（中心词）是一视同仁的，教给大家的一样的知识。至于你学到了多少，还要看下一轮（假如还在窗口内），或者以后的某一轮，你还有机会加入老师的课堂当中（再次出现作为周围词），跟着大家一起学习，然后进步一点。所以对于罕见词来说，学习效果比较差。CBOW总的预测次数是O(V)，只与训练样本的词汇数有关。

相对的，skip-gram是 1个学生 VS K个老师，1个学生（中心词）会从K个老师（K个周围词）那里学习知识。skip-gram总的预测次数是O(V*K)，与训练样本的词汇数以及窗口的大小有关。因此数据集较大时K不宜太大。

#### 15.6.4 相关的重要论文

1. Mikolov T, Chen K, Corrado G, et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013.

#### 15.6.5 Doc2Vec

与Word2Vec对应，既然有词向量，那也可以考虑文本向量。Doc2Vec 的目的就是获得文本的一个固定长度的向量表达。

word2vec只是基于词的维度进行“语义分析”的，并不具有上下文的“语义分析”能力。因此在word2vec的基础上增加一个段落向量，该方法是doc2vec。该模型也有两个方法：Distributed Memory(DM) 和 Distributed Bag of Words(DBOW)。 DM试图给定上下文和段落向量的情况下预测单词的概率。在一个句子或者段落文档训练过程中，段落ID保存不变，共享同一个段落向量。DBOW则在只给定段落向量的情况下预测段落中一组随机单词的概率。引自[word2vec和doc2vec](https://www.jianshu.com/p/048bff9b0f65)。

### 15.7 负采样

#### 15.7.1 什么是负采样

skip-gram模型可以构造一个监督学习任务，把上下文映射到目标词上，以学习一个实用的词嵌入，但缺点是softmax计算起来很慢。负采样能够做到和 skip-gram相似的功能但是使用起来更加高效，理论上就是将10000分类的softmax转化为10000个二分类任务，来简化训练过程。

简单来说，构造一个新的监督学习问题，比如给定一对单词，比如orange和juice，判断上下文词（context）与目标词（target）是否为匹配的一对，如果是一对，则是正样本，如果不是一对，则是负样本.
在这个例子中orange和juice称作个正样本，用1作为标记，orange和king就是个负样本，标为0。要做的就是采样得到一个上下文词和一个目标词，中间列叫做词（word）。然后：

- 正样本的生成是采样得到一个上下文词和一个目标词。先抽取一个context，在一定词距内（比如说正负10个词距内）随机选择一个单词作为target，生成这个表的第一行，即orange– juice -1的过程
- 生成一个负样本，用相同的context，再在字典中随机选一个词，如king、book、the、of，标记为0。因为如果随机选一个词，它很可能跟orange没关联。其中同一 上下文词生成 *K个* 负样本

- 注意：正负样本的区别仅取决于单词对的来源，比如of正好出现在了orange正负10个词之内标为1，但是of又被作为随机从字典中选取的单词， of - orange单词对仍然会被标记为负样本。

K值的选取：

- 小数据集的话，K设置为从5到20，数据集越小K被设定的越大。
- 如果数据集很大，K就选的小一点。对于更大的数据集K设置为从2到5。

#### 15.7.2 模型学习原理

- 算法定义 ：输入Context 为 c, Word为 t , 定义输出Target为 y

| context | word  | target |
| ------- | ----- | ------ |
| c       | t     | y      |
| $x_1$   | $x_2$ | y      |
| orange  | juice | 1      |
| orange  | king  | 0      |
| orange  | book  | 0      |
| orange  | the   | 0      |
| orange  | of    | 0      |

- 损失函数定义为给定样本单词对的情况下，y = 1的概率:
  - 使用$e_c$表示context的词嵌入向量，其中$\theta_{t}$表示每个样本对应的参数.
  - $P(y=1 \mid c, t)=\sigma\left(\theta_{t}^{T} e_{c}\right)$
  - 对于每个正样本都有 *K* 个负样本来训练一个类似logisitic回归的模型。

#### 15.7.3 算法流程

1. 如果输入词是orange ，即词典中的第6257个词，将其使用one-hot向量表示 $ o_{6257}$，和 E (词嵌入向量矩阵)相乘得到 orange的嵌入向量$ e_{6357}$。

2. $e_{6357}$是一个10,000维(字典中总单词数量)的向量，可以看成是1W个可能的logistic回归分类问题，其中一个（编号4）将会是用来判断目标词是否是juice的分类器，其他的词比如下面的某个分类器（编号5）是用来预测king是否是目标词。

每次迭代不都是训练所有的样本， 只会训练一个正样本和随机选取的 K 个负样本，即模型总共包含了k+1个binary classification。对比之前10000个输出单元的softmax分类，negative sampling转化为k+1个二分类问题，每次迭代并不是训练10000个，而仅训练其中k+1个，计算量要小很多，大大提高了模型运算速度。

   这种方法就叫做负采样（Negative Sampling）:*选择一个正样本，随机采样k个负样本。

#### 15.7.4 如何选取负样本

- 仅仅通过单词在语料库中出现的频率进行采样：导致负样本中一些类似a、the、of等词的频率较高。

- 均匀随机地抽取负样本，分母是词汇表中总词数，没有很好的代表性。
- 推荐采样公式：

$$
P\left(w_{i}\right)=\frac{f\left(w_{i}\right)^{\frac{3}{4}}}{\sum_{j=1}^{10,000} f\left(w_{j}\right)^{\frac{3}{4}}}
$$

这种方法处于上面两种极端采样方法之间，即不用频率分布，也不用均匀分布，而采用的是对词频的$\frac{3}{4}$除以词频$\frac{3}{4}$整体的和进行采样的。其中，$f(w_j)$是语料库中观察到的某个词的词频。


#### 15.7.5 相关的重要论文

Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2013:3111-3119.[地址](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)

讲解 skip-gram 模型以及优化和扩展，包括层次 Softmax，负采样等内容。

### 15.8 GloVe词向量

该算法虽然不如Word2Vec热门，但也是简单且不错的模型，在NLP社区有一定的势头。

#### 15.8.1 什么是GloVe算法

全称Global vectors for word representation（基于全局统计的词表征）。在此之前，我们曾通过挑选语料库中位置相近的两个词，列举出词对，即上下文和目标词，GloVe算法做的就是使其关系开始明确化。假定 $X_{ij}$是单词 $i$在单词 $j$上下文中出现的次数，那么这里 $i$与 $j$就和 $t$与 $c$的功能一样，所以你可以认为 $X_{ij}$等同于 $X_{tc}$。

如果上下文定义为目标词左右各10个词左右，还会得到$X_{ij}=X_{ji}$的结论。相反，如果上下文总是目标词的前一个则没有这种对称性。这个其实很容易理解，当目标词的上下文包括左右两部分时，则意味着对于上下文 $c$来说，目标词 $t$也是 $c$的上下文。

**共现矩阵**：其实$X_{ij}$就是共现矩阵的其中一个元素。共现矩阵的横、纵轴均为词汇表的大小，行对应目标词的编号，列对应上下文的编号。计算方法就是遍历一个句子，比如“I love you”，则目标词为“I”时上下文有“love”和“you”，执行$X_{I,love}+1$和$X_{I,you}+1$。最后遍历语料库后即完成共现矩阵的计算。

**目标**：GloVe的目标是最小化函数：
$$
\operatorname{minimize} \sum_{i=1}^{10,000} \sum_{j=1}^{10,000} f\left(X_{i j}\right)\left(\theta_{i}^{T} e_{j}+b_{i}+b_{j}^{\prime}-\log X_{i j}\right)^{2}
$$
这个函数表示的是两者间的差距。$\theta_{i}^{T} e_{j}$同$\theta_{t}^{T} e_{c}$一样描述了两者同时出现的频率是多少。如果想要最小化这个函数，毫无疑问需要让$\theta_{i}^{T} e_{j}+b_{i}+b_{j}^{\prime}$逼近$\log X_{ij}$。

通过梯度下降算法即可学习嵌入矩阵$E$，并且学会预测两个单词同时出现的频率。

**细节**：如果$X_{ij}=0$，则$\log X_{ij}$为无穷大，因此要添加一个额外的加权项$f(X_{ij})$，约定$f(0)(\ldots-\log 0)=0$。即当$X_{ij}=0$时不计入求和项；同时对于常见词如a、the、of等词，给与一个较小的权重；对于罕见词如durion则给予一个较大的权重。

还有一个细节就是现在的$\theta$和$e$现在是完全对称的，即$\theta_i$和$e_j$是对称的。只看公式的话两者功能相似。

**对比**：首先，两种算法学习词嵌入都用到了一个信息：词汇共现（co-occurrence），即利用两个词一起出现这个信息来学习词嵌入、隐含的语义等。不同之处在于，word2vec学习了这个共现，本质上做的是预测任务（Predictive），要么根据中心词预测上下文，要么根据上下文预测中心词；而GloVe做的是对已经得到的共现信息（共现矩阵）进行降维、编码（Count-based）。

**优缺点**：相对来说word2vec没有很好地利用共现信息，而GloVe则因为共现矩阵需要花费相当大的内存；但是不采用负采样的word2vec非常快，GloVe则更容易优化准确率也比没有用负采样的word2vec高。

#### 15.8.2 为什么不能保证嵌入向量的独立组成部分是能够理解的

首先，我们可以将词嵌入理解为“特征提取”，比如提取出Gender、Royal等属性来进行描述，这些特征对于我们来说是可以理解的。但是，我们刚刚提到的算法都是自己学自己的，并不一定会学出Gender等属性。

更具体一点，可以当成是进行了降维，用更少的轴来描述原来的单词；如果这些轴分别表示Gender、Royal等属性则是可理解的，并且相互正交；但是我们提到的算法都没有去保证“这两个特征之间是相互正交的”，也就是说他学到的特征很可能是多个正交特征的混合，比如第一个轴是Gender和Royal的混合，第二个轴是Royal和Age的混合等等。

所以，才说不能保证嵌入向量的独立组成部分是能够理解的。

#### 15.8.3 相关重要论文

1. Pennington J, Socher R, Manning C. Glove: Global Vectors for Word Representation[C]// Conference on Empirical Methods in Natural Language Processing. 2014:1532-1543.

### 15.9 情感分类

情感分类：Sentiment classification

词嵌入：word embeddings，将一个词语(word)转换为一个向量 (vector)表示, 所以词嵌入有时又被叫作“**word2vec**"。

情感分类是一个很难但很有趣的方向，相对来说也很容易出项目，应用到各种场合。

#### 15.9.1 什么是情感分类

情感分类：对带有感情色彩的主观性文本进行分析，分析说话人的态度是倾向正面还是反面，是NLP中最重要的模块之一。

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5kExYxed5BMneYIOgOE8PQn.3.I5.VtCFfQxdsOlcTApc4EQurbb0EwZWB4zOtCtzZUAR*qcNMIVOHIOghTz184!/b&bo=IALtAAAAAAADB.0!&rf=viewer_4)

举个例子，如上图所示，输入 $x$ 是一段文本，输出 $y$ 是要预测的相应情感，比如是对一个餐馆评价的星级。现在很多人会把用餐的情况与感受分享在一些社交平台上，如果基于像上面这样标记的数据集可以训练一个从 $x$ 到 $y$ 的映射也就是一个情感分类器，就可以用来搜集大家对这个餐馆的评价，从文本中分析出顾客对餐馆评论的情感是正面的还是负面的，可以让餐馆发现自身的问题所在也能让其他顾客了解到这个餐馆是否值得光顾。

情感分类最大的挑战是**标记的训练集不够多**，对于情感分类任务来说，训练集大小从10000到100000个单词都很常见，甚至有时会小于10000个单词，但采用**词嵌入**效果可以好很多，即使只有中等大小的标记的训练集，也能构建一个不错的情感分类器。

#### 15.9.2 一个简单的情感分类模型

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5lFe8NDbfNA77fCVkIHmGTgJVimWd9dsna3xy7GjfNRa1dIL8b4xY2cBLW8KRun.ydHJQ2yQiQd.nCaCyo6Z0Us!/b&bo=CQX0AQAAAAADB9s!&rf=viewer_4)

如上图所示，假设有一个句子“The dessert is excellent(甜点很棒)”，要构建一个分类器把它映射成输出四个星。首先，我们从词典里找到这些词相应的一位编码表示，如“the”的**一位编码**是 $o_{8928}$，乘以**嵌入矩阵 $E$**（$E$ 可以从一个很大的文本集里学习到，比如从一亿个词或者一百亿个词里学习嵌入表达，从中获取很多知识甚至从有些不常用的词中获取然后应用到要解决的问题中，即使你的标记数据集里没有这些词），得到单词“the”的**嵌入向量 $e_{8928}$**，对其他三个单词做同样的步骤。将这些嵌入向量求和或者求平均（把所有单词的意思加起来或取平均），假设这里用的是一个平均值计算单元Avg，这个单元会输出一个相同维度的特征向量，把这个特征向量送进softmax分类器，输出5个可能结果(从一星到五星)的概率值，进而输出预测结果 $\hat y$。

这个算法有一个问题就是**没考虑词序**，像“Completely lacking in good taste, good service, and good ambience（完全没有好的味道、好的服务、好的氛围）”虽然是负面评价，但”good“这个词出现了很多次，如果还用上面的算法，忽略词序而仅仅把所有单词的词嵌入加起来或者取平均，则最后的特征向量会有很多good的表示，分类器就可能误认为这是一个好评。

#### 15.9.3 用RNN实现情感分类

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5nftd.qb6*kgFabQY2o160P16CYAmNZopPyN3Ks0z4Zsm2emoJ.DqsgpTyj*fgXkYLxVOiGLKvb9X2heZeRvCFs!/b&bo=PgViAgAAAAADB3k!&rf=viewer_4)

首先找到这条评论中每一个单词的一位编码表示，乘以词嵌入矩阵 $E$ 得到词嵌入表达 $e$，然后把它们送进我们之前学过的多对一的RNN网络结构里，RNN的工作就是在最后一步计算一个特征向量来预测 $\hat y$。这种算法考虑了词的顺序，效果更好，能够意识到“Completely lacking in good taste, good service, and good ambience（完全没有好的味道、好的服务、好的氛围）”是个负面评价。由于词嵌入 $E$ 是在一个更大的数据集里训练的，可以更好地泛化一些没有见过的新单词，比如其他人可能会说“Completely absent of good taste...”，即使"absent"这个词不在标记的训练集里，但它仍然可以判断对并且泛化得很好。

### 15.10 词嵌入除偏

算法安全性；偏见原因：**数据集**不完整（cv：黑人数据不够多）、打**标签**有偏见、**算法**本身的偏见（推荐算法的导向性，推之前没有的，试探性，引导往其他方向走）；解决方法：采样更丰富和比例更合理（数据增强）、***设计算法创新处理有偏数据集得到无偏结果***；老人无健康码上不了车，考虑少数人、边缘化等；

词嵌入除偏：Debiasing word embeddings

#### 15.10.1 为什么需要除偏

这里的“偏(bias)"不是指偏见，而是指性别、种族、性取向方面的偏见。

“Man is to computer programmer as woman is to homemaker? debiasing word embeddings.”这篇论文提出，如果说Man对应Computer programmer则Woman会对应什么呢？一个已经完成学习的词嵌入可能会输出Homemaker，这个结果就反映了一个十分不良的性别歧视，如果输出的是Computer programmer则会更合理。再比如说Father对应Doctor那么Mother应该对应什么？有些完成学习的词嵌入会输出nurse，这也是不对的。

也就是说，根据训练模型所使用的文本，**词嵌入可能会反映出性别、种族、年龄、性取向等其他方面的偏见**，这些偏见都和社会经济状态相关。因为机器学习算法正用来制定十分重要的决策，影响着世间万物，所以尽量修改学习算法来尽可能**减少或是理想化消除这些非预期形式的偏见**是十分重要的，因此需要对词嵌入进行除偏。

**偏见产生的原因**：

* **数据集**不完整。比如在做图像处理的时候由于黑人的数据不够多而导致效果不好。

* **标签**有偏见。由于标签都是人打上去的，因此如果打标签的人带有偏见，则最后标签也会带有偏见并产生实际影响。

* **算法**本身的偏见。常见于推荐算法，有的推荐算法具有导向性，会通过试探性地推荐之前没有的物品，引导用户养成新的兴趣，有时候甚至会带有“恶意”将用户引导到不好的方向。

#### 15.10.2 什么是词嵌入除偏

假设我们已经完成一个词嵌入的学习，babysister、doctor、grandmother、grandfather、girl、boy、she、he这些词的嵌入位置如下图所示，

<img src="http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5mvl4bVsP1YvxXAbpq2kqR6VID*eIVoZuDIRfcIvwzQi*DE5SG05*oVVDjL9uxrrWDkfWYd3jhnGKp2AhfyxvTY!/b&bo=RQJtAgAAAAADBwo!&rf=viewer_4" style="zoom:50%;" />

词嵌入除偏有以下三步：

1. **辨别出我们想要减少或想要消除的特定偏见的趋势**。假设这里要讨论的是性别歧视，则可以是将向量 $e_{he}$ 减去向量 $e_{she}$ ，将 $e_{male}$ 减去向量 $e_{female}$ ，因为它们的性别不同，然后将这些差取平均，就能够发现像上图这种情况下横向趋势看起来就是性别趋势或说是偏见趋势(bias)，纵向趋与我们想要尝试处理的特定偏见并不相关，因此是无偏见趋势(non-bias)，如下图所示。在这种情况下，偏见趋势可以将它看做1维子空间，所以这个无偏见趋势就会是299维的子空间。偏见趋势可以比1维更高，同时相比于取平均值，实际上它会用一个更加复杂的算法叫做SVD（Singular value decomposition，奇值分解，与主成分分析很类似）。

   <img src="http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5i.*nKVtTIgUhh2qintZr0J0NbuaGef752FxQX0h9mpnt2Z.txNuKd5gYvw1avy5iZb.0QfxXEDiDw0D454UtOQ!/b&bo=dwO5AgAAAAADB.0!&rf=viewer_4" style="zoom:50%;" />

2. **中和（neutralize）**：对于那些**定义不确切的词**可以将其处理一下避免偏见。有些词本质上就和性别有关，像grandmother、grandfather、girl、boy、she、he，他们的定义中本就含有性别的内容，不过也有一些词，像doctor、babysister，我们想使之在性别方面是中立的，同时，在更通常的情况下，可能会更希望像doctor或babysister这些词成为种族中立的或是性取向中立的等等，不过这里仍然只用性别来举例说明。对于这些定义不明确的词即基本意思不像grandmother和grandfather这种定义里有着十分合理的性别含义的，可以将它们在横轴上进行处理，来减少或是消除它们的性别歧视趋势的成分，也就是说**减少它们在水平方向上的距离**，如下图所示。

   <img src="http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5kyQxXjeXItUlno96UewOHn5byP1aiY1GApKhwRwMgzsL0Xu2cTUCNvlad7sVo4vvCFM8F1n7kjGn0YhxE2OJGw!/b&bo=dgO0AgAAAAADB.E!&rf=viewer_4" style="zoom:50%;" />

3. **均衡（Equalize pairs）**：可能会有grandmother和grandfather、gir和boy这样的词对，对于这些词嵌入，**只希望性别是其区别**。在这个例子中，babysister和grandmother之间的距离或者说是相似度小于babysister和grandfather之间，如下图所示（$d1 < d2$），可能会加重不良状态或者可能是非预期的偏见，也就是说grandmother相比于grandfather最终更有可能输出babysister，所以在最后的均衡步中，我们想要确保的是像grandmother和grandfather这样的词都能够有**一致的相似度或者说是相等的距离**，这其中会有一些线性代数的步骤，主要做的就是将grandmother和grandfather**移至与中间轴线等距的一对点上**，如下图所示，现在性别歧视的影响也就是这两个词与babysister的距离就完全相同了。还有许多对象比如grandmother和grandfather、gir和boy、sorority和fraternity、girlhood和boyhood、sister和brother、niece和nephew、daughter和son这样的词对都可以通过均衡步来解决。

   <img src="http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5guI4JJNy.0.D16p0w.kneVAuiAiKx8xCqzg.tRMlbTPSOy8CGmg4zTMpQx1hYQq16LaapQ5izrannQzA5mU6Do!/b&bo=dwOzAgAAAAADB.c!&rf=viewer_4" style="zoom:60%;" />

   最后一个细节是怎样才能够决定哪个词是中立的呢？对于这个例子来说，doctor看起来像是一个应该是中立的单词来使之性别不确定或是种族不确定，相反地，grandmother和grandfather就不应是性别不确定的词，也会有些词像是beard，一个统计学上的事实是男性相比于女性更可能拥有胡子，因此相比于female，beard应该更靠近male一些。论文“Man is to computer programmer as woman is to homemaker? debiasing word embeddings. ”的作者做的就是训练一个分类器来尝试解决哪些词是有明确定义的，哪些词是性别确定的哪些词不是，结果表明英语里大部分词在性别方面上是没有明确定义的，即性别并不是其定义的一部分，只有一部分词像grandmother和grandfather、gir和boy、sorority和fraternity等不是性别中立的。一个线性分类器能够告诉你哪些词能够通过中和来预测这个偏见趋势或将其与这个本质是299维的子空间进行处理。最后，你需要平衡的词对的数量实际上是很小的，至少对于性别歧视这个例子来说，用手都能够数出来你需要平衡的大部分词对。完整的算法会比这里展示的更复杂一些，可以去看一下这篇论文了解详细内容。

#### 15.10.3 相关的重要论文

1. Yoshua Bengio , et al. “A neural probabilistic language model.”*Journal of machine learning research*3.Feb (2003): 1137-1155.

   证明使用神经网络训练的语言模型可以生成比One-hot更好的词向量。

2. Mikolov, T., Chen, K., Corrado, G., & Dean, J. Efficient estimation of word representations in vector space. *arXiv preprint arXiv:1301.3781*. (2013).

   提出词嵌入领域最经典的两个模型：连续词袋模型(Continuous Bag Of Words, CBOW) 和跨词序列模型(Skip-gram)。

3. Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In <i>Proceedings of the 30th International Conference on Neural Information Processing Systems</i> (<i>NIPS'16</i>). 

   发现并消除了词嵌入中的社会性别偏见问题。

4. Zhao Jieyu, Wang Tianlu, Yatskar Mark, Ordonez Vicente and Chang Kai-Wei "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints."(2017)

   发现并消除了视觉相关任务中使用结构预测模型偏见放大的问题。

5. Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: Frequency-agnostic word representation. In NeurIPS, 2018.

   针对高频词和低频词在传统词嵌入模型中的表现，把它们身上“高频/低频词''这个标签拿掉后再来看训练效果。

6. Tianlu Wang, Xi Victoria Lin, Nazeen Fatema Rajani, Bryan McCann, Vicente Ordonez and Caiming Xiong. Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation. ACL 2020.

   提出运用“双硬去偏”法来消除词频对性别方向的负面影响。

   > 参考：https://www.sohu.com/a/406401979_651893?_f=index_pagefocus_7





