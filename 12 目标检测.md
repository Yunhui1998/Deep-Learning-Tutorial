## 12 目标检测

### 12.1 目标定位

#### 12.1.1 定义

除了要识别出目标类（例如是否有汽车）以外，算法还需要能够给出目标的位置（例如用红框框出目标的位置），这是一种分类定位问题。分类定位问题可以是只有一个目标对象在图片中，也可以是多个目标对象在图片中的。先从单个目标对象的情况开始。

#### 12.1.2 实现

**1. 边界框参数定义**

图片分类我们已经比较熟悉了，知道最后通过 *softmax* 激活函数激活来表示该图片属于每一类的概率。相较于单纯的分类问题，目标定位还需要输出4个值：$b_x,b_y,b_h,b_w$。这四个值用于表示边框的位置和大小，具体为
$$
\begin{array}{l}
b_x:边框的中心点的x轴坐标 \\ 
b_y:边框的中心点的y轴坐标 \\
b_h:边框的高度 \\
b_w:边框的宽度 \\
\end{array}
$$
以上4个值的取值范围均为[0, 1]，表示的是边界框与图片对应的**比例**。

关于坐标轴的选取，一般选图片的左上角为原点（0, 0），右下角为（1, 1）。且$x$轴为纵轴正方向向下，$y$轴为横轴正方向向右。

**2. 标签的定义**

在确定需要增加的额外输出之后，也要重新定义目标标签 $y$：
$$
y=\left[\begin{array}{l}
p_{c} \\
b_{x} \\
b_{y} \\
b_{h} \\
b_{w} \\
c_{1} \\
c_{2} \\
c_{3}
\end{array}\right]
$$
其中额外增加了$p_c$，用于表示是否有对象属于类$c_1、c_2$或$c_3$，若为1则表示有目标类在图片中，为0表示只有背景。在只有背景（无目标类）时，其他输出均无效。

举一个具体的例子，如下图：

![image-20201008224515088](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5qtLlrvM3D9G2.DKG*ddcaTqBQvbLK97JrVZqDAOEFOzLEuPSvpfD0JRU6nhVgrR628erJ6uvVy3kn3oWXw3shA!/b&bo=fAJcAQAAAAADBwE!&rf=viewer_4)

x 是输入，y 是标签，假设$c_2$表示为车，则最终标签应该如上图所示。当输入中不含任何目标类时，$p_c=0$，其他参数全用“？”表示，即不参与 loss 的计算。

**3. 边界框参数取值范围**

关于边框的4个参数，吴恩达老师给出的取值范围都是[0, 1]的。虽然确实可以这么做，但是背后的原因却没有解释。个人猜测可能是以下原因：

* 固定取值在[0, 1]之间更容易用激活函数表示，比如sigmoid可以将单个值映射到[0, 1]；
* 让标签的所有参数都在一个取值范围，这样在计算loss的时候就不会变相加权了；
* 取值范围在[0, 1]内更容易训练、收敛。

**4. 损失函数**
$$
L(\hat y,y)=
\begin{cases}
(\hat{y_1}-y_1)^2+(\hat{y_2}-y_2)^2+...+(\hat{y_n}-y_n)^2 & {y_1=1}\\
(\hat{y_1}-y_1)^2 & {y_1=0}
\end{cases}
$$

* 对于$p_c$可以采用 Logistic 回归的方法，甚至用均方误差$(p_c-\hat{p_c})^2$也可以；
* 对于$b_x,b_y,b_w,b_h$，loss 可以用平方差或类似方法；
* 对于$c_1,c_2,c_3$，是多分类问题，和之前一样用 softmax 即可。

### 12.2 特征点检测

#### 12.2.1 定义

以下图为例，检测双眼的眼角位置

![特征点检测](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5rEX3dQ9FgSlnvKQ5qZlshdf4zwS7QYcecgV*w8TjuMeuce8XaStWUH2FWjZOLK0zdWsy2KOJ78nVhkGcz7qVco!/b&bo=EQE2AQAAAAADBwU!&rf=viewer_4)

这个就是特征点检测，主要任务是预测对应的特征点的位置，一般会同时预测多个点。

#### 12.2.2 实现

对于单个点，其坐标为$(l_{ix},l_{iy}),i \in [1, m]$，$m$是特征点的个数，$l_{ix},l_{iy} \in [0,1]$。坐标轴的选取和目标定位时一致，以及需要保证每个样本的特征点的顺序是一致的，比如$l_1$都是左眼的左眼角，$l_2$都是左眼的右眼角等。

关于最后一层的激活函数，可以对每个坐标的值用 sigmoid 激活。

### 12.3 目标检测

#### 12.3.1 滑动窗口目标检测算法

滑动窗口检测是分两步执行的：第一步是从原图中裁剪一部分并根据需求调整大小，第二步是检测裁剪部分有无目标对象。

对于第二步，是我们比较熟悉的分类算法，只是训练集需要人为调整到为几乎整张图都是目标对象，如下图：

![滑动窗口检测汽车](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5rEX3dQ9FgSlnvKQ5qZlshdKObnsuS9mPXLUC9irkKHnQp8VaojMjR3*WA89tBGSc2FVGggFHSrPwAm*c9XkaiE!/b&bo=vAJzAQAAAAADB.4!&rf=viewer_4)

$x$ 为一张无车的图片或者被车占满的图片，标签 $y$ 为1时有车，为0时无车。

对于第一步，滑动窗口用的方法是先选定一个裁剪大小，然后从图片的左上角开始，每隔一定像素（距离）裁剪一次，一直滑动到图片的右下角，然后下一个裁剪大小用相同的方法裁剪。如图，上图为3种裁剪大小，下图为一个滑动流程：

![大小](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcSrmgQVcdhRoBO3DiLDtZt2oTwjc*BSmAteWpS4Y0OGddHdf2WIp3pq9EhTxWelYn.9Vs4Wk1.VKXParzNYCmhY!/b&bo=IALyAAAAAAADF.I!&rf=viewer_4)

![滑动](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcSrmgQVcdhRoBO3DiLDtZt0v.y9..assySWEwoh0BMTtwVAb6Wo8Svos60inYKEm0jJy1Cj1cjSPyfG6Q7NgiJk!/b&bo=vAKCAAAAAAADFw4!&rf=viewer_4)

#### 12.3.2 滑动窗口缺点——计算成本

看到这里不难发现，滑动窗口检测由于需要多个窗口且每个窗口滑动多次而导致运算量会相当巨大，运行时间过慢。而且如果滑动的步幅过大或者裁剪的大小不合适也会导致很难检测到目标。

### 12.4 卷积的滑动窗口实现

#### 12.4.1 把神经网络的全连接层转化成卷积层

![](https://upload-images.jianshu.io/upload_images/24408091-8d885553bba2259b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 假设目标检测算法输入一个14×14×3的图像，过滤器大小为5×5，数量是16，14×14×3的图像在过滤器处理之后映射为10×10×16。

- 然后通过参数为2×2的最大池化操作，图像减小到5×5×16。

- 添加一个连接400个单元的全连接层，接着再添加一个全连接层。

- 最后通过softmax单元输出，用4个数字来表示 ，它们分别对应softmax单元所输出的4个分类出现的概率。这4个分类可以是行人、汽车、摩托车和背景或其它对象。

**全连接层转化为卷积层**

![](https://upload-images.jianshu.io/upload_images/24408091-4a8d583013dc4391.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 前几层和之前的一样。

- 而对于全连接层，用5×5的过滤器来实现，数量是400个。输入图像大小为5×5×16，用5×5的过滤器对它进行卷积操作，过滤器实际上是5×5×16，因为在卷积过程中，过滤器会遍历这16个通道，所以这两处的通道数量必须保持一致，输出结果为1×1。假设应用400个这样的5×5×16过滤器，输出维度就是1×1×400。它不再是一个含有400个节点的集合，而是一个1×1×400的输出层。从数学角度看，它和全连接层是一样的，因为这400个节点中每个节点都有一个5×5×16维度的过滤器，所以每个值都是上一层这些5×5×16激活值经过某个任意线性函数的输出结果。
- 再添加另外一个卷积层，这里用1×1卷积，假设有400个1×1的过滤器，在这400个过滤器的作用下，下一层的维度是1×1×400，它其实就是上个网络中的这一全连接层。最后经由1×1过滤器的处理，得到一个softmax激活值，通过卷积网络，我们最终得到这个1×1×4的输出层，而不是4个数字。

以上就是用卷积层代替全连接层的过程。

#### 12.4.2 在卷积上应用滑动窗口对象检测算法

![](https://upload-images.jianshu.io/upload_images/24408091-abdabe4d2823fcce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假设向滑动窗口卷积网络输入14×14×3的图片。和前面一样，神经网络最后的输出层，即softmax单元的输出是1×1×4，这里画得比较简单，严格来说，14×14×3应该是一个长方体，第二个10×10×16也是一个长方体，为了方便，这里只画了正面，所以这里显示的都是平面图，而不是3D图像。

![](https://upload-images.jianshu.io/upload_images/24408091-40d91c7dcf79117f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假设输入给卷积网络的图片大小是14×14×3，测试集图片是16×16×3，现在给这个输入图片加上黄色条块，在最初的滑动窗口算法中，你会把这片蓝色区域输入卷积网络（红色笔标记）生成0或1分类。接着滑动窗口，步幅为2个像素，向右滑动2个像素，将这个绿框区域输入给卷积网络，运行整个卷积网络，得到另外一个标签0或1。继续将这个橘色区域输入给卷积网络，卷积后得到另一个标签，最后对右下方的紫色区域进行最后一次卷积操作。我们在这个16×16×3的小图像上滑动窗口，卷积网络运行了4次，于是输出了了4个标签。

![](https://upload-images.jianshu.io/upload_images/24408091-a8bf95d6a9b7de72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这4次卷积操作中很多计算都是重复的。所以执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算。

- 卷积网络运行同样的参数，使得相同的5×5×16过滤器进行卷积操作，得到12×12×16的输出层。

- 然后执行同样的最大池化，输出结果6×6×16。照旧应用400个5×5的过滤器，得到一个2×2×400的输出层，现在输出层为2×2×400，而不是1×1×400。

- 应用1×1过滤器得到另一个2×2×400的输出层。

- 再做一次全连接的操作，最终得到2×2×4的输出层，而不是1×1×4。

- 最终，在输出层这4个子方块中，蓝色的是图像左上部分14×14的输出（红色箭头标识），右上角方块是图像右上部分（绿色箭头标识）的对应输出，左下角方块是输入层左下角（橘色箭头标识），也就是这个14×14区域经过卷积网络处理后的结果，同样，右下角这个方块是卷积网络处理输入层右下角14×14区域(紫色箭头标识)的结果。

该卷积操作的原理是我们不需要把输入图像分割成四个子集，分别执行前向传播，而是把它们作为一张图片输入给卷积网络进行计算，其中的公共区域可以共享很多计算，就像这里我们看到的这个4个14×14的方块一样。

![](https://upload-images.jianshu.io/upload_images/24408091-97ee24f1e9c36366.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

再看一个更大的图片样本，假如对一个28×28×3的图片应用滑动窗口操作，如果以同样的方式运行前向传播，最后得到8×8×4的结果。跟上一个范例一样，以14×14区域滑动窗口，首先在这个区域应用滑动窗口，其结果对应输出层的左上角部分。接着以大小为2的步幅不断地向右移动窗口，直到第8个单元格，得到输出层的第一行。然后向图片下方移动，最终输出这个8×8×4的结果。因为最大池化参数为2，相当于以大小为2的步幅在原始图片上应用神经网络。

**滑动窗口的实现过程总结**

![](https://upload-images.jianshu.io/upload_images/24408091-ad455dbc080c8441.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在图片上剪切出一块区域，假设它的大小是14×14，把它输入到卷积网络。继续输入下一块区域，大小同样是14×14，重复操作，直到某个区域识别到汽车。

但是我们不用依靠连续的卷积操作来识别图片中的汽车，比如，我们可以对大小为28×28的整张图片进行卷积操作，一次得到所有预测值，如果足够幸运，神经网络便可以识别出汽车的位置。

### 12.5 Bounding Box预测

滑动窗口法的卷积实现的算法虽然效率更高，但仍然存在问题，不能输出最精准的边界框。

<img src="https://upload-images.jianshu.io/upload_images/24408091-df8cfe95c88f0d0c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" style="zoom: 80%;" />

在滑动窗口法中，你取这些离散的位置集合，然后在它们上运行分类器，在这种情况下，这些边界框没有一个能完美匹配汽车位置，也许这个框（编号1）是最匹配的了。还有看起来这个真实值，最完美的边界框甚至不是方形，稍微有点长方形（红色方框所示），长宽比有点向水平方向延伸。

其中一个能得到更精准边界框的算法是**YOLO**算法，YOLO(You only look once)意思是你只看一次。

#### 12.5.1 YOLO算法的具体实现

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcYIcBSLWNRUT8vYiXXBfe64wc0jFreGa3MxVmAx38ntZbyB*NatQKv40O7fTjU5uT8yEY2z96G4ctzQl8Fzu1A4!/b&bo=SgVbAgAAAAADNwQ!&rf=viewer_4)

比如你的输入图像是100×100的，然后在图像上放一个网格。为了介绍起来简单一些，这里用3×3网格，实际实现时会用更精细的网格，可能是19×19。

基本思路是使用图像分类和定位算法，将算法应用到9个格子上。

具体一点，需要定义训练标签，对于9个格子中的每一个指定一个标签$y$，$y$是8维的(上图蓝字)：

- $p_{c}$等于0或1取决于这个绿色格子中是否有图像。

- $b_{x}$、$b_{y}$、$b_{h}$和$b_{w}$作用就是，如果那个格子里有对象，那么就给出边界框坐标。

- $c_{1}$、$c_{2}$和$c_{3}$就是你想要识别的三个类别，背景类别不算，所以你尝试在背景类别中识别行人、汽车和摩托车，那么$c_{1}$、$c_{2}$和$c_{3}$可以是行人、汽车和摩托车类别。

这张图有两个对象，**YOLO算法**做的就是，**取两个对象的中点，然后将这个对象分配给包含其中点的格子**。

所以左边的汽车就分配到这个格子上（绿框），然后这辆中点在这里（绿点），标签为（上图绿字）；右边的汽车分配给这个格子（黄色），中点在这里（黄点），标签为（上图黄字）。即使中心格子同时有两辆车的一部分，我们就假装中心格子没有任何我们感兴趣的对象。

没有检测对象的格子（除中间靠左、中间靠右外的其他格子）标签向量$y$都如上图紫字所示。

故对于这里9个格子中任何一个，你都会得到一个8维输出向量，因为这里是3×3的网格，所以有9个格子，总的输出尺寸是3×3×8，所以目标输出是3×3×8。

<img src="https://upload-images.jianshu.io/upload_images/24408091-3f095e6e13226930.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" style="zoom:80%;" />

所以这个算法的优点在于神经网络可以输出精确的边界框，所以测试的时候，你做的是喂入输入图像$x$，然后跑正向传播，直到你得到这个输出$y$。然后对于这里3×3位置对应的9个输出，就可以读出1或0。如果那里有个对象，就可以读出那个对象是什么，还有格子中这个对象的边界框是什么。

只要每个格子中对象数目没有超过1个，这个算法应该是没问题的。一个格子中存在多个对象的问题，之后再讨论。这里用的是比较小的3×3网格，实践中你可能会使用更精细的19×19网格，所以输出就是19×19×8。这样的网格精细得多，那么多个对象分配到同一个格子的概率就小得多。

把对象分配到一个格子的过程是，你观察对象的中点，然后将这个对象分配到其中点所在的格子，所以即使对象可以横跨多个格子，也只会被分配到9个格子其中之一，就是3×3网络的其中一个格子，或者19×19网络的其中一个格子。在19×19网格中，两个对象的中点处于同一个格子的概率就会更低。

YOLO算法有一个好处就是，它是一个卷积实现，共享了很多卷积计算，所以实际上它的运行速度非常快，可以达到实时识别。

**编码边界框**

![](https://upload-images.jianshu.io/upload_images/24408091-9b13922696926f94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于这个方框（中右侧），我们约定左上这个点是（0，0），然后右下这个点是（1，1）；

要指定橙色中点的位置，$b_{x}$大概是0.4，因为它的位置大概是水平长度的0.4，$b_{y}$大概是0.3；

边界框的高度用格子总体宽度的比例表示，所以这个红框的宽度可能是蓝线（图右下角蓝线）的90%，所以$b_h$是0.9，它的高度也许是格子总体高度的一半，这样的话$b_{w}$就是0.5。

换句话说，$b_{x}$、$b_{y}$、$b_{h}$和$b_{w}$单位是相对于格子尺寸的比例，所以$b_{x}$和$b_{y}$必须在0和1之间，因为从定义上看，橙色点位于对象分配到格子的范围内，如果它不在0和1之间，如果它在方块外，那么这个对象就应该分配到另一个格子上。但是$b_{h}$和$b_{w}$可能会大于1，特别是如果有一辆汽车的边界框是这样的（左下红框），那么边界框的宽度和高度有可能大于1。

指定边界框的方式有很多，但这种约定是比较合理的，还有其他更复杂的参数化方式，涉及到sigmoid函数，确保这个值（$b_{x}$和$b_{y}$）介于0和1之间，然后使用指数参数化来确保$b_{h}$和$b_{w}$都是非负数。还有其他更高级的参数化方式，可能效果要更好一点，但这里讲的办法应该是管用的。

#### 12.5.2 其他指定边界框的算法

**1. SSD**:

SSD（Single-Shot MultiBox Detector），使用 VGG19 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样）的单次检测器。我们在该网络之后添加自定义卷积层（蓝色），并使用卷积核（绿色）执行预测。

![](https:////upload-images.jianshu.io/upload_images/15050749-aa15690a663266b4.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

然而，卷积层降低了空间维度和分辨率。因此上述模型仅可以检测较大的目标。为了解决该问题，我们从多个特征图上执行独立的目标检测。

![](https:////upload-images.jianshu.io/upload_images/15050749-a178bfd6934f49c1.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

以下是特征图图示。

![img](https:////upload-images.jianshu.io/upload_images/15050749-2552e0c902046afa.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

SSD 使用卷积网络中较深的层来检测目标。如果我们按接近真实的比例重绘上图，我们会发现图像的空间分辨率已经被显著降低，且可能已无法定位在低分辨率中难以检测的小目标。如果出现了这样的问题，我们需要增加输入图像的分辨率。

![img](https:////upload-images.jianshu.io/upload_images/15050749-07e817f1ff6e3c31.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)




参考博客：https://www.jianshu.com/p/468e08f739bd

**2. FPN**:

使用不同尺寸特征图进行预测的网络称为特征金字塔网络（FPN），是一种旨在提高准确率和速度的特征提取器。数据流如下：

![img](https:////upload-images.jianshu.io/upload_images/15050749-64bf69f7278315d4.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/500/format/webp)

FPN 由自下而上和自上而下路径组成。其中自下而上的路径是用于特征提取的常用卷积网络。空间分辨率自下而上地下降。当检测到更高层的结构，每层的语义值增加。

![img](https:////upload-images.jianshu.io/upload_images/15050749-03de5ed9a022f2f0.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/470/format/webp)

SSD 通过多个特征图完成检测。但是，最底层不会被选择执行目标检测。它们的分辨率高但是语义值不够，导致速度显著下降而不能被使用。SSD 只使用较上层执行目标检测，因此对于小的物体的检测性能较差。

![img](https:////upload-images.jianshu.io/upload_images/15050749-5b22f5a8a1e50a95.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/400/format/webp)

FPN 提供了一条自上而下的路径，从语义丰富的层利用上采样构建高分辨率的层。

![img](https:////upload-images.jianshu.io/upload_images/15050749-05a2fffcb407e080.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/500/format/webp)

虽然该重建层的语义较强，但在经过所有的上采样和下采样之后，目标的位置不精确。在重建层和相应的特征图之间添加横向连接可以使位置侦测更加准确。

![img](https:////upload-images.jianshu.io/upload_images/15050749-ccf23bd8f5bc749f.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/500/format/webp)

**3. RetianNet**：

RetianNet是基于 ResNet、FPN以及利用 Focal loss 构建的 。

![img](https:////upload-images.jianshu.io/upload_images/15050749-a8cea15ead3d5a99.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1080/format/webp)

RetinaNet

**4. Focal Loss**

类别不平衡会损害性能。SSD 在训练期间重新采样目标类和背景类的比率，这样它就不会被图像背景淹没。Focal loss（FL）采用另一种方法来减少训练良好的类的损失。因此，只要该模型能够很好地检测背景，就可以减少其损失并重新增强对目标类的训练。我们从交叉熵损失 （Cross Entroy Loss）开始，并添加一个权重来降低高可信度类的交叉熵。

![img](https:////upload-images.jianshu.io/upload_images/15050749-324f6d6caf3144fa.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/800/format/webp)

例如，令 γ = 0.5, 经良好分类的样本的 Focal Loss 趋近于 0。

![img](https:////upload-images.jianshu.io/upload_images/15050749-fbb52eafebcded04.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/400/format/webp)

### 12.6 交并比

#### 12.6.1 交并比函数的作用

交并比(IoU, Intersection over Union)函数衡量了两个边界框重叠的相对大小，用来评价对象检测算法是否运行良好，即检测对象定位是否准确。

#### 12.6.2 交并比函数的实现

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5mBLgC*DmP5oANsIbkPlbT6.hHB1R3zeAaFzVy0hgHa8bDwdou1sysufutEFjw2llZt.tjfhWB6CqmC1qpMdQkw!/b&bo=SgFGAQAAAAADBy4!&rf=viewer_4)

假设识别的对象的实际边界框如上图中红框所示，用对象检测算法得到的边界框如紫框所示，这个结果是好是坏呢？

交并比函数做的是计算**两个边界框交集和并集之比**，交集部分如橙色阴影所示，并集部分如绿色阴影所示，如果预测边框和实际边框完美重叠，交集等于并集，$IoU = 1$，一般约定，在计算机检测任务中，若 $IoU \geq 0.5$，则说明预测的边界框是正确的，如果希望更严格，可以把IoU的阈值定得更高，IoU越高，边界框越精确。

### 12.7 非极大值抑制

#### 12.7.1 非极大值抑制的作用

到目前为止我们学到的对象检测中的一个问题是，算法可能对同一个对象做出多次检测而不是一次，非极大值抑制法可以**确保算法对每个对象只检测一次**。

#### 12.7.2 非极大值抑制的实现

非极大值抑制(Non-max suppression)：只输出概率最大的分类结果，抑制很接近但不是最大的其他预测结果。

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5ram3pA8n8kMePviCf7vYeNvRklo8.D2GS.AtuBZJFh.pz240sQ.B5xjTVtE.rRyrO4gpcrYJji6YuKDOH2CNQg!/b&bo=NAK*AQAAAAADB6o!&rf=viewer_4)

假设要在这张图里检测行人和汽车，我们可能会在上面放个19×19的网格，理论上每辆汽车只有一个中点，应该只分配到中点所在的格子里，实践中在跑对象分类和定位算法时，对每个格子都跑一次，橙色和绿色方框的那些格子可能会想，这辆车的中点应该在其格子内部，觉得它们格子内有车的概率很高，而不是19×19=361个格子中仅有两个格子会报告它们检测出一个对象，所以最后可能会对同一个对象做出多次检测。

非极大值抑制做的就是清理这些检测结果，对每个对象只检测一次，而不是每个对象都触发多次检测。

**非极大值抑制的实现步骤：**

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/45NBuzDIW489QBoVep5mcQwssLFqw3j80oxEaoxBGkB8AkipMFnbnAtXcqb8T.VuE0SrICcYk8BecPo9O0Xf3*3m4kziNMN5upJ7yzScfxI!/b&bo=EAKdAQAAAAADJ4w!&rf=viewer_4)

1. 去掉所有预测边界框中 $p_c \leq 0.6$ 的边界框（这里的0.6是阈值，可以自己设定），抛弃了所有概率比较低的输出边界框，则剩下的边界框中存在对象的概率至少有0.6。
2. 对于剩下的边界框，首先看看每次报告中每个检测结果相关的概率 $p_c$，找到**概率最大**的那个，这个例子中是0.9，认为这是最可靠的检测，用高亮标记，就说在这里找到了一辆车。
3. 接着，非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边界框有很高交并比、高度重叠的其他边界框的输出就会被抑制，所以这两个 $p_c$ 分别为0.6和0.7的矩形和最大边界框(淡蓝色矩形)重叠程度很高，会被抑制、变暗。

左边那辆车也同理，找到 $p_c$ 最大的一个是0.8，认为这里检测出一辆车，用高亮显示，而其他IoU值很高的矩形会被抑制而变暗。直接抛弃变暗的矩形，只剩下高亮显示的，就是最后得到的两个预测结果。

上面介绍的只是算法检测单个对象的情况，若想同时检测三种对象，比如行人、汽车和摩托，则需要独立进行三次非极大值抑制，对每个输出类别都做一次。

### 12.8 Anchor Boxes

#### 12.8.1 为什么要用Anchor Boxes？

之前的算法只能在一个格子中检测出一个对象，Anchor Boxes可以**一个格子检测出多个对象**。如下图所示，使用3x3的网格，行人和汽车的中心几乎在同一个网格，这时就需要用到Anchor Boxes。

![1.PNG](https://upload-images.jianshu.io/upload_images/24435792-4974525edfee0a3c.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 12.8.2 思路

1. 预先定义几个不同形状的anchor box，把预测结果和这些anchor box关联起来。本例假设两个anchor box，对应行人和汽车。

   ![2.png](https://upload-images.jianshu.io/upload_images/24435792-d4288e9117c97d3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

2. 定义类别标签，将原来的向量$$\left[\begin{array}{llllllll}
   p_{c} & b_{x} & b_{y} & b_{h} & b_{w} & c_{1} & c_{2} & c_{3}
   \end{array}\right]^{T}$$重复两次$$\left.\begin{array}{llllllllllllllll}
   y=\left[p_{c}\right. & b_{x} & b_{y} & b_{h} & b_{w} & c_{1} & c_{2} & c_{3} & p_{c} & b_{x} & b_{y} & b_{h} & b_{w} & c_{1} & c_{2} & c_{3}
   \end{array}\right]^{T}$$。其中，前面的8个参数和anchor box 1相关联，后面的8个参数和anchor box 2相关联。

   ![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5lZ4RdoAHgxaBKcWOjF08TWbEkfhjJ03.3tLhj4tgHulVIAEGCJDTCyszfUDIGhciTnLzM8U7u3LiwaY.dAsyF0!/b&bo=rAFUAQAAAAADB9o!&rf=viewer_4)

3. 行人一般符合anchor box1形状，所以用anchor box1来预测行人会达到很好的效果，这么编码 $p_c$ = 1，代表有个行人，用 $b_x, b_y ,b_h$ 和 $b_w$ 来编码包住行人的边界框，用$c_1, c_2 ,c_3$($c_1$ = 1,$c_2$ = 0,$c_3$  = 0)来说明这个对象是个行人。 汽车一般符合anchor box2形状，采用相同的编码方法，定义对象是汽车和边界框等，所有参数都和检测汽车相关( $p_c$ = 1, $b_x, b_y ,b_h$,  $b_w$, $c_1$ = 0,$c_2$ = 1,$c_3$  = 0)。

4. 当两个anchor boxes对相同位置做出判别时，以交并比作为取舍指标。选择IoU最高的那个，用这个anchor box来进行预测。输出y的维度是$n \times n \times m \times c$（n为图片分成$n \times n$份，$m$为anchor box数量，$c$为class类别数）

#### 12.8.3 怎么选择anchor box？

1. 一般手工指定anchor box形状，根据要检测的对象，指定有针对性的anchor box，可选择5-10个anchor box，使其尽可能覆盖到不同形状。
2. 使用K-means聚类算法获得anchor box，选择最具有代表性的一组anchor box。

### 12.9 YOLO 算法

目标检测架构分为两种，一种是two-stage，一种是one-stage。 two-stage 有region proposal （选取候选区域）过程，网络会根据候选区域生成位置和类别，而 one-stage 直接从图片生成位置和类别。

YOLO 是一种 one-stage 方法，即 You Only Look Once 的缩写，意思是神经网络只需要看一次图片，就能输出结果。


#### 12.9.1 YOLO算法的实现

- 构造训练集：假设检测三种对象，行人，汽车和摩托。
  输入图像，神经网络的输出尺寸是3x3x2x8（3x3的网格，2个anchor box，8个参数）。遍历9个格子，构造训练集，然后构成对应的目标向量y，判断和anchor box 有关的 $p_c$ 和边界框，指定正确类别。

![3.png](https://upload-images.jianshu.io/upload_images/24435792-5e8987d12a016421.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 如何预测：输入图像，神经网络的输出尺寸是3x3x2x8，对于9个格子，每个都有对应的向量，如上图所示。
  对于1号蓝格，检测不到任何对象，所以2个anchor box的 $p_c$ = 0。剩下的是一些数字，多多少少都是噪音，但这些数字基本上会被忽略，因为神经网络告诉表示那里没有任何东西。（输出 y 如编号3所示）。
  对于2号绿格，里面有汽车。对于边界框1（anchor box 1）来说 $p_c$ = 0，表示此格子中无行人。然后就是一组数字噪音，此格子中无行人。anchor box 2对应汽车，此格子中有车，因此希望数字可以对车子指定一个相当准确的边界框，即（$$\left.p_{c}=1, b_{x}, b_{y}, b_{h}, b_{w}, c_{1}=0, c_{2}=1, c_{3}=0\right)$$，这就是神经网络做出预测的过程。
- 运行非极大值抑制：看看一张新的测试图片，对于两个anchor box，那么对于9个格子中任何一个都会有两个预测的边界框。
  针对每个框，抛弃概率低的预测，去掉那些连神经网络都说这里很可能什么都没有的边界框。如果希望检测行人、车子和摩托三个对象检测类别，那么需要对每个类别单独运行非极大值抑制，处理预测结果即此类别的边界框。

![image.png](https://upload-images.jianshu.io/upload_images/24435792-17d04d3470377d46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 最后，输出位置和类别信息。

#### 12.9.2 YOLO v1、v2、v3和v4

YOLO 一共发布了四个版本，其中 YOLOv1 奠定了整个系列的基础，后面的系列就是在第一版基础上的改进，只为提升性能。

- YOLOv2
  提出了 ImageNet 和 COCO 数据集结合的联合训练方法，使算法可以在检测数据集和分类数据集上训练目标检测器。用检测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。
- YOLOv3
  YOLOv3 的提出不是为了解决什么问题，将模型复杂了不少，可以通过改变模型结构的大小来权衡速度与精度。
- YOLOv4
  YOLOv4的改进总结了几乎所有的检测技巧，又提出一点儿技巧，然后经过筛选，排列组合，挨个实验（ablation study）哪些方法有效，经过大量实验选出的性能最好的组合。
  发展历程：
  ![](https://pic3.zhimg.com/80/v2-7974cdeaf25c84179c59ffd110503986_1440w.jpg)

### 12.10 RPN网络

RPN：Region Proposal Network

#### 12.10.1 Region with CNN（R-CNN）

意思基于候选区域（Region Proposal）的卷积网络，不再针对每个滑动窗口跑检测算法，而是使用某种算法求出候选区域，然后对每个候选区域跑一下分类器，每个区域会输出一个标签和一个边界框。这样就只是选择一些有意义的窗口，在少数窗口上运行卷积网络分类器，以节省时间。

- 选出的候选区域方法：图像分割算法

![image.png](https://upload-images.jianshu.io/upload_images/24435792-03a635c204bc593a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图所示。分割色块后，选择其对应的一个边界框，然后在色块上跑分类器，这样的处理比滑动窗口法处理的窗口少的多，可以减少卷积网络分类器运行时间，而不必在所有位置运行一遍分类器。

另外，说明一下，R-CNN算法不会直接信任输入的边界框，它也会输出一个边界框$$b_{x},\quad b_{y},\quad b_{h},\quad b_{w}$$，这样得到的边界框比较精确，比单纯使用图像分割算法给出的色块边界要好。


#### 12.10.2 具体步骤

![](https://img-blog.csdnimg.cn/20200818164903600.png#pic_center)

1. 输入图像；运行图像分割算法，利用selective search对图像生成1K~2K的候选区域（region proposal）；
2. 将每个候选区域缩放(warp)成227×227的大小并输入到CNN，将CNN的全连接层fc7的输出作为特征；
3. 对CNN的fc7层的输出特征输入到SVM进行分类；如果有十个类别，那么每个region proposal要经过10个SVM，得到预测的类别。
4. 修正：对于SVM分好类的Region Proposal做边框回归，用Bounding box回归值校正原来的候选窗口，生成预测窗口坐标。

#### 12.10.3 缺点和改进

R-CNN训练耗时，占用磁盘空间大，测试速度慢，每个候选区域需要运行整个前向CNN计算；因此提出了其它改进算法。

![](https://img-blog.csdnimg.cn/20200818174843888.jpg#pic_center)

- Fast R-CNN：利用卷积实现了滑动窗口。相比于R-CNN，Fast R-CNN不再对每一个候选区域进行CNN特征提取，而是先将整张图片归一化后输入CNN网络进行特征提取，之后候选区域在特征图上进行截取，显著提升了R-CNN的速度。但是其缺点是得到候选区域的聚类步骤仍然非常缓慢。

- Faster R-CNN：加入RPN网络，将特征提取、候选区域提取、bounding box regression、分类都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。（仍比YOLO慢）

- Gaussian Faster R-CNN主要有以下两个创新点：
  1. 卷积的过程其实就是一个特征提取的过程，所以在预处理之后移除了全连接这个操作，简化了网络结构，以准确率换速度。
  2. 假设在多次卷积之后，整个特征图是一个平稳的随机序列，符合高斯分布，则可以采用 $3\sigma$ 原理对树进行裁剪，因为四周的像素对分类任务来说无关紧要，但是完全压缩和分类这些无关紧要的像素会花费大量的CPU时间。最后用随机森林代替B-P神经网络，将时间复杂度由 $O(N)$ 降到 $O(log2N)$。

### 12.11 目标检测算法的评价指标

#### 12.11.1 准确度指标——平均准确度均值mAP

- **什么是TP、TN、FP、FN？**

​      TP、TN、FP、FN即true positive, true negative, false positive, false  negative的缩写，positive和negative表示的是预测得到的结果，预测为正类则为positive，预测为负类则为negative; true和false则表示预测的结果和真实结果是否相同，相同则是true，不同则为false，如下图：

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5tN1*xL8RiPFVLVtV2RCwU35xjlr*DDv1tW3nRdPB0XcIL0h7xB1LhZClvGqkrhjnzRdyslsmRNtEBq85.R5yBo!/b&bo=9AGIAAAAAAADB18!&rf=viewer_4)

举个例子，假设现在有这样一个测试集，其中的图片只由大雁和飞机两种图片组成，如下图所示：

![](https://img-blog.csdn.net/20170105152944065?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHlzdGVyaWMzMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

假设分类系统最终的目的是取出测试集中所有飞机的图片，则 TP、TN、FP、FN的具体定义分别是：

**True positives :** 飞机的图片被正确的识别成了飞机。 
**True negatives:** 大雁的图片没有被识别出来，系统正确地认为它们是大雁。 
**False positives:** 大雁的图片被错误地识别成了飞机。 
**False negatives:** 飞机的图片没有被识别出来，系统错误地认为它们是大雁。

- **Precision 与 Recall**

**Precision**（精确度、查准率）指的是模型判断该图片为正类且该图片也的确是正类的概率，即在模型判断为正类的图片中，True positives所占的比率，**衡量的是一个分类器分出来的正类的确是正类的概率**。
$$
Precision=\frac{TP}{TP+FP}
$$
​        光是精确度还不能衡量分类器的好坏程度，比如50个正样本和50个负样本，如果分类器把49个正样本和50个负样本都分为负样本，剩下一个正样本分为正样本，这样精确度也是100%，所以还需要召回率来衡量。

**Recall**（召回率、查全率）指的是测试集的所有正样本比例中，被模型正确识别为正样本的比例，即模型判断为正类且实际的确是正类的图像数量/真实类别是正类的图像数量，**衡量的是一个分类器能把所有的正类都找出来的能力**。
$$
Recall=\frac {TP}{TP+FN}
$$
在上面所举的例子中，假设识别出了四个结果，如下图所示：

![](https://img-blog.csdn.net/20170105153246462?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHlzdGVyaWMzMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中，在识别出来的四张照片中：

**True positives :** 有三个，画绿色框的飞机，即 $TP=3$。

**False positives:** 有一个，画红色框的大雁，即 $FP=1$。

没被识别出来的六张图片中： 
**True negatives :** 有四个，这四个大雁的图片，系统正确地没有把它们识别成飞机，即 $TN=4$。 
**False negatives:** 有两个，两个飞机没有被识别出来，系统错误地认为它们是大雁，即 $FN=2$。

故 $Precision=\frac {TP}{TP+FP}=\frac {3}{3+1}=0.75$ ，意味着在识别出来的结果中，飞机的图片占75%；$Recall=\frac {TP}{TP+FN}=\frac{3}{3+2}=0.6$，意味着在所有的飞机图片中，60%的飞机图片被正确识别为飞机。

- **调整阈值**

可以通过调整阈值来选择让模型识别出多少图片，进而改变Precision或Recall的值。在某种阈值的前提下（蓝色虚线），模型识别出了四张图片，如下图所示：
![](https://img-blog.csdn.net/20170105154012966?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHlzdGVyaWMzMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

分类系统认为大于阈值（蓝色虚线之上）的四个图片更像飞机。

我们可以通过改变阈值（可以看作上下移动蓝色虚线）来选择让系统识别出多少张图片，从而改变Precision或Recall的值。比如，把蓝色虚线移到第一张图片下面，也就是说让系统只识别出最上面的那张飞机图片，那么Precision的值就是100%，而Recall的值是20%；如果把蓝色虚线放到第二张图片下面，也就是说让系统只识别出最上面的前两张图片，那么Precision的值还是100%，而Recall的值则增长到40%。下图为不同阈值条件下，Precision与Recall的变化情况：
![](https://img-blog.csdn.net/20170105154105095?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHlzdGVyaWMzMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

- **Precision-Recall曲线**

Precision-Recall曲线(P-R曲线)是以Precision和Recall作为纵、横轴坐标的二维曲线，通过选取不同阈值时对应的精确度和召回率画出，如下图所示。

![](https://img-blog.csdn.net/20170105154145685?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHlzdGVyaWMzMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

总体趋势是，精确度越高，召回率越低，当召回率达到1时，对应概率分数最低的正样本，这个时候正样本数量除以所有大于等于该阈值的样本数量就是最低的精确度。上图所示的分类器在不损失精度的条件下能达到40%的Recall，而当Recall达到100%时，Precision降到50%。

Precision-Recall曲线可以用来显示分类器在Precision和Recall之间的权衡，从而评估分类器的性能。如果一个分类器的性能较好，那么它应该有如下的表现：被识别出的图片中飞机所占的比重比较大，并且在识别出大雁之间，尽可能多地正确识别出飞机，也就是让Recall值增长的同时保持Precision的值在一个很高的水平。而性能较差的分类器可能会损失很多Precision值才能换来Recall值的提高。

- **平均准确度AP**

AP：Average-Precision，平均准确度，综合考量了Precision和Recall的影响，反映了模型对某个类别识别的好坏。

P-R曲线围起来的面积就是AP值，通常来说一个分类器越好，AP值越高。

- **平均准确度均值mAP**

mAP：mean Average-Precision，平均精确度均值，即所有类别AP的平均值，衡量的是在所有类别上的平均好坏程度。

mAP的取值范围是[0,1]，越大越好。

- 为什么可以使用mAP来评价目标检测的效果？

目标检测的效果取决于预测框的位置和类别是否准确，从mAP的计算过程中可以看出通过计算预测框和真实框的IoU来判断预测框是否准确预测到了位置信息，同时精确度和召回率指标的引用可以评价预测框的类别是否准确，因此mAP是目前目标检测领域非常常用的评价指标。

> 参考文献：
>
> 1. https://blog.csdn.net/syoung9029/article/details/56276567
>
> 2. https://www.cnblogs.com/wanghui-garcia/p/11084833.html
> 3. https://blog.csdn.net/weixin_43423892/article/details/106643649

#### 12.11.2 速度指标——FPS

目标检测技术的很多实际应用在准确度和速度上都有很高的要求，如果不计速度性能指标，只注重准确度表现的突破，其代价是更高的计算复杂度和更多内存需求。只有速度快，才能实现实时检测，这对一些应用场景极其重要。

FPS：Frame Per Second，每秒帧率，即每秒内可以处理的图片数量。当然要对比FPS，需要在同一硬件上进行。

另外也可以使用处理一张图片所需时间来评估检测速度，时间越短，速度越快。

> 参考：
>
> 1. https://zhuanlan.zhihu.com/p/70306015
> 2. https://blog.csdn.net/qq_29893385/article/details/81213377

#### 12.11.3 模型大小

自 2012 年  AlexNet 以来，卷积神经网络（简称 CNN）在图像分类、图像分割、目标检测等领域获得广泛应用。随着性能要求越来越高，AlexNet  已经无法满足大家的需求，于是乎各路大牛纷纷提出性能更优越的 CNN 网络，如 VGG、GoogLeNet、ResNet、DenseNet  等。由于神经网络的性质，为了获得更好的性能，网络层数不断增加，从 7 层 AlexNet 到 16 层 VGG，再从 16 层 VGG 到  GoogLeNet 的 22 层，再到 152 层 ResNet，更有上千层的 ResNet 和  DenseNet。虽然网络性能得到了提高，但随之而来的就是效率问题。

效率问题主要是**模型的存储问题和模型进行预测的速度问题**（以下简称速度问题）

- 第一，存储问题。数百层网络有着大量的权值参数，保存大量权值参数对设备的内存要求很高；
- 第二，速度问题。在实际应用中，往往是毫秒级别，为了达到实际应用标准，要么提高处理器性能（看英特尔的提高速度就知道了，这点暂时不指望），要么就减少计算量。

只有解决 CNN 效率问题，才能让 CNN 走出实验室，更广泛的应用于移动端。对于效率问题，通常的方法是进行**模型压缩**（Model Compression），即在已经训练好的模型上进行压缩，使得网络携带更少的网络参数，从而解决内存问题，同时可以解决速度问题。

相比于在已经训练好的模型上进行处理，轻量化模型设计则是另辟蹊径。**轻量化模型设计**主要思想在于设计更高效的网络计算方式（主要针对**卷积**方式），从而使网络参数减少的同时，不损失网络性能。近年提出的四个轻量化模型分别是：SqueezeNet、MobileNet、ShuffleNet、Xception，有兴趣的可以参考以下博客或查找论文以了解。

> 参考：https://blog.csdn.net/u012426298/article/details/80817788